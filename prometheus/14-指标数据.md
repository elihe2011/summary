# 1. 数据模型

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-model.png)

Prometheus 采集的所有指标都以时间序列的形式进行存储，每个时间序列有三部分组成：

- 指标名和指标标签集合：`metric_name{<label1=v1>,<label2=v2>...}`
  - 指标名：表示这个指标是监控哪一方面的状态，比如 http_request_total 表示请求数量
  - 指标标签：描述所选指标的维度，比如 http_request_total 下，有请求状态码 code = 200/400/500，请求方式 method = get/post等
- 时间戳：描述当前时间序列的时间，单位：毫秒
- 样本值：当前监控指标的具体数值



## 1.1 样本

Prometheus 会将所有采集到的样本数据以时间序列(time-series)的方式保存在内存数据库中，并定时持久化到磁盘。time-series 是按照时间戳和值的序列顺序存放的，称之为向量(vector)。每条 time-series 通过指定名称 (metrics name) 和 一组标签集(label-set)命名。

可以将 time-series 理解位一个以时间为 Y 轴的数字矩阵：

```text
  ^
  │   . . . . . . . . . . . . . . . . .   . .   node_cpu{cpu="cpu0",mode="idle"}
  │     . . . . . . . . . . . . . . . . . . .   node_cpu{cpu="cpu0",mode="system"}
  │     . . . . . . . . . .   . . . . . . . .   node_load1{}
  │     . . . . . . . . . . . . . . . .   . .  
  v
    <------------------ 时间 ---------------->
```

在 time-series 中的每一个点称为一个样本 (sample)，样本由三部分组成：

- 指标(metric)：指标名称和描述该样本特征的标签集
- 时间戳(timestamp)：精确到毫秒
- 样本值(value)：float64浮点型

```text
<--------------- metric ---------------------><-timestamp -><-value->
http_request_total{status="200", method="GET"}@1434417560938 => 94355
http_request_total{status="200", method="GET"}@1434417561287 => 94334

http_request_total{status="404", method="GET"}@1434417560938 => 38473
http_request_total{status="404", method="GET"}@1434417561287 => 38544

http_request_total{status="200", method="POST"}@1434417560938 => 4748
http_request_total{status="200", method="POST"}@1434417561287 => 4785
```



## 1.2 指标

```text
<metric name>{<label name>=<label value>, ...}
```

**指标名称**：反映被监控样本的含义，只能由 ASCII 字符、数字、下划线及冒号组成 `[a-zA-Z_:][a-zA-Z0-9_:]*`

**标签**：反应样本的特征维度，Prometheus可通过维度对样本数据进行过滤、聚合等。只能由 ASCII 字符、数字及下划线组成 `[a-zA-Z_][a-zA-Z0-9_]`。其中以 `__` 作为前缀的标签，是系统保留的关键字，仅系统内部使用

**标签值**：可以饱和任何 Unicode 编码的字符



在 Prometheus 底层限制中指标名称实际上以 `__name__=<metric name>` 形式保存在数据库中，因此以下两个方式均表示同一条 time-series：

```text
api_http_requests_total{method="POST", handler="/messages"}

{__name__="api_http_requests_total"，method="POST", handler="/messages"}
```



# 2. 指标类型

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-types.png)



## 2.1 Counter

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-counter.png)

**计数器**，用于记录请求总数、错误总数等。常见的监控指标有 http_requests_total，node_cpu_seconds_total 等

使用示例：

- 通过 rate() 函数获取 HTTP 请求量的增长率：

  ```text
  rate(http_requests_total[5m])
  ```

- 查询当前系统中，访问量前10的 HTTP 地址

  ```text
  topk(10, http_requests_total)
  ```


![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-counter-instance.png)



## 2.2 Gauge

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-gauge.png)

**仪表盘**，系统的瞬时状态。常见的监控指标有 node_memory_MemFree_bytes，node_memory_MemAvailable_bytes。

使用示例：

- 查看系统当前内存状态

  ```
  node_memory_MemFree_bytes
  ```

- 通过 delta() 获取样本在一段时间内的变化情况，计算内存在两小时内的变化

  ```text
  delta(node_memory_MemAvailable_bytes{instance="172.16.8.158:9100"}[2h])
  ```

- 通过 predict_linear() 对数据变化趋势进行预测，预测系统磁盘空间在4小时之后的剩余情况

  ```text
  predict_linear(node_filesystem_files{job="cvm"}[1h], 4 * 3600)
  ```


![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-gauge-instance.png)



## 2.3 Histogram

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-histogram.png)

### 2.3.1 直方图

直方图 (Histogram) 会在一段时间范围内对数据进行采样（通常是请求持续时间或响应大小等），并将其计入可配置的存储桶 (bucket) 中。

假设想监控某个应用在一段时间内的响应时间，最后监控到的样本的响应时间范围为 0s~10s。现在将样本的值域划分为不同的区间，即不同的 bucket，每个 bucket 的宽度是 0.2s。响应时间数据分布如下：

- bucket-1：(0, 0.2s]
- bucket-2：(0.2s, 0.4s]
- ...

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/histogram-api-latencies.png)



Prometheus 的 histogram 是一种累计直方图，每一个 bucket 的样本包含了之前所有 bucket 的样本，响应时间数据分布如下：

- bucket-1：(0, 0.2s]
- bucket-2：(0, 0.4s]
- ...

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/histogram-api-latencies-cumulative.png)



### 2.3.2 累积直方图

Prometheus histogram 类型指标的样本数据如下：

```
# HELP ts_http_seconds_bucket web http response time in seconds
# TYPE ts_http_seconds_bucket histogram
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="1"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="2"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="4"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="8"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="16"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="32"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="64"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="128"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="512"} 0
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="1024"} 877
http_seconds_bucket_bucket{method="put",path="/api/v1/auth",statusCode="200",le="+Inf"} 1062
http_seconds_bucket_sum{method="put",path="/api/v1/auth",statusCode="200"} 1.19617e+06
http_seconds_bucket_count{method="put",path="/api/v1/auth",statusCode="200"} 1062
```



在抓取时，丢弃 le 小于 32 的 bucket，可以通过 relabel 配置实现：

```yaml
scrape_configs:
- job_name: my_job
  static_configs:
  - targets:
    - 192.168.3.100:6060
  metric_relabel_configs:
  - source_labels: [__name__, le]
    regex: http_seconds_bucket_bucket;(0|[1-9]|1[0-9]|2[0-9]|3[0-1](\.\d+)?)
    action: drop
```

通过这种方法，可以丢弃任意的 bucket，但不能丢弃 `le="+Inf"` 的 bucket，因为 `histogram_quantile` 函数需要使用这个标签。

另外 histogram 还提供了 `_sum` 和 `_count` 指标，即使丢弃了所有的 bucket，任然可以通过这两个指标来计算平均响应时间。

```
rate(http_seconds_bucket_sum[1m]) / rate(http_seconds_bucket_count[1m])
```



### 2.3.3 分位数计算

Histogram 本质上是一些桶。为了计算 P99，可以将所有的请求分成 10 个桶。第一个桶存放 0~1ms 完成的请求数量，后面 9 个桶存发的请求耗时上区间分别是 5ms 10ms 50ms 100ms 200ms 300ms 500ms 1s 2s。这样只要保存 10 个数字即可。

要计算 P99，只需要知道第 99% 个数字落在了哪一个桶。假设落在了 300ms~500ms 的桶，即可认为 99% 的请求都在 500ms 之内完成（准确地说，应该是第 99% 个请求在 500ms ~ 500ms 之间完成）

由于监控一般是绘制一条曲线，而不是一个区间，所以 P99 在 300~500之间不行，需要计算一个数字出来。

Prometheus 是假设每个桶内的数据都是线性分布的，比如说现在 300~500的桶内一共有 100 个请求，小于300个桶内一共有 9850 个请求，所有的桶一共 10000 个请求。那么要找的 P99 其实是 10000 * 0.99 = 9900 个请求。第 9900 请求子啊 300~500 的桶内是第 9900 - 9850 = 50 个请。根据桶内面都是线性分布的假设，第 50 个请求在这个桶内的耗时是 `(500 - 300) * (50/100) + 300` = 400ms，即 P99 是 400ms

Prometheus 源代码 `prom/quantile.go` 的计算公式：

```go
return bucketStart + (bucketEnd-bucketStart)*float64(rank/count)
```

因为基于线性分布这个假设，部署准确的数据。比如假设 300~500 的桶内耗时最高的请求也只有 310ms，得到的结果也还是 400ms。

**桶区间越大，越不准确；桶区间越小，越准确**



### 2.3.4 `histogram_quantile`

`histogram_quantile(φ float, b instant-vector)` 从 bucket 类型的向量 b 中计算  φ (0 ≤ φ ≤ 1) 分位数 的样本的最大值。

```
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="0.005", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="0.01", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="0.025", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="0.05", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="0.1", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="0.25", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="0.5", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="1", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="10", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="2.5", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="5", method="get"}
http_request_duration_seconds_bucket{cluster="kubernetes", handler="metrics", instance="10.240.1.248:8081", job="kube-state-metrics", le="+Inf", method="get"}
```

计算过去 10 分钟，第 90 个百分位数：

```python
# 使用 rate() 函数来指定分位数计算的时间窗口
histogram_quantile(0.9, rate(http_request_duration_seconds_bucket[10m]))
```

根据 job 标签来对第 90 个百分位数进行聚合：

```
histogram_quantile(0.9, sum(rate(http_request_duration_seconds_bucket[10m])) by (job, le))
```

`histogram_quantile` 函数是根据假定每个区间内的样本分布是线性分布来计算结果值的(也就是说它的结果未必准确)，最高的 bucket 必须是 `le="+Inf"` (否则就返回 `NaN`)。

如果分位数位于最高的 `bucket(+Inf)`中，则返回第二个最高的 bucket 的上边界。如果该 bucket 的上边界大于 0，则假设最低的 bucket 的的下边界为 0，这种情况下在该 bucket 内使用常规的线性插值。

如果分位数位于最低的 bucket 中，则返回最低 bucket 的上边界。

如果 b 含有少于 2 个 buckets，那么会返回 `NaN`，如果 φ < 0 会返回 -Inf，如果 φ > 1 会返回 +Inf。



Prometheus TSDB 直方图：

```text
# HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction
# TYPE prometheus_tsdb_compaction_chunk_range histogram
prometheus_tsdb_compaction_chunk_range_bucket{le="100"} 0
prometheus_tsdb_compaction_chunk_range_bucket{le="400"} 0
prometheus_tsdb_compaction_chunk_range_bucket{le="1600"} 0
prometheus_tsdb_compaction_chunk_range_bucket{le="6400"} 0
prometheus_tsdb_compaction_chunk_range_bucket{le="25600"} 0
prometheus_tsdb_compaction_chunk_range_bucket{le="102400"} 0
prometheus_tsdb_compaction_chunk_range_bucket{le="409600"} 0
prometheus_tsdb_compaction_chunk_range_bucket{le="1.6384e+06"} 260
prometheus_tsdb_compaction_chunk_range_bucket{le="6.5536e+06"} 780
prometheus_tsdb_compaction_chunk_range_bucket{le="2.62144e+07"} 780
prometheus_tsdb_compaction_chunk_range_bucket{le="+Inf"} 780
prometheus_tsdb_compaction_chunk_range_sum 1.1540798e+09
prometheus_tsdb_compaction_chunk_range_count 780
```

观察请求耗时在各个桶的分布。Histogram 是累计直方图，即每个桶的只有上区间。如图表示小于 0.1 毫秒的请求数量是 18173 个，小于 0.2 毫秒 的请求为 18182 个。桶 le="0.2" 包含了桶 le="0.1" 的所有数据，0.1~0.2毫秒之间的请求量，两桶相减即得。 

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-histogram-instance.png)



## 2.4 Summary

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-summary.png)

### 2.4.1 摘要

**摘要**，随机正态分布数据，用来做统计分析的，与 Histogram 的区别在于，Summary 直接存储的就是百分比

类似于Histogram，但更注重于分位数的计算。它同样提供了_sum、_count以及多个quantile（分位数）。虽然它还提供观察结果的总数和所有观察值的总和，但它会在滑动时间窗口内计算可配置的分位数。

基本指标名称为 `<basename>` 的摘要会在抓取过程中显示多个时间序列：

- 流式传输观察到的事件的 **φ-分位数** (0 ≤ φ ≤ 1)： `<basename>{quantile="<φ>"}`
- 所有观察值**总和**： `<basename>_sum`
- 已观察到的事件**计数**，显示为 `<basename>_count`

```text
# HELP prometheus_tsdb_wal_fsync_duration_seconds Duration of WAL fsync.
# TYPE prometheus_tsdb_wal_fsync_duration_seconds summary
prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.5"} 0.012352463
prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.9"} 0.014458005
prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.99"} 0.017316173
prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002
prometheus_tsdb_wal_fsync_duration_seconds_count 216
```

从上面的样本中可以得知当前Prometheus Server进行wal_fsync操作的总次数为216次，耗时2.888716127000002s。其中中位数（quantile=0.5）的耗时为0.012352463，9分位数（quantile=0.9）的耗时为0.014458005s。

Summary 的百分比由客户端计算好，Prometheus 只负责抓取，可通过内置函数 histogram_quantile 在服务端计算

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/prometheus/prometheus-metric-type-summary-instance.png)

### 2.4.2 与Histogram对比

summary 表示摘要数据，跟 histogram 类似，反映的都是统计类数据。主要用于表示一段时间内数据采集结果（通常是请求持续时间或响应大学），它直接存储了 `quantile` 数据，而不是根据统计区间计算出来。

```
doris_fe_query_latency_ms{cluster_name="doris-igom", group="fe", instance="172.16.8.158:8030", job="doris", quantile="0.75", user="root"}  78
doris_fe_query_latency_ms{cluster_name="doris-igom", group="fe", instance="172.16.8.158:8030", job="doris", quantile="0.95", user="root"}  99
doris_fe_query_latency_ms{cluster_name="doris-igom", group="fe", instance="172.16.8.158:8030", job="doris", quantile="0.98", user="root"}  101
doris_fe_query_latency_ms{cluster_name="doris-igom", group="fe", instance="172.16.8.158:8030", job="doris", quantile="0.99", user="root"}  222
doris_fe_query_latency_ms{cluster_name="doris-igom", group="fe", instance="172.16.8.158:8030", job="doris", quantile="0.999", user="root"} 222
```



summary也提供 `_sum` 和 `_count` 两个指标，跟histogram差不多

```
doris_fe_query_latency_ms_sum{cluster_name="doris-igom", group="fe", instance="172.16.8.158:8030", job="doris"} 11311116.324963506

doris_fe_query_latency_ms_count{cluster_name="doris-igom", group="fe", instance="172.16.8.158:8030", job="doris"} 179470
```



注意：**不能对Summary产生的quantile值进行aggregation运算（例如sum, avg等）**



**Summary vs Histogram**:

- Summary 的 quantile 计算是在数据上报的时候就已经计算好的，需要在定义数据指标的时候就指定quantile的值，因为是数据上报计算的quantile，所以不支持包含数据过滤和聚合的quantile计算。
- 对于分位数的计算而言，Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。反之对于客户端而言Histogram消耗的资源更少。
- Summary 结构有频繁的全局锁操作，对高并发程序性能存在一定影响。histogram仅仅是给每个桶做一个原子变量的计数就可以了，而summary要每次执行算法计算出最新的X分位value是多少，算法需要并发保护。会占用客户端的cpu和内存
- histogram不能得到精确的分为数，设置的bucket不合理的话，误差会非常大



两条经验法则：

1. 如果需要汇总，选择直方图。
2. 否则，如果您对将要观察的值的范围和分布有所了解，请选择直方图。无论值的范围和分布如何，如果需要准确的分位数，请选择摘要

























