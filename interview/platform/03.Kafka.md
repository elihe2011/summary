# 1. 简介

## 1.1 Kafka 的特点

Kafka 是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统，常见可以用于web/nginx日志、访问日志，消息服务等

Kafka是一个分布式流媒体平台，它主要有三种功能：

- 发布和订阅消息流
- 以容错方式记录消息流，以文件方式存储消息流
- 可以在消息发布的时候进行处理

Kafka是分布式的，其所有的构件borker(服务端集群)、producer(消息生产)、consumer(消息消费者)都可以是分布式的。

在消息的生产时可以使用一个标识topic来区分，且可以进行分区；每一个分区都是一个顺序的、不可变的消息队列， 并且可以持续的添加。

同时为发布和订阅提供高吞吐量。

消息被处理的状态是在consumer端维护，而不是由server端维护。当失败时能自动平衡



## 1.2 使用场景

- **监控**：主机通过Kafka发送与系统和应用程序健康相关的指标，然后这些信息会被收集和处理从而创建监控仪表盘并发送警告。

- **消息队列**： 应用程度使用Kafka作为传统的消息系统实现标准的队列和消息的发布—订阅。Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。

- **站点用户活动追踪**: 为了更好地理解用户行为，改善用户体验，将用户查看了哪个页面、点击了哪些内容等信息发送到每个数据中心的Kafka集群上，并通过Hadoop进行分析、生成日常报告。

- **流处理**：保存收集流数据，以提供之后对接的Storm或其他流式计算框架进行处理。很多用户会将那些从原始topic来的数据进行阶段性处理、汇总、扩充或者以其他的方式转换到新的topic下再继续后面的处理。

- **日志聚合**: 使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟

- **持久性日志**：Kafka可以为一种外部的持久性日志的分布式系统提供服务。这种日志可以在节点间备份数据，并为故障节点数据回复提供一种重新同步的机制。



## 1.3 基础概念

1. Topic(话题)：Kafka中用于区分不同类别信息的类别名称。由producer指定   

2. Producer(生产者)：将消息发布到Kafka特定的Topic的对象(过程)    

3. Consumers(消费者)：订阅并处理特定的Topic中的消息的对象(过程)    

4. Broker(Kafka服务集群)：已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个话题，并从Broker拉数据，从而消费这些已发布的消息。   

5. Partition(分区)：Topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）    

6. Message：消息，是通信的基本单位，每个producer可以向一个topic（主题）发布一些消息。

   


## 1.4 消息格式

V1: 消息由一个固定大小的报头和可变长度但不透明的字节阵列负载。报头包含格式版本和CRC32效验和以检测损坏或截断

- CRC校验码: 4 字节的CRC 校验码， 用于确保消息在传输过程中 不会被恶意篡改

- key： 消息键， 对消息做partition时使用。即决定消息保存在某个topic下的那个partition

- value: 消息体，保存实际信息

- timestamp： 消息发生时间戳，用于流式处理以及其他依赖实际的处理语义

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/platform/kafka-message-structure.png)

```
    1. 4 byte CRC32 of the message
    2. 1 byte "magic" identifier to allow format changes, value is 0 or 1
    3. 1 byte "attributes" identifier to allow annotations on the message independent of the version
       bit 0 ~ 2 : Compression codec
           0 : no compression
           1 : gzip
           2 : snappy
           3 : lz4
       bit 3 : Timestamp type
           0 : create time
           1 : log append time
       bit 4 ~ 7 : reserved
    4. (可选) 8 byte timestamp only if "magic" identifier is greater than 0
    5. 4 byte key length, containing length K
    6. K byte key
    7. 4 byte payload length, containing length V
    8. V byte payload
```



# 2. 原理

## 2.1 架构

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/platform/kafka.png)



- Producer：生产者，消息的发布者
- kafka cluster：kafka集群，一台或多台服务器组成
  - Broker：即部署了Kafka实例的服务器节点。每个服务器上有一个或多个kafka的实 例。每个kafka集群内的broker都有一个不重复的编号，如图中的broker-0、broker-1等……
  - Topic：消息主题，即消息的分类，kafka数据保存在topic。在每个broker上 都可以创建多个topic。实际应用中通常是一个业务线建一个topic。
  - Partition：Topic的分区，每个topic可以有多个分区，分区的作用是做负载，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的，partition的表现形式就是一个一个的⽂件夹。
  - Replication：分区的副本。当主分区（Leader）故障时，会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10 个，且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。
- Consumer：消费者，即消息的消费方，是消息的出口。
  - Consumer Group：由多个消费组成一个消费者组。同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个 topic的不同分区的数据。



## 2.2 工作流程

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/platform/kafka-work-flow.png)

1) 生产者从kafka集群获取分区leader信息

2) 生产者将消息发送给leader

3) leader将消息写入本地磁盘

4) follower从leader拉取消息数据 （主动地）

5) follower将消息写入本地磁盘后向leader发送ACK

6) leader收到所有的follower的ACK之后向生产者发送ACK



## 2.3 选择 Partition 的原则

1）partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。

2）如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。

3）如果既没指定partition，又没有设置key，则会采用轮询⽅式，即每次取一小段时间的数据写入某个partition，下一小段的时间写入下一个partition

总结：**指定就用指定的；未指定则使用key的hash来确定；没有key的，则通过时间段轮询方式写入**


## 2.4 ACK 应答机制

producer 往集群发送数据，ACK应答，可设置 0, 1, all 三种值：

- 0：不需要等待集群的返回，不确保消息发送成功。安全性最低但效率最高。
- 1：只要leader应答就可以发送下一条，只确保leader发送成功。
- all：需要所有的follower都完成从leader的同步才会发送下一条，确保所有的副本都完成备份。安全性最高但效率最低。



## 2.5 Topic 和数据日志

topic 是同⼀类别的消息记录（record）的集合。在Kafka中，⼀个主题通常有多个订阅者。对于每个主题，Kafka集群维护了⼀个分区数据⽇志⽂件结构。

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/platform/kafka-topic-anatomy.png)

每个partition都是⼀个**有序**并且不可变的消息记录集合。当新的数据写⼊时，就被追加到partition的末尾。在每个partition中，每条消息都会被分配⼀个顺序的唯⼀标识，这个标识被称为offset，即偏移量。注意，**Kafka只保证在同⼀个partition内部消息是有序的，在不同partition之间，并不能保证消息有序。**

Kafka可以配置⼀个保留期限，⽤来标识⽇志会在Kafka集群内保留多⻓时间。Kafka集群会保留在保留期限内所有被发布的消息，不管这些消息是否被消费过。⽐如保留期限设置为两天，那么数据被发布到 Kafka集群的两天以内，所有的这些数据都可以被消费。当超过两天，这些数据将会被清空，以便为后续的数据腾出空间。由于Kafka会将数据进⾏持久化存储（即写⼊到硬盘上），所以保留的数据⼤⼩可以设置为⼀个⽐较⼤的值。



## 2.6 Partition 结构

每个 Topic 将消息分成多 Partition，每个Partition 存储在 append log文件中，任何发布到 该Partition的消息，都会直接追加到log文件的尾部。每条消息在文件中的位置成为 Offset，Partition 以文件的形式存在文件系统中，log文件根据 Broker 中的配置保存一定时间后删除来释放磁盘空间。

日志存储目录：`config/server.properties log.dirs=/tmp/kafka-logs`

**LogSegment:**

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/platform/kafka-log-segment.png)

Partition 是  Topic 的数据物理存储，本质是一个文件夹。每个分区被划分为多个日志分段 (LogSegment)，日志段是kafka日志对象分片的最小单位

LogSegment 的构成：

```bash
00000000000000000000.log      	# 数据文件
00000000000000000000.index  	# 索引文件
00000000000000000000.timeindex	# 索引文件
00000000000000000000.txnindex 	# 终止事务的索引文件
leader-epoch-checkpoint
```



为什么要分区?

- 分区后，上传HDFS建立分布式
- 提高吞吐量
- 一个分区只能被消费者组中的一个消费者所消费。



## 2.7 消费数据

多个消费者实例可以组成⼀个消费者组，并⽤⼀个标签来标识这个消费者组。⼀个消费者组中的不同消 费者实例可以运⾏在不同的进程甚⾄不同的服务器上。

如果所有的消费者实例都在同⼀个消费者组中，那么消息记录会被很好的均衡的发送到每个消费者实例。

如果所有的消费者实例都在不同的消费者组，那么每⼀条消息记录会被⼴播到每⼀个消费者实例。

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/platform/kafka-consumer-group.png)

同一个 Consumer Group 中，一个 Consumer 可消费一个 Partition

**每个消息分区，只能被同组的一个消费者进行消费**。需要再ZK上记录Partition和Consumer的关系，每个消费者一旦确定了对一个消息分区的消费权利，需要将其 Consumer ID 写入到 ZK 对应消息分区的临时节点上，例如：`/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]`

由于记录了offset，它的读取速度非常快



# 3. 安装 Kafka

## 3.1 安装包方式

下载 [kafka](http://kafka.apache.org/downloads.html) 并解压

```bash
# 启动 zookeeper
zookeeper-server-start.bat ..\..\config\zookeeper.properties

# 启动 kafka-broker
kafka-server-start.bat ..\..\config\server.properties

# 创建 topic
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic myTopic

# 获取 topic 列表
kafka-topics.bat --list --zookeeper localhost:2181

# 查询 topic 的配置信息
kafka-run-class.bat kafka.admin.TopicCommand --describe --zookeeper localhost:2181 --topic myTopic

# 启动生成者
kafka-console-producer.bat --broker-list localhost:9092 --topic myTopic
> hello kafka

# 启动消费者
kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic myTopic --from-beginning
hello kafka
```



## 3.2 Docker 集群

docker-compose.yml

```yaml
version: '3'

networks:
  kafka_network:
    external: true

services:
  zk1:
    image: confluentinc/cp-zookeeper:5.5.4
    container_name: zk1
    ports:
      - "12181:12181"
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 12181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zk1:12888:13888;zk2:22888:23888;zk3:32888:33888
    volumes:
      - ./zk1/data:/var/lib/zookeeper/data
      - ./zk1/log:/var/lib/zookeeper/log
    networks:
      - kafka_network

  zk2:
    image: confluentinc/cp-zookeeper:5.5.4
    container_name: zk2
    ports:
      - "22181:22181"
    environment:
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_CLIENT_PORT: 22181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zk1:12888:13888;zk2:22888:23888;zk3:32888:33888
    volumes:
      - ./zk2/data:/var/lib/zookeeper/data
      - ./zk2/log:/var/lib/zookeeper/log
    networks:
      - kafka_network

  zk3:
    image: confluentinc/cp-zookeeper:5.5.4
    container_name: zk3
    ports:
      - "32181:32181"
    environment:
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zk1:12888:13888;zk2:22888:23888;zk3:32888:33888
    volumes:
      - ./zk3/data:/var/lib/zookeeper/data
      - ./zk3/log:/var/lib/zookeeper/log
    networks:
      - kafka_network

  kfk1:
    image: confluentinc/cp-kafka:5.5.4
    container_name: kfk1
    ports:
      - "19092:19092"
    expose:
      - "19092"
    depends_on:
      - zk1
      - zk2
      - zk3
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zk1:12181,zk2:22181,zk3:32181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kfk1:19092
    volumes:
      - ./kfk1/data:/var/lib/kafka/data
    networks:
      - kafka_network

  kfk2:
    image: confluentinc/cp-kafka:5.5.4
    container_name: kfk2
    ports:
      - "29092:29092"
    expose:
      - "29092"
    depends_on:
      - zk1
      - zk2
      - zk3
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zk1:12181,zk2:22181,zk3:32181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kfk2:29092
    volumes:
      - ./kfk2/data:/var/lib/kafka/data
    networks:
      - kafka_network

  kfk3:
    image: confluentinc/cp-kafka:5.5.4
    container_name: kfk3
    ports:
      - "39092:39092"
    expose:
      - "39092"
    depends_on:
      - zk1
      - zk2
      - zk3
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zk1:12181,zk2:22181,zk3:32181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kfk3:39092
    volumes:
      - ./kfk3/data:/var/lib/kafka/data
    networks:
      - kafka_network

  kafka-manager:
    image: sheepkiller/kafka-manager:latest
    restart: unless-stopped
    container_name: kafka-manager
    hostname: kafka-manager
    ports:
      - "9000:9000"
    links:
      - kfk1
      - kfk2
      - kfk3
    external_links:
      - zk1
      - zk2
      - zk3
    environment:
      ZK_HOSTS: zk1:12181,zk2:22181,zk3:32181
      TZ: "Asia/Shanghai"
    networks:
      - kafka_network
```

集群管理：

```bash
# 启动集群
$ docker-compose up -d

# 安装 kafkacat
$ sudo apt install kafkacat

# 检查 kfk1 节点状态
$ kafkacat -L -b kfk1:19092
Metadata for all topics (from broker 1: kfk1:19092/1):
 3 brokers:
  broker 2 at kfk2:29092
  broker 3 at kfk3:39092
  broker 1 at kfk1:19092 (controller)
 1 topics:
  topic "__confluent.support.metrics" with 1 partitions:
    partition 0, leader 2, replicas: 2,3,1, isrs: 2,3,1

# 通过 kfk1 向 topic: hello 推送消息
$ kafkacat -P -b kfk1:19092 -t hello
hello, kafka
this is my first message via kafka!

# 通过 kfk3 从 topic: hello 接收消息
$ kafkacat -C -b kfk3:39092 -t hello
% Reached end of topic hello [0] at offset 0
hello, kafka
% Reached end of topic hello [0] at offset 1
this is my first message via kafka!
% Reached end of topic hello [0] at offset 2

# 通过 kfk2 从 topic: hello 接收消息
$ kafkacat -C -b kfk2:29092 -t hello
hello, kafka
this is my first message via kafka!
% Reached end of topic hello [0] at offset 2
go go go....
% Reached end of topic hello [0] at offset 3
```



# 4. 使用 Kafka

```bash
go get github.com/Shopify/sarama
```



## 4.1 生成者

```go
func main() {
	brokerUrls := []string{"192.168.80.250:19092", "192.168.80.250:29092", "192.168.80.250:39092"}

	config := sarama.NewConfig()
	config.Producer.RequiredAcks = sarama.WaitForAll          // 等待leader和所有follower都ACK
	config.Producer.Partitioner = sarama.NewRandomPartitioner // 新选出一个 partition
	config.Producer.Return.Successes = true                   // 成功交付的消息将在 success channel 返回
	config.Producer.Return.Errors = true
	config.Version = sarama.V2_7_0_0

	// 创建 topic
	topicName := "docker_container"
	addTopic(config, brokerUrls[0], topicName)

	// 生产者
	producer, err := sarama.NewAsyncProducer(brokerUrls, config)
	if err != nil {
		log.Fatal(err)
	}
	defer producer.AsyncClose()

	// 发送消息
	key := getRandomString(16)
	for i := 0; i < 1000000; i++ {
		go sendMsg(producer, topicName, key)
	}

	// 中断
	sigterm := make(chan os.Signal, 1)
	signal.Notify(sigterm, os.Interrupt, syscall.SIGTERM)

LOOP:
	for {
		select {
		case success := <-producer.Successes():
			log.Printf("[Success] offset: %d, timestamp: %s, partition: %d",
				success.Offset, success.Timestamp.String(), success.Partition)
		case failure := <-producer.Errors():
			log.Printf("[Failure] %v", failure)
			break LOOP
		case <-sigterm:
			log.Println("terminating: via signal")
			break LOOP
		}
	}
}

func addTopic(cfg *sarama.Config, addr, topicName string) {
	broker := sarama.NewBroker(addr)
	err := broker.Open(cfg)
	if err != nil {
		log.Fatal(err)
	}
	defer broker.Close()

	// 连接状态
	status, err := broker.Connected()
	if err != nil {
		log.Fatal(err)
	}
	log.Printf("broker [%s] connection status: %v", addr, status)

	// topic详情
	topicDetail := &sarama.TopicDetail{
		NumPartitions:     3,
		ReplicationFactor: 1,
		ConfigEntries:     make(map[string]*string),
	}

	topicDetails := make(map[string]*sarama.TopicDetail)
	topicDetails[topicName] = topicDetail

	// 创建topic
	req := &sarama.CreateTopicsRequest{
		Timeout:      15 * time.Second,
		TopicDetails: topicDetails,
	}

	resp, err := broker.CreateTopics(req)
	if err != nil {
		log.Fatal(err)
	}

	for key, val := range resp.TopicErrors {
		log.Printf("Key: %s, Val: %s, Msg: %#v", key, val.Err.Error(), val.ErrMsg)
	}

	log.Println(resp)
}

func sendMsg(producer sarama.AsyncProducer, topicName, key string) {
	randString := getRandomString(64)

	// 构造一个消息
	msg := &sarama.ProducerMessage{
		Topic:     topicName,
		Key:       sarama.StringEncoder(key),
		Value:     sarama.StringEncoder(randString),
		Timestamp: time.Now(),
	}
	producer.Input() <- msg

	//partition, offset, err := producer.SendMessage(msg)
	//if err != nil {
	//	log.Fatal(err)
	//}
	//
	//log.Printf("partition: %d, offset: %d", partition, offset)
}

func getRandomString(n int) string {
	rand.Seed(time.Now().UnixNano())
	randBytes := make([]byte, n/2)
	rand.Read(randBytes)
	return fmt.Sprintf("%x", randBytes)
}
```



## 4.2 消费者

```go
// Consumer 实现 ConsumerGroupHandler 接口
type Consumer struct {
	ready chan bool
}

func (consumer *Consumer) Setup(sarama.ConsumerGroupSession) error {
	close(consumer.ready)
	return nil
}

func (consumer *Consumer) Cleanup(sarama.ConsumerGroupSession) error {
	return nil
}

func (consumer *Consumer) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	for message := range claim.Messages() {
		log.Printf("Message claimed: value = %s, timestamp = %v, topic = %s", string(message.Value), message.Timestamp, message.Topic)
		session.MarkMessage(message, "")
	}
	return nil
}

func main() {
	brokerUrls := []string{"192.168.80.250:19092", "192.168.80.250:29092", "192.168.80.250:39092"}

	config := sarama.NewConfig()
	//config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRoundRobin
	config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategySticky
	config.Version = sarama.V2_7_0_0
	config.Consumer.Offsets.Initial = sarama.OffsetNewest
	config.Consumer.Offsets.AutoCommit.Enable = true
	config.Consumer.Offsets.AutoCommit.Interval = time.Second
	config.Consumer.Offsets.Retention = time.Hour

	topicName := "docker_container"
	groupId := "kafka-group"
	topics := []string{topicName}

	// set up a consumer
	consumer := Consumer{
		ready: make(chan bool),
	}

	ctx, cancel := context.WithCancel(context.Background())
	consumerGroup, err := sarama.NewConsumerGroup(brokerUrls, groupId, config)
	if err != nil {
		log.Fatal(err)
	}

	wg := &sync.WaitGroup{}
	wg.Add(1)

	go func() {
		defer wg.Done()
		for {
			if err := consumerGroup.Consume(ctx, topics, &consumer); err != nil {
				log.Fatal(err)
			}
			if ctx.Err() != nil {
				return
			}
			consumer.ready = make(chan bool)
		}
	}()

	// 等待 consumer 启动
	<-consumer.ready
	log.Println("Sarama consumer up and running...")

	// 中断
	sigterm := make(chan os.Signal, 1)
	signal.Notify(sigterm, os.Interrupt, syscall.SIGTERM)

	select {
	case <-ctx.Done():
		log.Println("terminating: context cancelled")
	case <-sigterm:
		log.Println("terminating: via signal")
	}

	cancel()
	wg.Wait()

	if err = consumerGroup.Close(); err != nil {
		log.Panicf("Error closing client: %v", err)
	}
}
```



