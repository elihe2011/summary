# 1. ServiceMesh

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/servicemesh.png)

Service mesh是一个服务网格的概念。在传统的架构中，服务治理程序和应用程序强耦合在一起，对程序升级和运维带来很多麻烦。service mesh通过sidecar来使服务治理能力独立。上图中绿色和蓝色是一个业务单元，绿色是应用程序，蓝色是专门负责服务治理的程序，比如在Istio中就是envoy。应用的流量出来先导入envoy里面，在envoy里面可以配置服务访问的策略，就可以访问到对应的服务。

特点：

- 治理能力独立 (Sidecar)

- 应用程序无感知
- 服务通信的基础设施层



# 2. EdgeMesh

EdgeMesh主要用来做边缘侧微服务的互访。



## 2.1 架构

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/edgemesh-arch.png)

edgemesh 负责边缘侧流量转发，它实现了CNI接口，支持跨节点流量转发。

- APP 的流量会被导入到 edgemesh 中，edgemesh里面有Listener负责监听；

- Resolver 负责域名解析，里面实现了一个 DNS server；

- Dispather 负责流量转发

- RuleMgr 负责把 endpoint、service、pod的信息通过 MetaManager 从数据库取出来



特点：

- edgemesh-proxy 负责边缘侧流量转发
- 边缘内置域名解析能力，不依赖中心DNS
- 支持 L4, L7流量治理
- 支持跨边和云的一致的服务发现和访问体验
- 使用标准的 istio 进行服务治理控制
- P2P计算跨子网通信



和kube-proxy的对比

- kube-proxy： 需要list-watch service，从而进行服务发现 容器化部署在每个节点(daemonset) service with cluster IP
- edgemesh： 从cloudcore接收service信息，从而进行服务发现 嵌入到edgecore headless service



**为什么域名解析会放到边缘？**

在k8s中，域名解析由coreDNS完成，它一般部署在主节点或某个独立的节点上。但是在边缘计算场景下，边缘与云的连接可能经常断开，这导致域名解析服务不能正常使用。因此需要将域名解析放到边缘上，云上的 service, endpoint, pod信息同步到边缘。



## 2.2 设计原理

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/edgemesh-principle.png)

-  edgemesh通过kubeedge边缘侧list-watch的能力，监听service、endpoints等元数据的增删改，再根据service、endpoints的信息创建iptables规则
- edgemesh使用域名的方式来访问服务，因为fakeIP不会暴露给用户。fakeIP可以理解为clusterIP，每个节点的fakeIp的CIDR都是9.251.0.0/16网段(service网络)
- 当client访问服务的请求到达节点后首先会进入内核的iptables
- edgemesh之前配置的iptables规则会将请求重定向，全部转发到edgemesh进程的40001端口里（数据包从内核台->用户态）
- 请求进入edgemesh程序后，由edgemesh程序完成后端pod的选择（负载均衡在这里发生），然后将请求发到这个pod所在的主机上



## 2.3 流量转发

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/edgemesh-flow.png)

client pod是请求方，service pod是服务方。client pod里面有一个init container，类似于istio的init container。client先把流量打入到init container，init container这边会做一个流量劫持，它会把流量转到edge mesh里面去，edge mesh根据需要进行域名解析后转到对应节点的pod里面去。

优点：init container现在在每一个client pod里面都有一个，而它的功能作用在每一个pod里面都是一样的，后续会考虑把init container接耦出来。



# 3. 工作原理

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/edgemesh-communication-flow.png)

云端是标准的Kubernetes集群，可以使用任意CNI网络插件，比如Flannel、Calico。可以部署任意Kubernetes原生组件，比如kubelet、kube-proxy；同时云端部署KubeEdge云上组件CloudCore，边缘节点上运行KubeEdge边缘组件EdgeCore，完成边缘节点向云上集群的注册。



EdgeMesh 的两个组件：

**EdgeMesh-Server：**

- 运行在云上，具有一个公网IP，监听来自EdgeMesh-Agent的连接请求，并协助EdgeMesh-Agent之间完成UDP打洞，建立P2P连接
- 在EdgeMesh-Agent之间打洞失败的情况下，负责中继EdgeMesh-Agent之间的流量，保证100%的流量中转成功率

**EdgeMesh-Agent：**

- DNS模块：内置的轻量级DNS Server，完成Service域名到ClusterIP的转换。
- Proxy模块：负责集群的Service服务发现与ClusterIP的流量劫持。
- Tunnel模块：在启动时，会建立与EdgeMesh-Server的长连接，在两个边缘节点上的应用需要通信时，会通过EdgeMesh-Server进行UDP打洞，尝试建立P2P连接，一旦连接建立成功，后续两个边缘节点上的流量不需要经过EdgeMesh-Server的中转，进而降低网络时延。



核心优势：

- 跨子网边边/边云服务通信：无论应用部署在云上，还是在不同子网的边缘节点，都能够提供通Kubernetes Service一致的使用体验。

- 低时延：通过UDP打洞，完成EdgeMesh-Agent之间的P2P直连，数据通信无需经过EdgeMesh-Server中转。

- 轻量化：内置DNS Server、EdgeProxy，边缘侧无需依赖CoreDNS、KubeProxy、CNI插件等原生组件。

- 非侵入：使用原生Kubernetes Service定义，无需自定义CRD，无需自定义字段，降低用户使用成本。

- 适用性强：不需要边缘站点具有公网IP，不需要用户搭建VPN，只需要EdgeMesh-Server部署节点具有公网IP且边缘节点可以访问公网。



# 4. 应用场景

## 4.1 子网内边边服务发现与流量转发

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/edgemesh-e2e-intra-subnet.png)

子网内边边服务发现与流量转发是EdgeMesh最先支持的特性，为同一个局域网内的边缘节点上的应用提供服务发现与流量转发能力。

智慧园区是典型的边缘计算场景。在同一个园区内，节点位于同一个子网中，园区中的摄像头、烟雾报警器等端侧设备将数据上传到节点上的应用，节点上的应用需要互相的服务发现与流量转发。这种场景下的用户使用流程如下：

- 如上图所示，EdgeNode1和EdgeNode2位于同一个子网中，用户在EdgeNode1上部署了一个Video Server，用于对外提供摄像头采集上来的视频流，并通过标准的Kubernetes Service形式暴露出来，比如video.cluster.local.service。
- 用户在同一个子网内的EdgeNode2上，通过video.cluster.local.service的形式对该Server发起访问，希望获取视频流信息并进行分析处理。
- 位于EdgeNode2上的EdgeMesh-Agent对域名进行解析，并对该访问进行流量劫持。
- EdgeNode1上的EdgeMesh-Agent与Video Server建立连接， Client与Video Server之间的数据通过EdgeMesh-Agent进行中转，从而获取视频流信息。



## 4.2 跨子网边边服务发现与流量转发

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/edgemesh-e2e-cross-subnet.png)

跨子网边边服务发现与流量转发是EdgeMesh1.8.0版本支持的特性，为位于不同子网内的边缘节点上的应用提供服务发现与流量转发能力。

智慧园区场景中，不同的园区之间通常需要共享一些信息，比如车库停车位数量、视频监控数据等，不同的园区通常位于不同的子网中，因此需要跨子网节点间的服务发现与流量转发能力。这种场景下的用户使用流程如下：

- 如上图所示，EdgeNode1与EdgeNode2位于不同的子网中，用户在EdgeNode1上部署了一个Park Server，用于实时提供园区内停车位的使用情况，并通过标准的Kubernetes Service形式暴露出来，比如park.cluster.local.service。
- EdgeNode2希望可以获取EdgeNode1所在园区的停车位使用情况，从而为车主提供更全面的停车信息。当位于EdgeNode2上的client以service域名的方式发起访问时，流量会被EdgeMesh-Agent劫持，但是因为EdgeNode1与EdgeNode2位于不同的子网中，两个节点上的EdgeMesh-Agent不能够直接建立连接，因此会出现获取停车位使用信息失败的情况。



## 4.3 **跨边云服务发现与流量转发**

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/edgemesh-e2c.png)

跨边云服务发现与流量转发是EdgeMesh1.8.0版本支持的特性，为位于云上和边缘节点上的应用提供服务发现与流量转发能力。下面介绍边访问云的情形，云访问边会遇到和边访问云同样的问题，这里不做赘述。

智慧园区场景中，园区入口需要对访问人员进行人脸识别去决定是否放行，受限于边侧算力，通常会在边侧进行人脸数据的采样，并将采样的数据上传到云端进行运算，因此需要跨边云的服务发现与流量转发。这种场景下的用户使用流程如下：

- 如上图所示，CloudNode1和EdgeNode2分别位于云上和边缘，用户在CloudNode1上部署了一个Face Server，对边侧上报上来的人脸数据进行处理，并返回是否放行的结果，Face Server通过标准的Kubernetes Service形式暴露出来，比如face.cluster.local.service。
- 在边缘侧的EdgeNode2上，用户通过face.cluster.local.service的形式对该Face Server发起访问并上报人脸数据，希望获得是否放行的结果。
- 位于EdgeNode2上的EdgeMesh-Agent对该域名进行解析，并对该访问进行劫持，因为CloudNode1和EdgeNode2位于不同的子网中，两个节点上的EdgeMesh-Agent不能够直接建立连接，因此会出现无法获取是否放行结果的情况。



# 5. 源码分析

最新的 v1.10 版本，通信相关的代码，全部改由 libp2p 库来完成，专注于其本身的逻辑



## 5.1 模块注册

`kubeedge/edgemesh/server/pkg/tunnel/module.go`

```go
// Register register tunnelserver to beehive modules
func Register(c *config.TunnelServerConfig, ifm *informers.Manager) error {
	server, err := newTunnelServer(c, ifm)
	if err != nil {
		return fmt.Errorf("failed to register module tunnelserver: %w", err)
	}
	core.Register(server)
	return nil
}

// TunnelServer is on cloud, as a signal and relay server
type TunnelServer struct {
	Config *config.TunnelServerConfig
	Host   host.Host
}

func newTunnelServer(c *config.TunnelServerConfig, ifm *informers.Manager) (server *TunnelServer, err error) {
	server = &TunnelServer{Config: c}
	if !c.Enable {
		return server, nil
	}
	server.Config.NodeName = util.FetchNodeName()

    // 初始化 TunnelServerController, 即APIConn实例
	controller.Init(ifm)

    // acl
	aclManager := security.NewManager(c.Security)
	aclManager.Start()

	privateKey, err := aclManager.GetPrivateKey()
	if err != nil {
		return server, fmt.Errorf("failed to get private key: %w", err)
	}

	addressFactory := func(addrs []ma.Multiaddr) []ma.Multiaddr {
		for _, advertiseAddress := range c.AdvertiseAddress {
			multiAddr, err := ma.NewMultiaddr(util.GenerateMultiAddr(c.Transport, advertiseAddress, c.ListenPort))
			if err != nil {
				klog.Warningf("New multiaddr err: %v", err)
			}
			// if the multiAddr is existed already, just skip
			existed := false
			for _, addr := range addrs {
				if string(addr.Bytes()) == string(multiAddr.Bytes()) {
					existed = true
					break
				}
			}
			if !existed {
				addrs = append(addrs, multiAddr)
			}
		}
		return addrs
	}

	opts := []libp2p.Option{
		libp2p.ListenAddrStrings(util.GenerateMultiAddr(c.Transport, "0.0.0.0", c.ListenPort)),
		util.GenerateTransportOption(c.Transport),
		libp2p.AddrsFactory(addressFactory),
		libp2p.EnableRelay(circuit.OptHop),
		libp2p.ForceReachabilityPrivate(),
		libp2p.Identity(privateKey),
	}

	if c.Security.Enable {
		if err := libp2ptlsca.EnableCAEncryption(c.Security.TLSCAFile, c.Security.TLSCertFile,
			c.Security.TLSPrivateKeyFile); err != nil {
			return nil, fmt.Errorf("go-libp2p-tls: enable ca encryption err: %w", err)
		}
		opts = append(opts, libp2p.Security(libp2ptlsca.ID, libp2ptlsca.New))
	} else {
		opts = append(opts, libp2p.NoSecurity)
	}

    // 打开 p2p 隧道
	h, err := libp2p.New(context.Background(), opts...)
	if err != nil {
		return nil, fmt.Errorf("failed to start tunnel server: %w", err)
	}

	server.Host = h
	return server, err
}
```



## 5.2 启动模块

```go
// Start tunnelserver
func (t *TunnelServer) Start() {
	t.Run()
}

func (t *TunnelServer) Run() {
	klog.Infoln("Start tunnel server success")
	for _, v := range t.Host.Addrs() {
		klog.Infof("%s : %v/p2p/%s\n", "Tunnel server addr", v, t.Host.ID().Pretty())
	}

    // 连接p2p对端地址
	err := controller.APIConn.SetPeerAddrInfo(constants.ServerAddrName, host.InfoFromHost(t.Host))
	if err != nil {
		klog.Errorf("failed update [%s] addr %v to secret: %v", constants.ServerAddrName, t.Host.Addrs(), err)
		return
	}
	klog.Infof("success update [%s] addr %v to secret", constants.ServerAddrName, t.Host.Addrs())
}

// 初始化TunnelServerController
func Init(ifm *informers.Manager) *TunnelServerController {
	once.Do(func() {
		kubeFactor := ifm.GetKubeFactory()
		APIConn = &TunnelServerController{
			secretInformer: kubeFactor.Core().V1().Secrets().Informer(),
			secretLister:   kubeFactor.Core().V1().Secrets().Lister(),
			secretOperator: ifm.GetKubeClient().CoreV1().Secrets(constants.SecretNamespace),
		}
	})
	return APIConn
}

func (c *TunnelServerController) SetPeerAddrInfo(nodeName string, info *peer.AddrInfo) error {
	peerAddrInfoBytes, err := info.MarshalJSON()
	if err != nil {
		return fmt.Errorf("Marshal node %s peer info err: %v", nodeName, err)
	}

	secret, err := c.secretLister.Secrets(constants.SecretNamespace).Get(constants.SecretName)
	if errors.IsNotFound(err) {
		newSecret := &apicorev1.Secret{
			ObjectMeta: metav1.ObjectMeta{
				Name:      constants.SecretName,
				Namespace: constants.SecretNamespace,
			},
			Data: map[string][]byte{},
		}
		newSecret.Data[nodeName] = peerAddrInfoBytes
		_, err = c.secretOperator.Create(context.Background(), newSecret, metav1.CreateOptions{})
		if err != nil {
			return fmt.Errorf("Create secret %s in %s failed: %v", constants.SecretName, constants.SecretNamespace, err)
		}
		return nil
	}
	if err != nil {
		return fmt.Errorf("Get secret %s in %s failed: %v", constants.SecretName, constants.SecretNamespace, err)
	}

	if secret.Data == nil {
		secret.Data = make(map[string][]byte)
	} else if bytes.Equal(secret.Data[nodeName], peerAddrInfoBytes) {
		return nil
	}

	secret.Data[nodeName] = peerAddrInfoBytes
	secret, err = c.secretOperator.Update(context.Background(), secret, metav1.UpdateOptions{})
	if err != nil {
		return fmt.Errorf("Update secret %v err: %v", secret, err)
	}
	return nil
}
```



# 6. 实例验证

## 6.1 准备操作

云端部署测试pod

```yaml
cat > test-pod.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: alpine-test
spec:
  containers:
    - name: alpine-curl
      image: simplesurance/alpine-curl:4af690a-190222175842
      imagePullPolicy: IfNotPresent
      args:
        - sleep
        - "12000"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/edge
              operator: DoesNotExist
            - key: node-role.kubernetes.io/agent
              operator: DoesNotExist
  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule    
EOF

kubectl apply -f test-pod.yaml
```



## 6.2 HTTP

边缘测，部署应用

```yaml
cat > hostname-edge.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hostname-edge
  labels:
    app: hostname-edge
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hostname-edge
  template:
    metadata:
      labels:
        app: hostname-edge
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/edge
                  operator: Exists
                - key: node-role.kubernetes.io/agent
                  operator: Exists
      containers:
        - name: hostname
          image: poorunga/serve_hostname:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: hostname-svc
spec:
  selector:
    app: hostname-edge
  ports:
    - name: http-0
      port: 12345
      protocol: TCP
      targetPort: 9376
EOF

kubectl apply -f hostname-edge.yaml <<EOF
```



