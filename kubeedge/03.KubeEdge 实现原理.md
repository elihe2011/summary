# 1. 架构

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-arch.png)

## 1.1 云边通信

云端和边缘端的通信，由 CloudHub 和 EdgeHub 两个模块来完成，它们之间通过 websocket 或 quic  通信，相当于建立了一条底层通信隧道，供 k8s 或其他应用通信。

- **CloudHub**: 云端通信模块。websocket服务端，负责监听云端的变化, 缓存和向 EdgeHub 发送消息
- **EdgeHub**: 边缘端通信模块。websocket客户端，负责同步云端的资源更新、报告边缘主机和设备状态变化到云端等功能



## 1.2 边缘端（EdgeCore）

- **Edged**：在边缘节点上运行并管理容器化应用程序的代理，用于管理容器化的应用程序。
- **EventBus**：MQTT 客户端，负责与 MQTT 服务器（mosquitto）交互，为其他组件提供订阅和发布功能。
- **ServiceBus**：运行在边缘的HTTP客户端，接收来自云上服务的请求、和边缘应用进行 http 交互
- **DeviceTwin**: 负责存储设备状态和同步设备状态到云，它还为应用程序提供查询接口
- **MetaManager**: edged 和 edgehub 之间的消息处理器，它负责向轻量级数据库（SQLite）存储/检索元数据。



## 1.3 云端 （CloudCore）

- **EdgeController**：管理edge节点。一种扩展的 Kubernetes 控制器，它管理边缘节点和 Pod元数据
- **DeviceController**:  设备管理。一种扩展的 Kubernetes 控制器，用于管理边缘设备，确保设备元数据、状态等可以在云边之间同步。



# 2. 模块实现

## 2.1 Beehive

Beehive 是 KubeEdge 的通信框架，用于 KubeEdge 模块之间的通信。beehive 实现了两种通信机制：unixsocket 和 go-channel

beehive模块在整个kubeedge中扮演了非常重要的作用，它实现了一套Module管理的接口，程序中各个模块的启动、运行、模块间的通信等都是由其统一封装管理



## 2.2 边缘端

### 2.2.1 入口

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-edgecore.png)

- 初始化的时候，分别加载edge端 modules 的 init 函数，注册 modules 到 beehive 框架中 

- 在 core.Run 中遍历启动 StartModules



### 2.2.2 EdgeHub

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-edgehub.png)

- 重点在启动两个 go routine，实现往两个方向的消息接收和发送

- `go ehc.routeToEdge` 接收cloud端转发至edge端的消息，然后调用`ehc.dispatch` 解析出消息的目标 module 并基于 beehive 模块 module 间消息的通信机制转发出去
- `go ehc.routeToCloud` 将edge端消息转发到CloudHub。另外还实现了对同步消息的response等到超时处理的逻辑：未超时则转发response给消息发送端模块；消息发送失败，该goroutine退出，通知所有模块，当前与 cloud 端未连接状态，然后重新发起连接
- metaManager 在与 cloud 断开期间，使用本地sqlite的数据，不会发往cloud端查询



### 2.2.3 Edged

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-edged.png)

调用kubelet的代码，实现了较多的启动流程。另外，将之前kubelet的client作为fake接口，转而将数据都通过metaClient存储到metaManager，从而代理之前直接访问 api-server 的操作

差异在`e.syncPod`的实现，通过metaManager和EdgeController的pod任务列表，来执行本地pod的操作。同时，这些pod关联的configmap和secret也会随着处理pod的过程一并处理。对pod的操作也是基于一个操作类别的 queue，比如`e.podAddWorkerRun`就启动了一个用于消费加pod的queue的goroutine。外部封装成这样，内部完全通过引用kubelet原生包来处理



### 2.2.4 MetaManager

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-metamanager.png)

- 外层按照一定周期给自己发送消息，触发定时同步pod状态到 cloud 端
- mainLoop 中，启动一个独立的goroutine接收外部消息，并执行处理逻辑，处理逻辑基于消息类型分类：
  - cloud 端发起的增删改查
  - edge 端模块发起的查询请求 （当与cloudhub的连接是disconnect时，本地查询）
  - cloud 端返回的查询响应的结果
  - edgeHub发来的用于更新与cloudHub连接状态的消息
  - 自己发给自己，定期同步edge端pod状态到cloud端的消息
  - 函数式计算相关的消息



### 2.2.5 EventBus

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-eventbus.png)

EventBus 用于对接 MQTT Broker 和 beehive，主要有两种启动模式：

- 使用内嵌 MQTT Broker
- 使用外部 MQTT Broker

在内嵌 MQTT Broker模式下，EventBus 启动了Golang实现的 broker 包 `gomqtt` 作为外部 MQTT 设备的接入，其操作主要有：

- 向 broker 订阅关注的 topic

  ```go
  SubTopics = []string{
  	"$hw/events/upload/#",
  	"$hw/events/device/+/state/update",
  	"$hw/events/device/+/twin/+",
  	"$hw/events/node/+/membership/get",
  	"SYS/dis/upload_records",
  }
  ```

- 当接收到对应的event时，触发回调函数 onSubscribe

- 回调函数中，对event做了简单的分类，分别发到不同的目的地(DeviceTwin或EventHub)

  - `$hw/events/device/+/twin/+` 和`$hw/events/node/+/membership/get`类型的topic的event发送到 DeviceTwin
  - 其他的event直接发送到EventHub再同步到 cloud 端

  

### 2.2.6 ServiceBus

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-servicebus.png)

ServiceBus 启动一个 goroutine 来接收来自 beehive 的消息，然后基于消息中带的参数，通过调用 http client 将消息通过 REST-API 发送到本地地址127.0.0.1上目的APP。它相当于一个客户端，而APP时一个 HTTP Rest-API server，所有的操作和设备状态都需要客户端调用接口来下发和获取



### 2.2.7 DeviceTwin

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-devicetwin.png)

- 数据存储到本地sqlite中，包含三张表：`device`，`deviceAttr`，`deviceTwin`

- 处理其他模块发送到 twin module 的消息，然后调用 `dtc.distributeMsg` 来处理消息。在消息处理逻辑里面，消息被分为四类，分别由对应的action处理
  - membership
  - device
  - communication
  - twin



## 2.3 云端

### 2.3.1 入口

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-cloudcore.png)



### 2.3.2 CloudHub

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-cloudhub.png)

`handler.WebSocketHandler.ServeEvent` 在 websocket server 上接收新边缘节点的连接，并为该节点分配 channel queue，然后将消息交给负责内容读写的逻辑处理

`channelq.NewChannelEventQueue` 为每个边缘节点维护一个对应的 channel queue(默认10个消息缓存)，然后调用`go q.dispatchMessage` 来接收由 controller 发送到 CloudHub 的消息，基于消息内容解析其目的节点，然后将消息发送到节点对应的 channel queue 处理

CloudHub 的核心逻辑：

- `handler.WebSocketHandler.EventWriteLoop` 在channel中读取，并负责通过 ws 隧道发送到对应的节点上，如果节点不存在或 offline将终止发送
-  `handler.WebSocketHandler.EventReadLoop` 从 ws 隧道读取来自边缘节点的消息，然后将消息发送到 controller 模块处理（如果是 keepalive 心跳消息直接忽略）

- 如果 CloudHub发往边缘节点的消息失败，将触发 EventHandler 的 CancelNode 操作；结合EdgeHub端的行为，我们知道EdgeHub会重新发起到Cloud端的新连接，然后重新走一遍同步流程



### 2.3.3 EdgeController

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-edgecontroller.png)

核心逻辑：

- upstream：接收由 beehive 转发的消息，然后基于消息的资源类型，通过 `go uc.dispatchMessage` 转发到不同的 goroutine 处理。其中的操作包括 nodeStatus、podStatus、queryConfigMap、querySecret、queryService、queryEndpoint 等，它们均通过k8s client来调用 api-server 来更新状态
- downstream：通过 k8s client 来监听各种资源的变化，比如pod通过 `dc.podManager.Events` 来读取消息，然后调用`dc.messageLayer.Send` 将消息发送到 edge 端处理。资源包括pod、confimap、secret、node、service、endpoint等



### 2.3.4 DeviceController

![img](https://fastly.jsdelivr.net/gh/elihe2011/bedgraph@master/kubeedge/kubeedge-code-devicecontroller.png)

与 EdgeController 类似，但不关注 k8s 的 workload 的子资源，而是为 device 定义 CRD，即 device 和 deviceModel



