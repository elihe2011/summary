# 1. 简介

MetalLB 在 bare metal 上实现云服务提供商提供的 provider。

**核心功能**：

- 地址分配：为用户的 Load Balancer 类型的 service 分配 IP地址。该IP地址需要用户预选分配
- 外部声明：通知网络中其他主机，支持两种声明模式：
  - Layer 2模式：ARP/NDP
  - BGP 模式



**Layer 2 模式**：

- 每个service会有集群中的一个node来负责，当客户端发起ARP解析时，对应的node会响应该ARP请求，之后该service的流量均指向该node。（看上去该node上有多个地址）
- 不是真正的负载均衡。流量都会先经过一个node，然后通过kube-proxy转给多个 endpoint。如果该node故障，MetalLB会迁移 IP 到另一个 node，并重新发送ARP通知告知客户端迁移。
- 不需要用户有额外的设备，更为通用。但是它使用ARP/ND，地址池分配需要跟客户端在同一个子网，地址分配略显繁琐



**BGP 模式**：

- 所有node都会跟上级路由器建立BGP连接，并会告知路由器应该如何转发service流量
- 是真正的 Load Balancer



# 2. 部署

```bash
# 1. ipvs 开启严格 arp模式
kubectl edit configmap -n kube-system kube-proxy

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
ipvs:
  strictARP: true

# 2. 安装组件
mkdir -p $HOME/metallb && cd $_
wget https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml
wget https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml

kubectl apply -f namespace.yaml
kubectl apply -f metallb.yaml
```

组件说明：

- `metallb-system/controller`：负责IP地址的分配，以及service和endpoint的监听
- `metallb-system/speaker`：负责保证service地址可达，在Layer 2模式下，speaker会负责ARP请求应答



# 3. Layer 2 模式

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/metallb-layer2.png) 

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: config
  namespace: metallb-system
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.70.250-192.168.70.254
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21.4
        ports:
        - name: http
          containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
```

验证结果：

```bash
# 获取
kubectl get svc nginx-service 
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE
nginx-service   LoadBalancer   10.110.31.243   192.168.70.250   80:32373/TCP   41m

# ok
curl 192.168.70.250 

# 需要存在和 192.168.79.250 同网段的主机 IP，否则外面无法访问
ip addr show ens38
3: ens38: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 00:0c:29:b0:f1:b0 brd ff:ff:ff:ff:ff:ff
    inet 192.168.70.100/24 brd 192.168.70.255 scope global ens38
       valid_lft forever preferred_lft forever
    inet6 fe80::20c:29ff:feb0:f1b0/64 scope link
       valid_lft forever preferred_lft forever
```



问题验证：

```bash
$ arping -I ens38 192.168.70.250
ARPING 192.168.70.250
60 bytes from 00:0c:29:37:c3:d2 (192.168.70.250): index=0 time=406.005 usec
60 bytes from 00:0c:29:37:c3:d2 (192.168.70.250): index=1 time=936.153 usec
60 bytes from 00:0c:29:37:c3:d2 (192.168.70.250): index=2 time=709.627 usec

# 查询多个节点上查询mac地址 （在节点k8s-node02上，即该节点为流量入口）
ip addr | grep -B1 -A1 00:0c:29:37:c3:d2
3: ens38: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 00:0c:29:37:c3:d2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.70.102/24 brd 192.168.70.255 scope global ens38
```



# 4. BGP 模式

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/metallb-bgp.png) 

BGP模式不限于一个二层网络里，各个节点都会与交换机建立BGP Peer，宣告Service External IP的下一跳为自身，这样通过ECMP实现了一层负载。客户端请求通过交换机负载到后端某个节点后，再由Kube-proxy进行转发。

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    peers:
    - peer-address: 10.0.0.1
      peer-asn: 64501
      my-asn: 64500
    address-pools:
    - name: default
      protocol: bgp
      addresses:
      - 198.51.100.0/24
      bgp-advertisements:
      - aggregation-length: 32
        localpref: 100
        communities:
        - no-advertise
      - aggregation-length: 24
    bgp-communities:
      no-advertise: 65535:65282
```



# 5. IP地址共享

默认情况下，MetalLB只会将一个IP地址分配到一个LoadBalancer Service上，用户可以通过`spec.loadBalancerIP`来指定自己想用的IP，如果用户指定了已被分配了的IP会，则会报错。但MetalLB也提供了方式去支持多个Service共享相同的IP，主要为了解决：K8S不支持对LoadBalancer Service中的Port指定多协议；有限的IP地址资源。

具体的方式是：创建两个Service，并加上`metallb.universe.tf/allow-shared-ip`为Key的`annotation`，表明Service能容忍使用共享的LoadBalancerIP；然后通过`spec.loadBalancerIP`给两个Service指定共享的IP。

IP地址共享也有限制：

1）两个Service的`metallb.universe.tf/allow-shared-ip`值是一样的。

2）两个Service的“端口”（带协议）不同，比如`tcp/53`和`udp/53`是属于不同的“端口”。

3）两个Service对应的后端Pod要一致，如果不一致，那么他们的externalTrafficPolicy需要都是Cluster，不然会无法进行正确的BGP。
