# 1. 卷

Kubernetes 中卷的本质是目录，给 pod 中的容器使用，至于卷的类型，它不关心。

它解决的问题：

- 容器磁盘上的文件生命周期是短暂的，容器重启后，之前新增的文件将丢失，即以干净的初始状态启动
- pod 中多个容器可能需要共享文件



生命周期：

- 容器重启，volume 数据还在
- Pod重启，volume 数据可能丢失



常见的 volume 类型：

- emptyDir: 初始内容为空的卷
- hostPath: 挂载宿主机文件或目录
- nfs: 挂载 NFS 卷
- cephfs：挂载 CephFS 卷
- secret：使用一个 secret 为 pod 提供加密信息
- configMap: 使用一个 configMap 为 pod 提供配置信息
- downwardAPI：用于使向下 API 数据（downward API data）对应用程序可用。它挂载一个目录，并将请求的数据写入纯文本文件



Pod 使用存储卷：

- **spec.volumes**: 声明存储卷
- **spec.containers.volumeMounts**: 使用存储卷



## 1.1 emptyDir

创建 Pod 时，会自动创建 `emptyDir`  卷，Pod 中的容器可以在该数据卷中进行文件的写入和读取操作。当删除 Pod 时，emptyDir将自动删除。容器崩溃不会导致 Pod 被删除，因此 emptyDir 卷中的数据在容器崩溃时是安全的。

**用途**：

- 暂存空间，多个容器可共享
- 用于长时间计算崩溃恢复时的检查点
- Web服务器容器提供数据时，保存内容管理容器提取的文件



示例：Pod 中目录共享

```yaml
# volume-emptyDir.yml
apiVersion: v1 
kind: Pod 
metadata:
  name: emptydir-pod 
spec:
  containers:
  - image: busybox
    name: c1
    command: ["sleep", "86400"]
    volumeMounts:
    - mountPath: /path1 
      name: cache-volume
  - image: busybox
    name: c2
    command: ["sleep", "86400"]
    volumeMounts:
    - mountPath: /path2
      name: cache-volume
  volumes:
  - name: cache-volume 
    emptyDir: {}
```

验证：

```bash
$ kubectl exec -it emptydir-pod -c c1 -- touch /path1/abc.txt

$ kubectl exec -it emptydir-pod -c c2 -- ls /path2
abc.txt
```



## 1.2 hostPath

挂载宿主机文件系统到Pod中，挂载类型检查：

| 值                | 行为                                                      |
| ----------------- | --------------------------------------------------------- |
| “”                | 空字符串(默认)，挂载时不做任何检查                        |
| DirectoryOrCreate | 目录不存在自动创建，权限0755，与kubectl具有相同组和所有权 |
| Directory         | 目录必须存在                                              |
| FileOrCreate      | 文件不存在自动创建，权限0644，与kubectl具有相同组和所有权 |
| File              | 文件必须存在                                              |
| Socket            | Unix 套接字必须存在                                       |
| CharDevice        | 字符设备必须存在                                          |
| BlockDevice       | 块设备必须存在                                            |

示例：挂载宿主机目录到Pod

```yaml
# volume-hostPath.yml
apiVersion: v1 
kind: Pod 
metadata:
  name: hostpath-pod
spec:
  containers:
  - image: busybox
    name: busybox
    command: ["sleep", "86400"]
    volumeMounts:
    - name: data-volume
      mountPath: /data  
  volumes:
  - name: data-volume 
    hostPath: 
      path: /data
      type: Directory
```

验证：

```bash
$ kubectl exec -it hostpath-pod -- touch /data/abc.txt

$ kubectl get pod hostpath-pod -o wide
NAME           READY   STATUS    RESTARTS   AGE    IP            NODE         NOMINATED NODE   READINESS GATES
hostpath-pod   1/1     Running   0          114s   10.244.2.13   k8s-node02   <none>           <none>

# k8s-node02
$ ls -l /data
total 0
-rw-r--r-- 1 root root 0 Feb 18 13:28 abc.txt
```



## 1.3 nfs

将现有的 NFS（网络文件系统）共享挂载到您的容器中

```yaml
# nginx-nfs.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: data
          mountPath: /usr/share/nginx/html
      volumes:
      - name: data
        nfs:
          path: /mnt/nfs_share
          server: 192.168.3.103
```

验证：

```bash
# nfs服务器
$ echo '<h1>Hello World!</h1>' > /mnt/nfs_share/index.html

# 获取 nginx 应用
$ kubectl get pod -l app=nginx
NAME                    READY   STATUS    RESTARTS   AGE
nginx-6fd47969f-bhskh   1/1     Running   0          4m7s
nginx-6fd47969f-mt7ch   1/1     Running   0          4m7s
nginx-6fd47969f-nq4dg   1/1     Running   0          4m7s

# 端口转发
$ kubectl port-forward --address 127.0.0.1 pod/nginx-6fd47969f-bhskh 8081:80 &

$ curl 127.0.0.1:8081
Handling connection for 8081
<h1>Hello World!</h1>
```



## 1.4 ConfigMap

```yaml
# volume-configmap.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-config
data:
  log_level: "INFO"
  ui.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice
---
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: busybox
      image: busybox
      command: ["sleep", "86400"]
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level
          - key: ui.properties
            path: ui.properties
```

验证：

```bash
$ kubectl exec -it configmap-pod -- ls -l /etc/config
total 0
lrwxrwxrwx    1 root     root            16 Feb 18 06:27 log_level -> ..data/log_level
lrwxrwxrwx    1 root     root            20 Feb 18 06:27 ui.properties -> ..data/ui.properties

$ kubectl exec -it configmap-pod -- cat /etc/config/log_level
INFO

$ kubectl exec -it configmap-pod -- cat /etc/config/ui.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice
```



## 1.5 DownwardAPI

downwardAPI 支持获取 Pod 自身相关信息

```yaml
# volume-downwardApi.pod
apiVersion: v1
kind: Pod
metadata:
  name: dwapi-pod
  labels:
    app: dnsutils
    version: 1.1
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/scheme: "http"
    prometheus.io/port: "80"
spec:
  containers:
    - name: dnsutils
      image: e2eteam/dnsutils:1.1
      command: ["sh", "-c"]
      args:
      - while true; do
          if [[ -e /etc/podinfo/labels ]]; then
            echo -en '\n\n'; cat /etc/podinfo/labels; fi;
          if [[ -e /etc/podinfo/annotations ]]; then
            echo -en '\n\n'; cat /etc/podinfo/annotations; fi;
          sleep 5;
        done;
      volumeMounts:
        - name: podinfo
          mountPath: /etc/podinfo
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations
```

验证：

```bash
$ kubectl logs dwapi-pod
app="dnsutils"
version="1.1"

kubectl.kubernetes.io/last-applied-configuration="{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"prometheus.io/port\":\"80\",\"prometheus.io/scheme\":\"http\",\"prometheus.io/scrape\":\"true\"},\"labels\":{\"app\":\"dnsutils\",\"version\":\"1.1\"},\"name\":\"dwapi-pod\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"args\":[\"while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/annotations; fi; sleep 5; done;\"],\"command\":[\"sh\",\"-c\"],\"image\":\"e2eteam/dnsutils:1.1\",\"name\":\"dnsutils\",\"volumeMounts\":[{\"mountPath\":\"/etc/podinfo\",\"name\":\"podinfo\"}]}],\"volumes\":[{\"downwardAPI\":{\"items\":[{\"fieldRef\":{\"fieldPath\":\"metadata.labels\"},\"path\":\"labels\"},{\"fieldRef\":{\"fieldPath\":\"metadata.annotations\"},\"path\":\"annotations\"}]},\"name\":\"podinfo\"}]}}\n"
kubernetes.io/config.seen="2022-02-18T13:51:25.433545445+08:00"
kubernetes.io/config.source="api"
prometheus.io/port="80"
prometheus.io/scheme="http"
```



# 2. 持久卷

集群中，通常不使用 `emptyDir` 和 `hostPath`，一般只在测试环境中使用。

**`PersistentVolume`**：持久化存储卷，是对底层共享存储的一种抽象。共享存储被定义为**一种集群级别的资源**，不属于任何 Namespace，用户使用 PV 需要通过 PVC 申请。PV 由管理员进行创建和配置的，与底层的共享存储技术实现方式有关，比如 Ceph， Gluster FS， NFS等，都是通过插件机制完成与共享存储的对接，且不同存储的PV配置参数也不同

**`PersistentVolumeClaim`**：用户申请存储资源，它属于某**一个 Namespace 的资源**。

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-pv-pvc-bound.png)

总结：

- PV 针对不同的共享存储，使用不同的配置，屏蔽它们之间的差异
- PVC 寻找和是的 PV 进行绑定



## 2.1 PV

PersistentVolume 的类型实现为插件，目前 Kubernetes 支持以下插件：

- RBD：Ceph 块存储
- FC：光纤存储设备
- NFS：网络数据存储卷
- iSCSI：iSCSI 存储设备
- CephFS：开源共享存储系统
- Glusterfs：一种开源共享存储系统。
- HostPath：宿主机目录，仅能用于单机
- ...



**PV 的生命周期**：

- Available：可用状态，尚未被 PVC 绑定
- Bound： 绑定状态，已被 PVC 绑定
- Failed：删除 PVC 清理资源，自动回收卷失败
- Released: 绑定的 PVC 已被删除，但资源尚未被集群回收

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-pv-life-cycle.png)



**PV 类型**: 

- 静态 PV：集群管理员创建的 PV，等待 PVC 消费
- 动态 PV：当管理员创建的静态 PV 都不匹配用户的 PersistentVolumeClaim时，集群可能会尝试动态地为 PVC创建卷。此配置基于`StorageClasses` （PVC必须请求存储类），并且管理员必须创建并配置该类才能够尽兴动态创建。



创建 NFS 类型 PV：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
  label:
    app: nfs
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.2
  nfs:
    path: /mnt/nfs_share
    server: 192.168.3.103
```



配置参数说明：

- `volumeMode`:

  - Filesystem: 文件系统，默认选项
  - Block: 块设备。仅有FC、iSCSI、RBD 等支持

- `accessModes`:

  - RWO, ReadWriteOnce：单节点读写模式

  - ROX, ReadOnlyMany：多节点只读模式

  - RWX, ReadWriteMany：多节点读写模式

- storageClassName: 存储类名称。如果设置了它，PVC也必须做相同的设置才能匹配绑定

- persistentVolumeReclaimPolicy: 回收策略

  - Retain：保留数据，由管理手动清理

  - Recycle：删除数据，`rm -rf /thevolume/*`，目前只有 NFS 和 HostPath 支持

  - Delete：删除存储资源，仅部分云存储支持，如AWS EBS、GCE PD、Azure Disk 等



## 2.2 PVC

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 5Gi
  storageClassName: slow
  selector:
    matchLabels:
      app: nfs
```

PVC 匹配 PV：

- storage 大小筛选
- storageClassName 存储类型筛选
- accessModes 访问类型筛选
- selector 选择器筛选，一般根据 label 等



## 2.3 StorageClass

**Static Provisioning**：PV 由存储管理员创建，开发操作PVC，但大规模集群中，存储管理员为满足开发的需求，要手动创建很多个PV，管理起来相当繁琐。

**Dynamic Provisioning**：通过 StorageClass创建一个PV模板，在创建 PVC 时指定 StorageClass，与 StorageClass 关联的存储插件会自动创建对应的 PV 与该 PVC 进行绑定

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: nfs-client
mountOptions: 
  - hard
  - nfsvers=4.2
parameters:
  archiveOnDelete: "true"
```

参数说明：

- provisioner: 存储分配提供者，如果集群中没有先创建它，那么创建的 `StorageClass` 只能作为标记，而不能提供创建 `PV` 的作用

- archiveOnDelete：删除 PV 后是否保留数据
- `storageclass.kubernetes.io/is-default-class: "true"`: 创建 PVC 时如果未指定 StorageClass 则会使用默认的 StorageClass



### 2.3.1 存储类延迟绑定

**Step 1**：准备存储

节点 k8s-node01 上操作

```bash
$ mkdir -p /data/nginx
$ echo '<h1>hello storage class</h1>' > /data/nginx/index.html
```

**Step 2**: 创建 SC, PV, PVC

```yaml
# delay-bound-storage.yml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /data/nginx
  nodeAffinity: # local 类型需要设置节点亲和
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - k8s-node01
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-storage
```

**Step 3**: 检查存储状态

```bash
$  kubectl get storageclasses
NAME            PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-storage   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  79s

$ kubectl get pv nginx-pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
nginx-pv   5Gi        RWO            Delete           Available           local-storage            101s

$ kubectl get pvc nginx-pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nginx-pvc   Pending                                      local-storage   106s

$ kubectl describe pvc nginx-pvc
Name:          nginx-pvc
Namespace:     default
StorageClass:  local-storage
Status:        Pending
Volume:
Labels:        <none>
Annotations:   <none>
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Used By:       <none>
Events:
  Type    Reason                Age                 From                         Message
  ----    ------                ----                ----                         -------
  Normal  WaitForFirstConsumer  5s (x10 over 2m6s)  persistentvolume-controller  waiting for first consumer to be created before binding
```

**Step 4**: 创建应用

```yaml
# delay-bound-nginx.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      appname: nginx
  template:
    metadata:
      name: nginx
      labels:
        appname: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        volumeMounts:
          - name: data
            mountPath : /usr/share/nginx/html
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: nginx-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080
EOF
```

**Step 5**: 验证

```bash
# 已绑定
$ kubectl get pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nginx-pvc   Bound    nginx-pv   5Gi        RWO            local-storage   19m

# 本地卷挂载成功
$ curl 192.168.80.100:30080
<h1>hello storage class</h1>
```



### 2.3.2 动态存储分配(NFS)

https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner



#### 2.3.2.1 创建 RBAC

```yaml
# nfs-rbac.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```



#### 2.3.2.2 部署 NFS Provisioner

```yaml
# nfs-provisioner-deploy.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.3.103
            - name: NFS_PATH
              value: /mnt/nfs_share
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.3.103
            path: /mnt/nfs_share
```



#### 2.3.2.3 创建 StorageClass

```yaml
# nfs-storageclass.yml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"
```



#### 2.3.2.4 部署 StatefulSet 应用

```go
# nfs-nginx-deploy.yml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 1
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "nfs-storage"
      resources:
        requests:
          storage: 1Gi
```



#### 2.3.2.5 验证

```bash
# 应用已启动
$ kubectl get pod
NAME                                     READY   STATUS              RESTARTS   AGE
nfs-client-provisioner-5444cbbb6-jjv2l   1/1     Running             0          20m
web-0                                    1/1     Running             0          2m39s
web-1                                    1/1     Running             0          16s
web-2                                    0/1     ContainerCreating   0          8s

# PV 自动创建
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS          REASON   AGE
pvc-4e8413aa-51bd-4d74-bb88-0856b974394f   1Gi        RWO            Delete           Bound    default/data-web-0       nfs-storage                    32s
pvc-ab9ba91d-4cbc-4dbd-a977-b1d1aa63b047   1Gi        RWO            Delete           Bound    default/data-web-2       nfs-storage                    11s
pvc-c6e64d66-33c5-4e79-8e0c-20db8f003731   1Gi        RWO            Delete           Bound    default/data-web-1       nfs-storage                    19s

# PVC 自动绑定
$ kubectl get pvc
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-web-0   Bound    pvc-4e8413aa-51bd-4d74-bb88-0856b974394f   1Gi        RWO            nfs-storage    2m44s
data-web-1   Bound    pvc-c6e64d66-33c5-4e79-8e0c-20db8f003731   1Gi        RWO            nfs-storage    21s
data-web-2   Bound    pvc-ab9ba91d-4cbc-4dbd-a977-b1d1aa63b047   1Gi        RWO            nfs-storage    13s
```



# 3. 系统架构

## 3.1 Process of mounting volume

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-mount-volume.png)

- **Step 1:** The user creates a pod containing a PVC.
- **Step 2:** The PV controller constantly observes the API server. If it finds that a PVC has been created but not been bound, it tries to bind a persistent volume (PV) to the PVC.

The PV controller tries to find a suitable PV in the cluster. If no suitable PV is found, the volume plug-in is called for provisioning. Provisioning creates a volume from a specific remote storage medium, creates a PV in the cluster, and then binds the PV to the PVC.

- **Step 3:** The scheduler performs scheduling.

When a pod is running, the scheduler must select a node. The scheduler performs scheduling based on multiple references, such as nodeSelector and nodeAffinity defined in the pod and some tags defined in the volume.

You can add some tags to the volume. In this way, the pod using this PV will be scheduled by the scheduler to the expected node based on the tags.

- **Step 4:** If the PV defined by a pod has not been attached after the pod is scheduled to a node, the AD controller calls the volume plug-in to mount the remote volume to the device (for example, /dev/vdb) on the target node.
- **Step 5:** When the volume manager finds that a pod has been scheduled to its node and the volume has been mounted, it mounts the local device (/dev/vdb) to a subdirectory of the pod on the node. It may also perform some additional operations, such as formatting and mounting to GlobalPath.
- **Step 6:** Map the locally mounted volume to containers in the pod.



## 3.2 Storage Architecture

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-arch.png)

- **PV controller:** binds PVs and PVCs, manages their lifecycles, and creates or deletes volumes as required.
- **AD controller:** attaches storage devices to and detaches storage devices from target nodes.
- **Volume manager:** manages the mount and unmount operations of volumes, formats volume devices, and mounts volumes to common directories.
- **Volume plug-in:** implements all the mounting functions.

The PV controller, AD controller, and volume manager call operations, while volume plug-ins implement the operations.

- **Scheduler:** schedules pods and performs storage-related scheduling according to certain storage-related definitions.



### 3.2.1 PV Controller

The following describes several basic concepts:

- **PV**

- **PVC**

- **StorageClass**



![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-pv-controller.png)



The following figure shows how to select a PV to be bound to a PVC.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-pv-bound-pvc.png)



The ClaimWorker implements PVC status change.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-claimworker.png)



The VolumeWorker implements PV status changes.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-volumeworker.png)



### 3.2.2 AD Controller

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-ad-controller.png)

The name AD controller is short for the attach and detach controller.

It has two core objects: desiredStateOfWorld and actualStateOfWorld.

- DesiredStateofWorld indicates the volume mounting status to be achieved in a cluster.
- ActualStateOfWorld indicates the actual volume mounting status in a cluster.

The AD controller has two core logic parameters, desiredStateOfWorldPopulator and reconciler.

- desiredStateOfWorldPopulator synchronizes some data in a cluster and DSW and ASW data updates. For example, when we create a PVC or a pod in a cluster, it synchronizes the status of the PVC or pod to the DSW.
- reconciler synchronizes DSW and ASW statuses. It changes the ASW status to the DSW status. During this status change, it performs operations such as attach and detach.

The following table provides an example of desiredStateOfWorld and actualStateOfWorld objects.

- desiredStateOfWorld defines each worker, including the volumes contained in a worker and mounting information.
- actualStateOfWorl defines all volumes, including the target node of each volume and the mounting status.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-dsw-asw.png)



The following figure shows a logical diagram of AD controller implementation.

The AD controller contains many informers, which synchronize the pod statuses, PV statuses, node statuses, and PVC statuses in the cluster to the local machine.

During initialization, populateDesireStateOfWorld and populateActualStateOfWorld are called to initialize desiredStateOfWorld and actualStateOfWorld.

During execution, desiredStateOfWorldPopulator synchronizes data statuses in the cluster to desireStateofWorld. reconciler synchronizes the data of actualStateOfWorld and desiredStateOfWorld in polling mode. During synchronization, it calls the volume plug-in for attach and detach operations and also calls nodeStatusUpdater to update the node status.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-dsw-asw-flow.png)



### 3.2.3 Volume Manager

The volume manager is one of the managers in the kubelet. It is used to attach, detach, mount, and unmount volumes on the local node.

Like the AD controller, it contains desiredStateOfWorld and actualStateOfWorld. It also contains volumePluginManager, which manages plug-ins on nodes. Its core logic is similar to that of the AD controller. It synchronizes data through desiredStateOfWorldPopulator and calls APIs through reconciler.

The following describes the attach and detach operations:

As mentioned above, the AD controller also performs the attach and detach operations. We can use the `--enable-controller-attach-detach` tag to specify whether the AD controller or the volume manager performs the operations. If the value is True, the AD controller performs the operations. If the value is False, the volume manager performs the operations.

This is a tag of the kubelet, which can only define the behavior of a node. Therefore, if a cluster contains 10 nodes and the tag is defined as False on five nodes, the kubelet on the five nodes performs the attach and detach operations while the AD controller on the other five nodes performs these operations.

The following figure shows the implementation logic of the volume manager.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-volume-manager.png)

The outermost layer is a loop while the inner layer is a poll based on different objects, including the desiredStateOfWorld and actualStateOfWorld objects.

For example, the actualStateOfWorld.MountedVolumes object is polled. If a volume of the object also exists in desiredStateOfWorld, the actual and desired volumes are mounted. Therefore, we do not have to do anything. If the volume does not exist in desiredStateOfWorld, the volume is expected to be in the Umounted state. In this case, run UnmountVolume to change its status to that in desiredStateOfWorld.

In this process, underlying APIs are called to perform the corresponding operations based on the comparison between desiredStateOfWorld and actualStateOfWorld. The desiredStateOfWorld.UnmountVolumes and actualStateOfWorld.AttachedVolumes operations are similar.



### 3.2.4 Volume Plug-ins

The PV controller, AD controller, and volume manager mentioned above manage PVs and PVCs by calling volume plug-in APIs, such as Provision, Delete, Attach, and Detach. The implementation logic of these APIs is in volume plug-ins.

Volume plug-ins can be divided into the In-Tree and Out-of-Tree plug-ins based on the source code locations.

- In-Tree indicates that the source code is placed inside Kubernetes and released, managed, and iterated with Kubernetes. However, the iteration speed is slow and the flexibility is poor.
- Out-of-Tree indicates that the source code is independent of Kubernetes, which is provided and implemented by the storage provider. Currently, the main implementation mechanisms are FlexVolume and Container Storage Interface (CSI), which can implement different storage plug-ins based on storage types. Therefore, we prefer Out-of-Tree.

Volume plug-ins are the libraries called by the PV controller, AD controller, and volume manager. They are divided into In-Tree and Out-of-Tree plug-ins. The volume plug-in calls remote storage based on these implementations and mounts a remote storage to a local device. For example, the `mount -t nfs ***` command for mounting a NAS file system is implemented in a volume plug-in.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-volume-plugins.png)

Volume plug-ins can be divided into many types. In-Tree plug-ins contain dozens of common storage implementations. However, some companies define their own types and have their own APIs and parameters, which are not supported by common storage plug-ins. In this case, Out-of-Tree storage implementations need to be used, such as CSI and FlexVolume.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-intree-outoftree.png)

We will discuss the specific implementations of volume plug-ins later. Here, let's take a look at the management of volume plug-ins.

Kubernetes manages plug-ins in the PV controller, AD controller, and volume manager by using VolumePlguinMg, mainly with the plug-ins and prober data structures.

Plugins is an object used to save the plug-in list, while prober is a probe used to discover new plug-ins. For example, FlexVolume and CSI are extension plug-ins, which are dynamically created and generated and can be discovered only by a probe.

The following figure shows the entire process of plug-in management.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-volume-plugins-manage.png)





# 4. FlexVolume 

FlexVolume is an extension of volume plug-ins. It implements the attach, detach, mount, and unmount operations. These operations were originally implemented by volume plug-ins. However, for some storage types, we need to extend them and implement the operations outside volume plug-ins.

FlexVolume is a kubelet-driven executable file. Each call is equivalent to running the Is script of shell. It is not a resident memory daemon.

Stdout of FlexVolume is a call result of the kubelet in JSON format.

The default storage address on FlexVolume is /usr/libexec/kubernetes/kubelet-plugins/volume/exec/alicloud~disk/disk.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-flexvolume.png)

The following figure shows a command format and call example.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-flexvolume-cmd.png)

## 4.1 Introduction to FlexVolume APIs

FlexVolume provides the following APIs:

- **init:** performs initialization operations. For example, if initialization operations are performed during plug-in deployment or update, the data structure of the DriveCapabilities type is returned to describe the features of the FlexVolume plug-in.
- **GetVolumeName:** returns the plug-in name.
- **Attach:** implements the attach function. The --enable-controller-attach-detach tag determines whether the AD controller or the kubelet initiates the attach operation.
- **WaitforAttach:** waits for the completion of the asynchronous attach operation before continuing other operations.
- **MountDevice:** is part of the mount operation. In this example, the mount operation is divided into MountDevice and SetUp. MountDevice performs simple preprocessing, such as formatting the device and mounting it to the GlobalMount directory.
- **GetPath:** obtains the local mounting directory for each pod.
- **Setup:** binds devices in GlobalPath to the local directory of the pod.
- **TearDown**, **UnmountDevice**, and **Detach:** implement the inverse processes of some of the preceding APIs.
- **ExpandVolumeDevice:** expands volumes. It is called by the expand controller.
- **NodeExpand:** expands the file system. It is called by the kubelet.

Not all of the preceding operations must be implemented. If an operation is not implemented, define the result as follows to notify the callers that the operation was not implemented:

```
{
    "status": "Not supported",
    "message": "error message"
}
```

In addition to serving as a proxy, the API provided by FlexVolume in a volume plug-in also provides some default implementations, such as the mount operation. Therefore, if this API is not defined in your FlexVolume, the default implementation will be called.

When defining a PV, we can use the secretRef field to define certain secret functions. For example, secretRef can transfer in the username and password required for mounting.

## 4.2 Analysis of FlexVolume Mounting

Let's take a look into the mounting and unmounting processes of FlexVolume.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-flexvolume-mount.png)

First, the Attach operation calls a remote API to attach the storage to a device on the target node. Then, the MountDevice operation mounts the local device to the GlobalPath and performs certain operations, such as formatting. The Mount operation (SetUp) mounts the GlobalPath to the PodPath, which is a directory mapped upon pod startup.

For example, assume the volume ID of a disk is d-8vb4fflsonz21h31cmss. After the Attach and WaitForAttach operations are completed, it is mounted to the /dec/vdc directory on the target node. After the MountDevice operation is performed, the device is formatted and mounted to a local GlobalPath. After the Mount operation is performed, the GlobalPath is mapped to a pod-related subdirectory. Finally, the Bind operation maps the local directory to the pod. This completes the mounting process.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-flexvolume-mount-process.png)

The unmounting process is an inverse process. The preceding describes the process of mounting a block device. Only the Mount operation needs to be performed for file storage. Therefore, only the Mount and Unmount operations need to be performed to implement FlexVolume for the file system.

## 4.3 FlexVolume Code Example

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-flexvolume-code.png)

The init(), doMount(), and doUnmount() methods are implemented. When the script is executed, the input parameters determine the command to be run.

GitHub provides many FlexVolume examples for your reference. Alibaba Cloud provides a [FlexVolume Implementation](https://github.com/AliyunContainerService/flexvolume) for reference.

## 4.4 Use of FlexVolume

The following figure shows a PV template of the FlexVolume type. It is similar to other templates, except that the type is defined as flexVolume. In flexVolume, driver, fsType, and options are defined.

- driver defines an implemented driver, such as the alicloud/disk driver in the figure or the alicloud/nas driver.
- fsType defines the file system type, such as ext4.
- options contains specific parameters, such as volumeId.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-flexvolume-use.png)

The following figure shows a detailed running result. A disk in /dev/vdb is attached to the pod. We can run the mount | grep disk command to view the mounting directory. It mounts /dev/vdb to the GlobalPath, runs the mount command to mount the GlobalPath to the local subdirectory defined in the pod, and then maps the local subdirectory to /data.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-flexvolume-use-result.png)



# 5. CSI

Similar to FlexVolume, CSI is an abstract interface that provides volumes for third-party storage.

Why is CSI required since we can use FlexVolume?

FlexVolume is only used by the Kubernetes orchestration system, while CSI can be used by different orchestration systems, such as Mesos and Swarm.

In addition, CSI adopts container-based deployment, which is less environment-dependent but more secure and provides rich plug-in functions. FlexVolume is a binary file in the host space. Running FlexVolume is equivalent to running a local shell command, which allows us to install some dependencies when installing FlexVolume. These dependencies may affect customer applications. Therefore, this hurts security and environment dependencies.

When implementing operators in the Kubernetes ecosystem, we often use role-based access control (RBAC) to call Kubernetes APIs to implement certain functions in containers. However, these functions cannot be implemented in the FlexVolume environment because it is a binary program in host space. These functions can be implemented by RBAC for CSI.

CSI includes the CSI controller server and the CSI node server.

- The CSI controller server provides the create, delete, attach, and detach functions on the control side.
- The CSI node server provides the mount and unmount functions on nodes.

The following figure shows the CSI communication process. The CSI controller server and external CSI sidecar communicate with each other through a UNIX Socket, while the CSI node server and the kubelet communicate with each other through a UNIX Socket.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-communication.png)

The following table lists the CSI APIs. The APIs are divided into general management APIs, node management APIs, and central management APIs.

- General management APIs return general information about CSI, such as the plug-in names, driver identity, and plug-in capabilities.
- The NodeStageVolume and NodeUnstageVolume node management APIs are equivalent to MountDevice and UnmountDevice in FlexVolume. The NodePublishVolume and NodeUnpublishVolume APIs are equivalent to the SetUp and TearDown APIs.
- The CreateVolume and DeleteVolume central management APIs create and delete volumes, while the ControllerPublishVolume and ControllerUnPublishVolume APIs are used for attach and detach operations.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-func.png)



## 5.1 CSI System Structure

CSI is implemented in the form of CRD. Therefore, it introduces the following object types: VolumeAttachment, CSINode, CSIDriver, and implementation of the CSI controller server and CSI node server.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-system-structure.png)

The AD controller and volume plug-in in the CSI controller server are similar to those in Kubernetes, and they create the VolumeAttachment object.

In addition, the CSI controller server contains multiple external plug-ins, each implementing a certain function when combined with a CSI plug-in. For example:

- External-provisioner and controller server are combined to create and delete volumes.
- External-attacher and the controller server are combined to mount and unmount volumes.
- External-resizer and controller server are combined to scale out volumes.
- External-snapshotter and controller server are combined to create and delete snapshots.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-combined.png)

The CSI node server contains kubelet components, including the volume manager and volume plug-in. The components call CSI plug-ins for mount and unmount operations. The driver registrar component implements the registration function for CSI plug-ins.

This is the overall topology of CSI. The following sections describe different objects and components.

## 5.2 CSI Objects

Now, we will introduce the VolumeAttachment, CSIDriver, and CSINode objects.

VolumeAttachment describes information about mounting and unmounting a volume in a pod. For example, if a volume is mounted to a node, we use VolumeAttachment to track it. The AD controller creates a VolumeAttachment, while external-attacher mounts or unmounts volumes according to the status of the VolumeAttachment.

The following figure shows an example of a VolumeAttachment. For the VolumeAttachment, kind is set to VolumeAttachment. In spec, attacher is set to ossplugin.csi.alibabacloud.com, specifying the operator of mounting, nodeName is set to cn-zhangjiakou.192.168.1.53, specifying the mounting node, and persistentVolumeName is set to oss-csi-pv for source, specifying the mounting and unmounting volume.

In status, attached specifies the attachment status. If the value is false, external-attacher performs an attach operation.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-attach-operation.png)

CSIDriver describes the list of CSI plug-ins deployed in the cluster. These plug-ins must be created by the administrator based on the plug-in type.

For example, some CSIDrivers are created, as shown in the following figure. We can run the kuberctl get csidriver command to view the three types of CSIDrivers: disk, Apsara File Storage NAS (NAS), and Object Storage Service (OSS).

The CSIDriver name is defined, and the attachRequired and podInfoOnMount tags are defined in spec.

- attachRequired specifies whether a plug-in supports the attach function. This is used to distinguish block storage from file storage. For example, if the Attach operation is not required for file storage, we can set the tag to false.
- podInfoOnMount defines whether Kubernetes carries pod information when calling the mount API.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-driver.png)

CSINode is the node information in a cluster and is created upon node-driver-registrar startup. CSINode information is added to the CSINode list each time a new CSI plug-in is registered.

As shown in the following figure, the CSINode list is defined, and each CSINode has its specific information (YAML file on the left). Take cn-zhangjiakou.192.168.1.49 as an example. It contains the disk CSIDriver and the NAS CSIDriver. Each CSIDriver has its nodeID and topologyKeys. If no topology information is available, set topologyKeys to null. In this way, if a cluster contains 10 nodes, we can define CSINodes only for certain nodes.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-node.png)



## 5.3 Node-driver-registar

Node-driver-registrar implements CSI plug-in registration, as shown in the following figure.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-node-driver-register.png)

- **Step 1:** A convention is specified upon startup. For example, adding a file to the /var/lib/kuberlet/plugins_registry directory is equivalent to adding a plug-in.

After being started, node-driver-registrar calls the GetPluginInfo API of the CSI plug-in. Then, the API returns the listening address of CSI and a driver name of the CSI plug-in.

- **Step 2:** The node-driver-registrar listens to the GetInfo and NotifyRegistrationStatus APIs.
- **Step 3:** A socket in the `/var/lib/kuberlet/plugins_registry` directory is started to generate a socket file, such as diskplugin.csi.alibabacloud.com-reg.sock. The kubelet discovers this socket through the watcher and then calls the GetInfo API of the node-driver-registrar through this socket. GetInfo returns the obtained CSI plug-in information to the kubelet, including the listening address of the CSI plug-in and the driver name.
- **Step 4:** The kubelet calls the NodeGetInfo API of the CSI plug-in based on the obtained listening address.
- **Step 5:** After calling the API, the kubelet updates some status information, such as the annotations, tags, and status.allocatable of the node, and creates a CSINode object.
- **Step 6:** By calling the NotifyRegistrationStatus API of the node-driver-registrar, the kubelet notifies us that the CSI plug-in is registered.

The preceding steps complete CSI plug-in registration.

## 5.4 External-attacher

External-attacher calls APIs of the CSI plug-in to mount and unmount volumes. It mounts or unmounts volumes according to the status of the VolumeAttachment. The AD controller calls the CSI attacher in the volume plug-in to create the VolumeAttachment. The CSI attacher is an In-Tree API implemented by Kubernetes.

When the VolumeAttachment status is False, external-attacher calls the attach function at the underlying layer. If the desired value is False, the detach function is implemented through the ControllerPublishVolume API. External-attacher also synchronizes certain PV information.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-external-attacher.png)

### CSI Deployment

This section describes the deployment of block storage.

As mentioned earlier, the CSI controller is divided into the CSI controller server and the CSI node server.

Only one CSI controller server needs to be deployed. For multiple backups, two CSI controller servers can be deployed. The CSI controller server is implemented by multiple external plug-ins. For example, multiple external containers and a container containing the CSI controller server can be defined in a pod. In this case, different external components are combined with the CSI controller server to provide different functions.

The CSI node server is a DaemonSet, which is registered on each node. The kubelet directly communicates with the CSI node server through a socket and calls the attach, detach, mount, and unmount methods.

The driver registrar only provides the registration function and is deployed on each node.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-dev-block-storage.png)

The deployment for file storage is similar to that for block storage, except that no external-attacher or VolumeAttachment is available.

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-csi-dev-file-storage.png)



# 6. 总结

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-storage-summary.png)
