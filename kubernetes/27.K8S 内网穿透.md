# 1. 内网穿透

**Intranet Penetration**

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-osi.png) 

- 方案一：在L7 **应用层**，使用**kubectl自带的子命令**（proxy port-forward）打通部分服务；
- 方案二：在L3 **网络层**，使用**自定义路由**无缝打通Kubernetes集群内网；
- 方案三：在L4/L5+ **传输/应用层**，使用**Socks5代理**基本打通Kubernetes集群内网；
- 方案四：在L2/L3 **数据链路/网络层**，实用Kubernetes集群内搭建的**VPN服务器**实现无缝打通集群内网。



# 2. 方案详解

## 2.1 方案一：`kubectl proxy & port-forward` (L7)

**`kubectl proxy`:**

proxy需要token才能调用 `API Server`的接口，不能比较透明地打通某个服务，所以很少使用

`kubectl proxy --port PORT`: 开启监听

代理地址：`http://localhost:8001/api/v1/namespaces/${ns}/services/${schema}:${service_name}:${port}/proxy/`

```bash
$ kubectl proxy --port 8001 &

$ curl http://localhost:8001/api/v1/namespaces/default/services/http:nginx-svc:80/proxy/
```



**`kubectl port-forward`**:

支持在本地监听某个端口，本地流量通过网络隧道到达某个Pod的端口

`kubectl port-forward pods/${pod-name} 8080:8080 -n ${ns}`

```bash
$ kubectl port-forward pods/nginx-7848d4b86f-hcdd4 80:80 &
Forwarding from 127.0.0.1:80 -> 80

$ curl http://localhost
```



**适用场景**：

- 联调时调用个别的HTTP服务、简单的TCP服务，以及**无需查找集群内DNS**的场景。

**优点：**

- 原生解决方案，只需执行kubectl命令即可
- Port Forward 具体哪些Pod的权限可通过RBAC机制控制，比较安全

**缺点：**

- 需要每个使用者都执行命令，而且每个组件都要Forward一下，不方便；
- Pod发生改变时需要重新执行kubectl来Forward
- 对于一些需要打通Kubernetes DNS的TCP服务，或者直连同样的内网IP的场景不适用。比如Redis Cluster， Kafka，MongoDB等。Forward集群的MongoDB到本地，本地连接到localhost:27017是MongoDB的Primary节点，服务端返回了一个 `mongo-secondary.mongo.cluster.local` 的域名让客户端去连，客户端将无法识别



## 2.2 方案二：静态路由 (L3)

如果Kubernetes集群就部署在局域网内或者部署在自己的数据中心，整个链路上的**网关可配**的话，用**静态路由表**是最简单的办法，其原理是作用在网络模型的第三层 **网络层**，直接告诉网关某些IP要发给谁。

例如，某开发环境的Kubernetes部署在和办公室同一个局域网，有下面两条线路可以打通网络：

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-tunnel-static-route.png)

在网关路由器上添加**静态路由规则**，把属于Kubernetes的**Pod/Service CIDR**的IP包全转给其中某个Kubernetes节点，这样访问10.96.0.1这样的IP，网络包会到达某个集群物理节点，而集群内的物理节点或VM，一般K8S网络插件(CNI)都会做与Pod/Service CIDR的互通。

如果Kubernetes和本地机器处于**同一个网关**下，甚至仅在**本地机器上**添加一条静态路由到路由表即可：

```bash
# Windows 
route ADD 10.96.0.0 MASK 255.240.0.0 192.168.1.20

# Linux
sudo ip route add 10.96.0.0/12 via 192.168.1.20 dev eth0

# MacOS
sudo route -n add -net 10.96.0.0 -netmask 255.240.0.0 192.168.1.20

$ kubectl get svc nginx-svc
NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
nginx-svc   ClusterIP   10.96.84.125   <none>        80/TCP    54m

$ curl http://10.96.84.125
```

如果Kubernetes部署的机器和公司办公室**不在同一个网关下**，或者部署在自建数据中心的，整个链路会多几个网关，链路上**每个网关**都需要配置路由表路由相应的CIDR到相邻的跃点：

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-static-route2.png)

有了第三层的路由支持，**DNS的问题**也迎刃而解了，直接把Kubernetes的DNS Server（如CoreDNS），当作本地DNS服务器，或者插入到DNS查找链中作为一个Forwarder即可。这样整个局域网已经与Kubernetes内网完全互联互通了。

**适用场景：**

- 网关配置可以修改，比如都在公司局域网内，或集群所在数据中心的网关路由器是可修改的；
- 内部的开发环境Kubernetes，不担心敏感数据等安全问题。

**优点：**

- 方便，无需部署任何组件，仅在一个或多个网关上做静态路由配置即可，透明、高效；
- 对开发测试人员几乎完全透明，不需要任何额外操作。

**缺点：**

- 需要负责网络的IT人员额外配置；
- Pod/Service网段不能和本地局域网的网段有冲突，多个Kubernetes集群之间也不能有CIDR冲突，否则不好配置路由表；
- 除了一些局域网或自建DC中的Kubernetes，大部分情况下可能用的是**云服务**，我们没办法修改云服务商的路由表，此方案很难实现，就要用方案三和四了。



## 2.3 方案三：Shadowsocks打通容器网络 (L4/L5+)

通过在Kubernetes内网搭建一个”间谍”服务，客户端连到这个服务建立一个虚拟的隧道，让局域网的部分网络流量通过这个专属的隧道打到Kubernetes内网中，感觉就像在Kubernetes内网一样。方案三的Shadowsocks与方案四的VPN两个方案不同的地方在于，前者在L4/L5而后者一般作用在L2/L3

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-tunnel-ss.png)

- 首先本机通过RFC 1928定义的标准的[Socks5协议](https://www.ietf.org/rfc/rfc1928.txt)代理TCP连接的流量；
- 然后SS Client和SS Server通过加密通信把流量丢给SS Server；
- SS Server最终发送这些TCP包到目标机器，再原路返回回去，通过两重转发实现“代理”的目的。

**Step 1: 部署 Shadowsocks Server**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shadowsocks
  namespace: default
  labels:
    app: ss-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ss-server
  template:
    metadata:
      labels:
        app: ss-server
    spec:
      containers:
      - name: ss-server
        image: shadowsocks/shadowsocks-libev:v3.3.5
        command: [
          # need to modify "-d" to k8s core-dns
          "ss-server", "-p", "8388", "-t", "300", "-k", "mypassword", "-m", "aes-256-cfb", "--fast-open", "-d", "10.96.0.10,8.8.8.8", "-u"
        ]
        ports:
        - containerPort: 8388
          protocol: TCP
          name: tcp
        - containerPort: 8388
          protocol: UDP
          name: udp
      nodeSelector:
        kubernetes.io/os: linux
---
apiVersion: v1
kind: Service
metadata:
  name: socks-proxy-svc
  namespace: default
spec:
  type: NodePort
  ports:
  - port: 8388
    targetPort: 8388
    nodePort: 32088
    protocol: TCP
    name: tcp
  - port: 8388
    targetPort: 8388
    nodePort: 32088
    protocol: UDP
    name: udp
  selector:
    app: ss-server
```

**Step 2: 安装 Shadowsocks 客户端**

https://github.com/shadowsocks/shadowsocks-windows/releases

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-tunnel-ss-client.png)



**Step 3: 浏览器安装 SwitchyOmega 插件**

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-tunnel-ss-switchyomega.png)



**Step 4: 访问服务**

```bash
$ kubectl get svc nginx-svc
NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
nginx-svc   ClusterIP   10.96.84.125   <none>        80/TCP    90m
```

访问地址：

http://10.96.84.125

http://nginx-svc

http://nginx-svc.default.svc



浏览器和一些应用软件，也可以读取操作系统的设置作为默认值，但仍然**不能确保**所有进程的TCP流量，都走Sock5代理。

比如Java进程的**JVM启动参数**如果不加：**-DsocksProxyHost和-DsocksProxyPort** 两个参数，并不一定会走Socks5代理。这时有个更彻底的办法，用Proxifier**强制某些进程**的流量全部走Sock5代理，传送门：https://www.proxifier.com/download/。Proxifier是个非常好用的工具，在更底层拦截了网络流量转到代理服务器中，这里就不扯远了。

**适用场景：**

- 想**几乎**无缝透明的访问集群的任何内部Pod/Service IP和Domain，定向透传到Kubernetes集群内的流量；
- Kubernetes集群在云服务器上，有公网IP可以当作SS Server服务器IP。

**优点：**

- 服务端方案比较轻量，维护相对简单；
- 代理开关方便，客户端比较灵活；
- 按需代理，不影响大多数网络流量，即使有瓶颈扩容也很方便。

**缺点：**

- 客户端初次使用可能稍微有些麻烦；
- 虽然浏览器或程序用代理能使用集群内部DNS，但即使开了代理，本地直接nslookup解析Kubernetes内部域名也不通，**DNS问题是间接解决的**。



## 2.4 方案四：VPN 打通容器网络 (L2/L3)

VPN是在远程办公场景时常用的方案，借用VPN的思路打通Kubernetes内网也可以实现。常用的VPN有两类，作用于网络模型的L2或L3：

- L2TP（Layer Two Tunneling Protocol）：主要是作用于第二层，支持非IP网络，开销稍高，搭配第三层的IPSec协议可以做隧道验证，密钥协商和流量的加密；
- PPTP（Point to Point Tunneling Protocol）：点对点隧道协议， 使用第三层的GRE（Generic Routing Eneapsulation）建立隧道，以及第二层的点对点协议传输数据。

在Kubernetes环境中部署VPN服务器实现办公室局域网到Pod/Service网络的互通，网络上也有一些教程。有个Github项目是在容器环境下搭建IPSec VPN的：https://github.com/hwdsl2/docker-ipsec-vpn-server，把运行容器命令改成Apply Deployment/StatefulSet Yaml，或者以在某个Kubernetes节点上以Host Network的模式运行容器，也相当于在Kubernetes集群中放了一个“间谍”，搭好服务端之后，客户端用操作系统自带的VPN工具连进去即可。

```bash
docker run \
    --name ipsec-vpn-server \
    --restart=always \
    -v ikev2-vpn-data:/etc/ipsec.d \
    -v /lib/modules:/lib/modules:ro \
    -p 500:500/udp \
    -p 4500:4500/udp \
    -d --privileged \
    hwdsl2/ipsec-vpn-server
```



**Step 1: 修改SVC的默认端口范围**

NodePort默认端口范围为 30000~32676，VPN服务的端口：

- 点对点隧道协议（PPTP） 1723 

- 第2层隧道协议（L2TP） 500 

为方便远程客户端连接，需要修改Service的端口范围

```bash
$ vi /etc/kubernetes/manifests/kube-apiserver.yaml
...
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --service-node-port-range=80-32767
    image: registry.aliyuncs.com/google_containers/kube-apiserver:v1.21.4
...
```

Static Pod 的配置文件被修改后，立即生效。

- Kubelet 会监听该文件的变化，当修改了配置文件后，kubelet 将自动终止原有的 kube-apiserver-{nodename} 的 Pod，并自动创建一个使用了新配置参数的 Pod 作为替代。
- 如果您有多个 Kubernetes Master 节点，您需要在每一个 Master 节点上都修改该文件，并使各节点上的参数保持一致。



**Step 2: 部署VPN服务**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ipsec-vpn-server
  namespace: default
  labels:
    app: vpn-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vpn-server
  template:
    metadata:
      labels:
        app: vpn-server
    spec:
      containers:
      - name: vpn-server
        image: hwdsl2/ipsec-vpn-server
        ports:
        - containerPort: 500
          protocol: UDP
          name: p1
        - containerPort: 4500
          protocol: UDP
          name: p2
        securityContext:
          privileged: true
        env:
        - name: VPN_IPSEC_PSK
          value: "bKgg9nRvU2xCxnHbB2Tc"
        - name: VPN_USER
          value: "vpnuser"
        - name: VPN_PASSWORD
          value: "NJZiT3BrA5YTHHpF"
        volumeMounts:
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
      volumes:
      - name: lib-modules
        hostPath:
          path: /lib/modules
---
apiVersion: v1
kind: Service
metadata:
  name: vpn-server-svc
  namespace: default
spec:
  type: NodePort
  ports:
  - port: 500
    targetPort: 500
    nodePort: 500
    protocol: UDP
    name: p1
  - port: 4500
    targetPort: 4500
    nodePort: 4500
    protocol: UDP
    name: p2
  selector:
    app: vpn-server
```



**Step 3: 配置VPN客户端**

**解决WIN10 vpn 连接错误 "无法建立计算机与VPN服务器之间的网络连接,因为远程服务器未响应"，** L2TP/ipsec VPN两种连接方式的注意事项：

- **使用证书连接方式**：添加ProhibitIpSec值为0

  `计算机\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\RasMan\Parameters`

  ![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-vpn-regedit1.png)

- **使用共享密钥方式**：添加一个AssumeUDPEncapsulationContextOnSendRule值为2

  `计算机\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\PolicyAgent`

  ![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-vpn-regedit2.png)

**修改完成后，重启电脑**



**Windows PowerShell 命令来创建 VPN 连接**：

```bash
Add-VpnConnection -Name 'k8s-vpn' -ServerAddress '192.168.3.103' -L2tpPsk 'bKgg9nRvU2xCxnHbB2Tc' -TunnelType L2tp -EncryptionLevel Required -AuthenticationMethod Chap,MSChapv2 -Force -RememberCredential -PassThru
```

**设置账号密码**：windows配置 =》网络和 Internet =》VPN =》k8s-vpn =》高级选项

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-net-vpn-edit.png)

**连接VPN**：windows配置 =》网络和 Internet =》VPN =》k8s-vpn =》连接



**Step 4: 访问服务**

```bash
$ kubectl get svc nginx-svc
NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
nginx-svc   ClusterIP   10.96.84.125   <none>        80/TCP    90m
```

访问地址：

http://10.96.84.125

http://nginx-svc

http://nginx-svc.default.svc



**适用场景：**

- 需要完全**无缝透明地**访问Kubernetes的Pod/Service内部网络。

**优点：**

- 操作系统自带VPN功能，客户端比较简单，只需要开关VPN即可实现透明的网络互通了；
- 作用于网络模型更底层，能实现完全透传。

**缺点：**

- VPN打开会影响所有网络流量，导致大量不需要走Kubernetes的流量走到集群内了；
- VPN的实现相对比较重，效率可能不如其他方案。



# 3. 总结

方案一：最简单，推荐使用 port forward 方案，无法解决问题时，再考虑其他方案

方案二：需要静态路由，DNS解析链条配置好，对使用端几乎完全透明，也没有任何性能损耗

方案三和四：集群内部放一个“间谍”（SS Server 、 VPN Server），网络流量从代理服务器发出去。VPN的管理复杂一些，没有IP和域名级别细粒度的流量控制，可能导致无关流量都涌进来成为瓶颈；Socks代理对使用端稍微麻烦一些，不是完全的透传，但效率和灵活度更高。



参考资料：

https://code2life.top/2019/04/15/0037-k8s-tunnel/

https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/docs/clients-zh.md

https://me.jinchuang.org/archives/381.html
