# 1. 简介

Cilium 是一个基于 eBPF 和 XDP 的高性能容器网络方案的开源项目，目标是为微服务环境提供网络、负载均衡、安全功能，主要定位是容器平台。

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-arch.png)

微服务网络高频率更新带来的问题：

- iptables 规则被频繁更新
- IP地址存在时间可能短短的几秒钟，难以提供精确的可视化追踪痕迹

Cilium 通过利用 BPF 具有能够透明的注入网络安全策略并实施的功能，区别于传统的IP地址标识方式，Cilium 是基于 service/pod/container 标识来实现，并可以在应用层 实现 L7 Policy 网络过滤。

Cilium 通过解耦 IP 地址，不仅可以在高频变化的微服务环境中应用简单的网络安全策略，还能支持 L3/L4 基础上通过多 http 层进行操作来提供更强大的网络安全隔离。



## 1.1 主要功能特性

- 支持 L3/L4/L7 安全策略

  - 基于身份的安全策略
  - 基于 CIDR 的安全策略
  - 基于标签的安全策略

- 支持三层扁平网络，支持如下网络模型

  - Overlay 网络，主要支持Vxlan和Geneve
  - Native Routing, 使用Linux常规路由或云服务商的高级网络路由

- 提供基于 BPF 的负载均衡

- 提供便利的监控手段和排错能力，除传统的 tcpdump 和 ping 命令，还提供

  - 元数据监控：当一个Packet包丢失，不止会报告源Ip和目的IP，还会提供发送放和接收方所有相关的标签信息
  - 决策追踪：为什么一个packet 包被丢弃，为何一个请求被拒绝？策略准则框架允许追踪正在允许的工作负载和基于任意标签定义的策略决策过程
  - 通过 Prometheus 暴露 Metrics 指标
  - Hubble：Cilium 专有可视化平台，可通过 flow log来提供微服务间的依赖关系，监控告警及应用服务安全策略可视化




## 1.2 组件

- Cilium Agent: 最核心组件，通过DaemonSet方式，以特权容器模型，运行在集群的每个主机上。其主要作用：
  - 作为用户空间守护程序，通过插件与容器运行时和容器编排系统进行交互，为本机上的容器进行网络即安全的相关配置
  - 提供开放API，共其他组件调用
  - 在进行网络和安全相关配置时，采用eBPF程序进行实现。Cilium Agent结合容器标识和相关策略，生成eBPF程序，并将eBPF程序编译成字节码，传递到Linux内核。
- Cilium Operator： 负责管理集群中的任务，尽可能的保证以集群为单位，而不是单独的以节点为单位进行任务处理。通过etcd为节点之间的通信资源信息，确保 Pod 的 DNS 可以被 Cilium 管理、集群 NetworkPolicy 的管理和更新等。
- Cilium CLI: 命令行工具，提供创建和管理cilium网络等多种功能



## 1.3 组网模式

- VXLAN 的 Overlay 网络 （默认）
- BGP 路由， 实现集群间 Pod 的组网和互联
- AWS 的 ENI (Elastic Network Interfaces) 模式下部署和使用 Cilium
- 基于 ipvlan 组网，而不是默认的 veth
- Cluster Mesh 组网，实现跨多个 kubernetes 集群的网络连通和安全性等多种组网





# 2. 安装

## 2.1 预检查

```bash
# 删除其他网络插件卸载残留
rm -rf /etc/cni/
rm -rf /var/lib/cni

# 检查是否已挂载 bpf
$ mount |grep bpf
none on /sys/fs/bpf type bpf (rw,nosuid,nodev,noexec,relatime,mode=700)

# 未挂载，则先挂载
mount bpffs /sys/fs/bpf -t bpf
```



## 2.2 通过 cilium 安装

有些小问题

```bash
wget https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz
tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin

cilium install --config cluster-pool-ipv4-cidr=10.244.0.0/16

cilium status --wait

cilium config set cluster-pool-ipv4-cidr 10.244.0.0/16
```



## 2.3 通过 helm 安装

### 2.3.1 安装 helm
```bash
wget https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz 
tar zxvf helm-v3.1.2-linux-amd64.tar.gz
mv linux-amd64/helm /usr/local/bin/
```

### 2.3.2 安装 cilium
```bash
helm repo add cilium https://helm.cilium.io/
helm install cilium cilium/cilium --version 1.10.4 \
   --namespace kube-system\
   --set nativeRoutingCIDR=10.244.0.0/16 \
   --set ipam.operator.clusterPoolIPv4PodCIDR=10.244.0.0/16 \
   --set ipam.operator.clusterPoolIPv4MaskSize=24
   
# 校验安装
$ helm ls -A
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
cilium  kube-system     1               2021-10-13 13:59:58.23181352 +0800 CST  deployed        cilium-1.10.4   1.10.4

$ cilium status --wait
    /¯¯\
 /¯¯\__/¯¯\    Cilium:         OK
 \__/¯¯\__/    Operator:       OK
 /¯¯\__/¯¯\    Hubble:         disabled
 \__/¯¯\__/    ClusterMesh:    disabled
    \__/

DaemonSet         cilium             Desired: 3, Ready: 3/3, Available: 3/3
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium             Running: 3
                  cilium-operator    Running: 2
Cluster Pods:     0/0 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: 3
                  cilium-operator    quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: 2
  
# 连通性测试
$ cilium connectivity test
  
# 服务列表
$ kubectl exec -it -n kube-system  cilium-5nxfq  -- cilium service list
Defaulted container "cilium-agent" out of: cilium-agent, mount-cgroup (init), clean-cilium-state (init)
ID   Frontend            Service Type   Backend
1    10.96.184.59:8080   ClusterIP      1 => 10.244.1.248:8080
2    10.96.0.1:443       ClusterIP      1 => 192.168.80.240:6443
3    10.96.24.122:8080   ClusterIP      1 => 10.244.0.64:8080
4    10.96.34.228:80     ClusterIP      1 => 10.244.0.93:4245
5    10.96.98.23:80      ClusterIP      1 => 10.244.1.87:8081
6    10.96.0.2:53        ClusterIP      1 => 10.244.2.43:53
7    10.96.0.2:9153      ClusterIP      1 => 10.244.2.43:9153
```

### 2.3.3 安装 hubble
Hubble是建立在Cilium和eBPF之上，以一种完全透明的方式，提供网络基础设施通信以及应用行为的深度可视化，是一个应用于云原生工作负载，完全分布式的网络和安全可观察性平台。

```bash 
helm upgrade cilium cilium/cilium --version 1.10.4 \
   --namespace kube-system \
   --reuse-values \
   --set hubble.listenAddress=":4244" \
   --set hubble.relay.enabled=true \
   --set hubble.ui.enabled=true
  
# hubble-ui 服务改为 NodePort 方式
kubectl patch svc hubble-ui -p '{"spec": {"type": "NodePort"}}' -n kube-system

$ kubectl -n kube-system get svc hubble-ui
NAME        TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
hubble-ui   NodePort   10.96.98.23   <none>        80:45584/TCP   24m
```

访问hubble-ui页面: `http://192.168.80.240:45584`

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-hubble-ui.png)



另一个测试：

```bash
$ wget https://raw.githubusercontent.com/cilium/cilium/HEAD/examples/kubernetes/connectivity-check/connectivity-check.yaml
$ kubectl apply -f connectivity-check.yaml

$ kubectl get pod
NAME                                                     READY   STATUS    RESTARTS   AGE
echo-a-c4cdff77c-99xdf                                   1/1     Running   0          2m12s
echo-b-598c78b9fc-2ptg5                                  1/1     Running   0          2m12s
echo-b-host-5556f9488-59l9t                              1/1     Running   0          2m12s
host-to-b-multi-node-clusterip-7f8c9699d6-m5wwz          1/1     Running   0          2m12s
host-to-b-multi-node-headless-7df4d6fdb-bmvwf            1/1     Running   1          2m11s
pod-to-a-679f686cb-khnj2                                 1/1     Running   0          2m12s
pod-to-a-allowed-cnp-54755cc9c6-mql9b                    1/1     Running   0          2m12s
pod-to-a-denied-cnp-7bfb7d69b8-ntspk                     1/1     Running   0          2m12s
pod-to-b-intra-node-nodeport-6bd8d9468d-bqcw7            1/1     Running   0          2m11s
pod-to-b-multi-node-clusterip-7984ccf8c6-d9rp8           1/1     Running   0          2m12s
pod-to-b-multi-node-headless-7fb5f5c84b-cnjnk            1/1     Running   0          2m12s
pod-to-b-multi-node-nodeport-54c89d778-79lsb             1/1     Running   0          2m11s
pod-to-external-1111-c98db84d4-tnd6p                     0/1     Running   1          2m12s
pod-to-external-fqdn-allow-google-cnp-5f55d8886b-vsnk4   0/1     Running   1          2m12s
```

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-hubble-ui-2.png)



# 3. 通信解析

## 3.1 网络接口

安装完成后，cilium agent 会在 Node 上创建四个虚拟网络接口：

- cilum_vxlan：跨主机通信，处理对数据包的vxlan隧道操作，采取metadata模式，不会为它分配ip地址
- cilium_host：主机上该子网的一个网关，配置器管理的 CIDR IP
- cilium_net： 和  cilium_host 是一对 veth pair
- lxc_health

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-flow.png)

```bash
# agent 创建网络接口
17: cilium_net@cilium_host: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 72:3f:22:a7:f4:7e brd ff:ff:ff:ff:ff:ff
    inet6 fe80::703f:22ff:fea7:f47e/64 scope link
       valid_lft forever preferred_lft forever
18: cilium_host@cilium_net: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 5e:4b:33:9a:26:aa brd ff:ff:ff:ff:ff:ff
    inet 10.244.1.229/32 scope link cilium_host
       valid_lft forever preferred_lft forever
    inet6 fe80::f410:7eff:feef:e0c0/64 scope link
       valid_lft forever preferred_lft forever
19: cilium_vxlan: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 06:28:96:2c:fd:84 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::428:96ff:fe2c:fd84/64 scope link
       valid_lft forever preferred_lft forever
27: lxc_health@if76: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether ea:67:31:b5:6b:a9 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::e867:31ff:feb5:6ba9/64 scope link
       valid_lft forever preferred_lft forever
 
# 路由走 cilium_host 接口
$ kubectl exec -it busybox-deploy-6d65884477-5f6nw -- route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.244.1.229    0.0.0.0         UG    0      0        0 eth0
10.244.1.229    0.0.0.0         255.255.255.255 UH    0      0        0 eth0

# agent 隧道信息
$ kubectl -n kube-system exec -it cilium-5nxfq -- cilium bpf tunnel list
TUNNEL         VALUE
10.244.2.0:0   192.168.80.240:0
10.244.0.0:0   192.168.80.242:0

# agent endpoint信息
$ kubectl -n kube-system exec -it cilium-5nxfq -- cilium bpf endpoint list
IP ADDRESS         LOCAL ENDPOINT INFO
10.244.1.222:0     id=3885  flags=0x0000 ifindex=89  mac=5E:D6:4C:90:A3:ED nodemac=AA:B9:5B:82:52:69
10.244.1.34:0      id=3633  flags=0x0000 ifindex=77  mac=22:A1:9E:A6:A1:2F nodemac=EA:67:31:B5:6B:A9
10.244.1.39:0      id=187   flags=0x0000 ifindex=101 mac=06:D9:0C:51:2C:33 nodemac=96:AC:56:C5:73:A4
10.244.1.248:0     id=967   flags=0x0000 ifindex=83  mac=5A:BE:09:B2:47:6E nodemac=96:C9:7F:CE:E2:5B
10.244.1.40:0      id=3238  flags=0x0000 ifindex=95  mac=B2:35:5A:2D:50:10 nodemac=4A:B3:FA:ED:0D:4B
10.244.1.87:0      id=3235  flags=0x0000 ifindex=85  mac=6E:0F:48:CA:64:6B nodemac=02:15:6D:12:A6:4F
10.244.1.229:0     (localhost)
192.168.80.241:0   (localhost)
10.244.1.19:0      id=63    flags=0x0000 ifindex=91  mac=22:7B:08:24:93:C2 nodemac=96:E2:3C:6B:A9:2F
10.244.1.2:0       id=910   flags=0x0000 ifindex=81  mac=EE:2C:A4:0C:CB:FD nodemac=8A:34:FA:19:57:A7
10.244.1.245:0     id=1947  flags=0x0000 ifindex=87  mac=3E:22:5B:DF:F2:D5 nodemac=B2:9F:D6:74:E8:35
10.244.1.67:0      id=1167  flags=0x0000 ifindex=99  mac=2A:34:53:ED:23:F6 nodemac=CA:6C:75:43:28:FF
10.244.1.205:0     id=1116  flags=0x0000 ifindex=93  mac=26:CB:3D:CA:4E:BE nodemac=26:A0:AE:86:7B:54
10.244.1.125:0     id=2250  flags=0x0000 ifindex=97  mac=3A:9F:D9:DB:64:E6 nodemac=E2:C5:AC:A5:FA:62
10.244.1.168:0     id=1642  flags=0x0000 ifindex=79  mac=CE:2E:53:87:E0:22 nodemac=32:94:23:F9:9E:5F

# 路由信息
# ip route
default via 192.168.80.2 dev ens33 proto static
10.244.0.0/24 via 10.244.1.229 dev cilium_host src 10.244.1.229 mtu 1450
10.244.1.0/24 via 10.244.1.229 dev cilium_host src 10.244.1.229
10.244.1.229 dev cilium_host scope link
10.244.2.0/24 via 10.244.1.229 dev cilium_host src 10.244.1.229 mtu 1450
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.80.0/24 dev ens33 proto kernel scope link src 192.168.80.241
```



## 3.2 主机内通信

同一个主机内，两个Pod间通信，数据通路：

Pod1 --> eth0 --> lxc909734ef58f7 --> lxc7c0fcdd49dd0 --> eth0 --> Pod2

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-vxlan-in-host.png)



## 3.3 跨主机通信

跨主机，两个Pod的通信，需要通过vxlan实现隧道封装，数据路径：

pod1 --> eth0 --> lxc909734ef58f7 --> cilium_vxlan --> eth0(node-161) --> eth0(node-162) --> cilium_vxlan --> lxc2df34a40a888 --> eth0 --> pod3

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-vxlan-cross-host.png)

node01 上对 `cilium_vxlan` 抓包，可看到 CA 容器对 icmp 包经过了 `cilium_vxlan`：

```bash
$ tcpdump -i cilium_vxlan icmp -n -vv
tcpdump: listening on cilium_vxlan, link-type EN10MB (Ethernet), capture size 262144 bytes
15:47:19.628640 IP (tos 0x0, ttl 64, id 62338, offset 0, flags [DF], proto ICMP (1), length 36)
    10.244.0.81 > 10.244.1.34: ICMP echo request, id 3284, seq 64511, length 16
15:47:19.631570 IP (tos 0x0, ttl 64, id 60084, offset 0, flags [none], proto ICMP (1), length 36)
    10.244.1.34 > 10.244.0.81: ICMP echo reply, id 3284, seq 64511, length 16
15:47:25.616639 IP (tos 0x0, ttl 64, id 39544, offset 0, flags [DF], proto ICMP (1), length 36)
    10.244.2.198 > 10.244.1.34: ICMP echo request, id 23113, seq 4516, length 16
15:47:25.618360 IP (tos 0x0, ttl 64, id 43336, offset 0, flags [none], proto ICMP (1), length 36)
    10.244.1.34 > 10.244.2.198: ICMP echo reply, id 23113, seq 4516, length 16
```

对 node01 上的 ens33 抓包，可看到 `cilium_vxlan` 已将 CA 的流量进行 vxlan 封包，src ip 改为本机 node ip 192.168.66.226, dst ip 改为 192.168.66.221

```bash
$ tcpdump -i ens33 -n dst 192.168.80.242 and udp -vv
tcpdump -i ens33 -n dst 192.168.80.242 and udp -vv
tcpdump: listening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes
15:51:45.291092 IP (tos 0x0, ttl 64, id 54645, offset 0, flags [none], proto UDP (17), length 110)
    192.168.80.241.51911 > 192.168.80.242.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 35602
IP (tos 0x0, ttl 64, id 3903, offset 0, flags [DF], proto TCP (6), length 60)
    10.244.1.245.60276 > 10.244.0.151.8080: Flags [S], cksum 0xf171 (correct), seq 975211802, win 64860, options [mss 1410,sackOK,TS val 629360941 ecr 0,nop,wscale 7], length 0
15:51:45.291751 IP (tos 0x0, ttl 64, id 54646, offset 0, flags [none], proto UDP (17), length 102)
    192.168.80.241.51911 > 192.168.80.242.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 35602
IP (tos 0x0, ttl 64, id 3904, offset 0, flags [DF], proto TCP (6), length 52)
    10.244.1.245.60276 > 10.244.0.151.8080: Flags [.], cksum 0xd8b1 (correct), seq 975211803, ack 3707204327, win 507, options [nop,nop,TS val 629360942 ecr 1559467480], length 0
15:51:45.292427 IP (tos 0x0, ttl 64, id 54647, offset 0, flags [none], proto UDP (17), length 183)
    192.168.80.241.51911 > 192.168.80.242.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 35602
IP (tos 0x0, ttl 64, id 3905, offset 0, flags [DF], proto TCP (6), length 133)
    10.244.1.245.60276 > 10.244.0.151.8080: Flags [P.], cksum 0xc88c (correct), seq 0:81, ack 1, win 507, options [nop,nop,TS val 629360943 ecr 1559467480], length 81: HTTP, length: 81
        GET /public HTTP/1.1
        Host: echo-a:8080
        User-Agent: curl/7.66.0
        Accept: */*

15:51:45.297956 IP (tos 0x0, ttl 64, id 54649, offset 0, flags [none], proto UDP (17), length 102)
    192.168.80.241.51911 > 192.168.80.242.8472: [no cksum] OTV, flags [I] (0x08), overlay 0, instance 35602
IP (tos 0x0, ttl 64, id 3906, offset 0, flags [DF], proto TCP (6), length 52)
```



# 4. Cilium & eBPF
Linux 内核本质是事件驱动的 （the Linux Kernel is fundamentally event-driven），cilium创建的虚拟网卡接收到的流量将会触发连接到TC（Traffic Control）ingress 钩子的 bpf 程序，对流量包进行相关策略对处理

## 4.1 egress datapath

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-dp-egress.png)

处理过程：

- cilium 在宿主机上创建 bpf 程序，及 kernel bpf 钩子
- 若使用 L7 Policy，cilium 将创建 iptables 规则
- 流量从容器的 endpoint 通过容器上的 veth pair 网卡 lxcXXX 发出，即触发 bpf_sockops.c / bpf_redir.c bpf 程序。若使用了 L7 Policy 则进入用户空间进行 L7 层数据处理，若没有则触发 TC egress 钩子， bpf_lxc 对数据进行处理（如果在 L3 加密，还会触发其他 bpf 钩子） 

- 数据最终被路由到 cilium_host 网关，然后通过 overlay 模式(vxlan) 转发出去



## 4.2 ingress datapath

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-dp-ingress.png)

流量进入主机网络设备上，cilium 根据相关配置：

- 对数据进行预处理 (prefilter / L3加解密 / 负载均衡 / L7 Policy 处理)

- 直接路由到 cilium_host 触发相应的 bpf 程序，最终到 endpoint



# 5. 微隔离

## 5.1 L3 / L4 隔离

```yaml
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
description: "L3-L4 policy to restrict deathstar access to empire ships only"
metadata:
  name: "rule1"
spec:
    endpointSelector:
        matchLabels:
            org: empire
         class: deathstar       
 ingress:
 - fromEndpoints:  
    - matchLabels:
               org: empire
    toPorts:
    - ports:
      - port: "80"
         protocol: TCP     
```

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-l34-isolation.png)



## 5.2 L7 隔离

L3 / L4 层的网络安全策略。缺乏对微服务层的可见性以及对API的细微颗粒度隔离访问控制，在微服务架构中时不够的

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-l7-isolation-1.png)

通过 eBPF，Cilium 提供一种简单而有效的方法来定义和执行基于容器、Pod身份的网络层和应用层安全策略，可通过下面的 NetworkPolicy 实现一个 L7 层网络的安全策略。

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-l7-isolation-2.png)

```yaml
apiVersion: "cilium.io/v2"
 kind: CiliumNetworkPolicy
 description: "L7 policy to restrict access to specific HTTP call"
 metadata:
   name: "rule1"
  spec:
     endpointSelector:
         matchLabels:
          org: empire
         class: deathstar
   ingress:
   - fromEndpoints:
      - matchLabels:
           org: empire
      toPorts:
      - ports:
        - port: "80"
          protocol: TCP
         rules:
             http:
           - method: "POST"
              path: "/v1/request-landing"
```

## 5.3 L7 Proxy

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-l7-proxy.png)

Cilium Agent 采用 eBPF 实现对数据包的重定向，将需要过滤的数据包首先转发至 Proxy 代理，Proxy代理根据其相应的过滤规则，对收到的数据包进行过滤，然后再将其发回数据包的原始路径。Proxy代理进行过滤的规则，则通过 Cilium Agent 进行下发和管理。



# 6. 总结

## 6.1 整体部署组件

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/k8s-network-cilium-provision.png)







