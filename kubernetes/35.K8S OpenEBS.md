# 1. 简介

## 1.1 OpenEBS 是什么？

OpenEBS 是一种开源云原生存储解决方案，托管于 CNCF 基金会，目前该项目处于沙箱阶段。

OpenEBS 是一组存储引擎，允许您为有状态工作负载 (StatefulSet) 和 Kubernetes 平台类型选择正确的存储解决方案。在高层次上，OpenEBS 支持两大类卷——本地卷和复制卷。

OpenEBS 是 Kubernetes 本地超融合存储解决方案，它管理节点可用的本地存储，并为有状态工作负载提供本地或高可用的分布式持久卷。作为一个完全的 Kubernetes 原生解决方案的另一个优势是，管理员和开发人员可以使用 kubectl、Helm、 Prometheus、Grafana、Weave Scope 等 Kubernetes 可用的所有优秀工具来交互和管理 OpenEBS。



## 1.2 OpenEBS 能做什么？

OpenEBS 管理 k8s 节点上存储，并为 k8s 有状态负载（StatefulSet）提供本地存储卷或分布式存储卷。

- 本地卷（Local Storage）

- OpenEBS 可以使用**宿主机裸块设备或分区，或者使用 Hostpaths 上的子目录，或者使用 LVM、ZFS 来创建持久化卷**
- 本地卷直接挂载到 Stateful Pod 中，而不需要 OpenEBS 在数据路径中增加任何开销
- OpenEBS 为本地卷提供了额外的工具，用于监控、备份 / 恢复、灾难恢复、由 ZFS 或 LVM 支持的快照等

- 对于分布式卷 (即复制卷)

- OpenEBS 使用其中一个引擎 (Mayastor、cStor 或 Jiva) 为每个分布式持久卷创建微服务
- 有状态 Pod 将数据写入 OpenEBS 引擎，OpenEBS 引擎将数据同步复制到集群中的多个节点。OpenEBS 引擎本身作为 Pod 部署，并由 Kubernetes 进行协调。当运行 Stateful Pod 的节点失败时，Pod 将被重新调度到集群中的另一个节点，OpenEBS 将使用其他节点上的可用数据副本提供对数据的访问
- 有状态的 Pods 使用 iSCSI (cStor 和 Jiva) 或 NVMeoF (Mayastor) 连接 OpenEBS 分布式持久卷
- OpenEBS cStor 和 Jiva 专注于存储的易用性和持久性。它们分别使用自定义版本的 ZFS 和 Longhorn 技术将数据写入存储。OpenEBS Mayastor 是最新开发的以耐久性和性能为设计目标的引擎，高效地管理计算 (大页面、核心) 和存储 (NVMe Drives)，以提供快速分布式块存储

  

 注意：OpenEBS 分布式块卷被称为复制卷，以避免与传统的分布式块存储混淆，传统的分布式块存储倾向于将数据分布到集群中的许多节点上。**复制卷是为云原生有状态工作负载设计的，这些工作负载需要大量的卷，这些卷的容量通常可以从单个节点提供，而不是使用跨集群中的多个节点分片的单个大卷**



## 1.3 对比传统分布式存储

OpenEBS 与其他传统存储解决方案不同的几个关键方面 :

- 使用微服务体系结构构建，就像它所服务的应用程序一样。OpenEBS 本身作为一组容器部署在 Kubernetes 工作节点上。使用 Kubernetes 本身来编排和管理 OpenEBS 组件
- 完全建立在用户空间，使其高度可移植性，以运行在任何操作系统 / 平台。
- 完全意图驱动，继承了 Kubernetes 易用性的相同原则
- OpenEBS 支持一系列存储引擎，因此开发人员可以部署适合于其应用程序设计目标的存储技术。像 Cassandra 这样的分布式应用程序可以使用 LocalPV 引擎进行最低延迟的写操作。像 MySQL 和 PostgreSQL 这样的单片应用程序可以使用使用 NVMe 和 SPDK 构建的 Mayastor 或基于 ZFS 的 cStor 来实现弹性。像 Kafka 这样的流媒体应用程序可以在边缘环境中使用 NVMe 引擎 Mayastor 以获得最佳性能。



驱使用户使用 OpenEBS 的主要原因是 :

- 在所有的 Kubernetes 发行版上都是可移植的
- 提高了开发人员和平台 SRE 的生产力
- 与其他解决方案相比，易于使用
- 优秀的社区支持
- 免费开源



**本地卷类型**:

本地卷只能从集群中的单个节点访问。必须在提供卷的节点上调度使用 Local Volume 的 Pods。本地卷通常是分布式工作负载的首选，比如 Cassandra、MongoDB、Elastic 等，这些工作负载本质上是分布式的，并且内置了高可用性（分片）

根据附加到 Kubernetes 工作节点上的存储类型，您可以从不同的动态本地 PV 进行选择——Hostpath、Device、LVM、ZFS 或 Rawfile



**可复制卷类型**:

复制卷顾名思义，是指将数据同步复制到多个节点的卷。卷可以支持节点故障。还可以跨可用性区域设置复制，以帮助应用程序跨可用性区域移动。

复制卷还能够提供像快照、克隆、卷扩展等企业存储特性。复制卷是有状态工作负载 (如 Percona/MySQL、Jira、GitLab 等) 的首选。

根据附加到 Kubernetes 工作节点的存储类型和应用程序性能需求，您可以从 Jiva、cStor 或 Mayastor 中进行选择



## 1.4 OpenEBS 存储引擎建议

| 应用需求                                         | 存储类型                        | OpenEBS 卷类型                                               |
| ------------------------------------------------ | ------------------------------- | ------------------------------------------------------------ |
| 低时延、高可用性、同步复制、快照、克隆、精简配置 | SSD/ 云存储卷                   | OpenEBS Mayastor                                             |
| 高可用性、同步复制、快照、克隆、精简配置         | 机械 /SSD/ 云存储卷             | OpenEBS cStor                                                |
| 高可用性、同步复制、精简配置                     | 主机路径或外部挂载存储          | OpenEBS Jiva                                                 |
| 低时延、本地 PV                                  | 主机路径或外部挂载存储          | Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile      |
| 低时延、本地 PV                                  | 本地机械 /SSD/ 云存储卷等块设备 | Dynamic Local PV - Device                                    |
| 低延迟，本地 PV，快照，克隆                      | 本地机械 /SSD/ 云存储卷等块设备 | OpenEBS Dynamic Local PV - ZFS , OpenEBS Dynamic Local PV - LVM |

总结：

- 多机环境，如果有额外的块设备（非系统盘块设备）作为数据盘，选用 OpenEBS Mayastor、OpenEBS cStor
- 多机环境，如果没有额外的块设备（非系统盘块设备）作为数据盘，仅单块系统盘块设备，选用 OpenEBS Jiva
- 单机环境，建议本地路径 Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile，由于单机多用于测试环境，数据可靠性要求较低。

由此看来，OpenEBS 常用场景为以上三个场景



## 1.5 OpenEBS 特性

**容器附加存储**

![img](https://s6.51cto.com/oss/202206/09/654080b43c12e6c3966757b3bd4942585b6b44.jpg)

OpenEBS 是一个容器附加存储 (Container Attached Storage, CAS) 的例子。通过 OpenEBS 提供的卷总是被容器化。每个卷都有一个专用的存储控制器，用于提高有状态应用程序的持久性存储操作的敏捷性和粒度。



**同步复制**

![img](https://s3.51cto.com/oss/202206/09/041f84277d024e746e73003d4708bb1288e21f.jpg)

同步复制是 OpenEBS 的一个可选的流行特性。当与 Jiva、cStor 和 Mayastor 存储引擎一起使用时，OpenEBS 可以同步复制数据卷以实现高可用性。跨 Kubernetes 区域进行复制，从而为跨 AZ 设置提供高可用性。这个特性对于使用 GKE、EKS 和 AKS 等云提供商服务上的本地磁盘构建高可用状态应用程序特别有用



**快照和克隆**

![img](https://s2.51cto.com/oss/202206/09/c52e3724018fafb09ea864d499544c80da2f97.jpg)

写时拷贝快照是 OpenEBS 另一个可选的流行特性。使用 cStor 引擎时，快照是瞬时创建的，并且不受快照个数的限制。增量快照功能增强了跨 Kubernetes 集群和跨不同云提供商或数据中心的数据迁移和可移植性。对快照和克隆的操作完全以 Kubernetes 原生方法执行，使用标准 kubectl 命令。常见的用例包括用于备份的高效复制和用于故障排除或针对数据的只读副本进行开发的克隆



**备份和恢复**

![img](https://s6.51cto.com/oss/202206/09/037dbf527faccb57a1d36702436751445a7bc0.jpg)

OpenEBS 卷的备份和恢复可以通过开源的 OpenEBS Velero 插件与 Kubernetes 备份和恢复解决方案 (如 Velero(前身为 Heptio Ark)) 协同工作。经常使用 OpenEBS 增量快照功能，将数据备份到 AWS S3、GCP object storage、MinIO 等对象存储目标。这种存储级别的快照和备份只使用增量数据进行备份，节省了大量的带宽和存储空间。



**真正的 Kubernetes 云原生存储**

![img](https://s6.51cto.com/oss/202206/09/a1594a339c52730993f000537abb47c020e291.jpg)

OpenEBS 是 Kubernetes 上有状态应用程序的云原生存储，云原生意味着遵循松散耦合的体系结构。因此，云原生、松散耦合体系结构的一般好处是适用的。例如，开发人员和 DevOps 架构师可以使用标准的 Kubernetes 技能和实用程序来配置、使用和管理持久存储需求



**减少存储 TCO 高达 50%**

在大多数云上，块存储的收费是基于购买的多少，而不是使用的多少 ; 为了实现更高的性能，并在充分利用容量时消除中断的风险，容量经常被过度配置。OpenEBS 的精简配置能力可以共享本地存储或云存储，然后根据需要增加有状态应用程序的数据量。可以动态添加存储，而不会中断暴露给工作负载或应用程序的卷。某些用户报告说，由于使用了 OpenEBS 的精简配置，节省了超过 60% 的资源。



**高可用性**

![img](https://s4.51cto.com/oss/202206/09/02014032871f7884ee829483958eefd98d66c4.jpg)

由于 OpenEBS 遵循 CAS 架构，在节点故障时，Kubernetes 将重新调度 OpenEBS 控制器，而底层数据则通过使用一个或多个副本来保护。更重要的是——因为每个工作负载都可以利用自己的 OpenEBS——不存在因存储丢失而导致系统大范围宕机的风险。例如，卷的元数据不是集中的，它可能会像许多共享存储系统那样受到灾难性的通用中断的影响。相反，元数据保持在卷的本地。丢失任何节点都会导致只存在于该节点上的卷副本的丢失。由于卷数据至少在其他两个节点上进行了同步复制，因此当一个节点出现故障时，这些数据将在相同的性能级别上继续可用



## 1.6 CAS 介绍

在 CAS 或容器附加存储 (Container Attached Storage) 体系结构中，存储在容器中运行，并且与存储绑定到的应用程序密切相关。存储作为微服务运行，没有内核模块依赖关系。像 Kubernetes 这样的编排系统编排存储卷，就像任何其他微服务或容器一样。CAS 具有 DAS 和 NAS 的优点



**非 CAS 系统上的 PV**

在非 CAS 模型中，Kubernetes 持久卷仍然与内核模块紧密耦合，使得 Kubernetes 节点上的存储软件本质上是单片的

![img](https://s7.51cto.com/oss/202206/09/86b435472bfdfda6548569a6b8f21b887b7763.jpg)



**基于 CAS 系统上的 PV**

![img](https://s7.51cto.com/oss/202206/09/07db2c7093083fd39e210293d235718c24f278.jpg)

相反，CAS 使您能够利用云原生应用程序的灵活性和可伸缩性。定义 Kubernetes PV (Persistent Volume) 的存储软件是基于微服务架构的。存储软件的控制平面 (存储控制器) 和数据平面 (存储副本) 作为 Kubernetes Pods 运行，因此，使您能够将云原生的所有优势应用到 CAS。



**CAS 优势**:

- 敏捷

CAS 中的每个存储卷都有一个容器化的存储控制器和相应的容器化副本。因此，围绕这些组件的资源的维护和调优是真正敏捷的。Kubernetes 滚动升级功能可以实现存储控制器和存储副本的无缝升级。可以使用容器 cGroups 调优 CPU 和内存等资源配额。

- 存储策略粒度化

将存储软件容器化并将存储控制器专用于每个卷可以带来最大的存储策略粒度。在 CAS 体系结构中，可以按卷配置所有存储策略。此外，您可以监视每个卷的存储参数，并动态更新存储策略，以实现每个工作负载的预期结果。随着卷存储策略中这种额外粒度级别的增加，存储吞吐量、IOPS 和延迟的控制也会增加。

- 云原生

CAS 将存储软件装入容器，并使用 Kubernetes 自定义资源定义 (CRDs) 来声明低级存储资源，如磁盘和存储池。这个模型使存储能够无缝地集成到其他云原生工具中。可以使用 Prometheus、Grafana、Fluentd、Weavescope、Jaeger 等云原生工具来供应、监控和管理存储资源

- PV 是 CAS 中的一个微服务

如上图所示，在 CAS 架构中，存储控制器和副本的软件完全是基于微服务的，因此不涉及内核组件。通常，存储控制器 POD 被调度在与持久卷相同的节点上，以提高效率，副本 POD 可以被调度在集群节点上的任何位置。每个副本使用本地磁盘、SAN 磁盘和云磁盘的任意组合完全独立于其他副本进行配置。这为大规模管理工作负载的存储分配提供了巨大的灵活性。

- 超融合非分布式

CAS 架构没有遵循典型分布式存储架构。通过从存储控制器到存储副本的同步复制，存储变得高度可用。卷副本的元数据不是在节点之间共享的，而是在每个本地节点上独立管理。如果一个节点故障，存储控制器 (在本例中是一个无状态容器) 将在一个节点上轮转，该节点上运行着第二个或第三个副本，数据仍然可用。

与超融合系统类似，CAS 中的卷的存储和性能是可扩展的。由于每个卷都有自己的存储控制器，因此存储可以在一个节点的存储容量允许的范围内进行扩展。在给定的 Kubernetes 集群中，随着容器应用程序数量的增加，会增加更多的节点，从而提高存储容量和性能的整体可用性，从而使存储对新的应用程序容器可用。这一过程与 Nutanix 等成功的超融合系统非常相似。



# 2. OpenESB 架构介绍

OpenESB 遵循容器附加存储（CAS）模型，每个卷都有一个专用的控制器 POD 和一组副本 POD。CAS 体系结构的优点在`CNCF` 博客[1] 上进行了讨论。OpenEBS 操作和使用都很简单，因为它看起来和感觉上就像其他云原生和 Kubernetes 友好的项目。

![img](https://s3.51cto.com/oss/202206/09/656b73049c31883648b9075590b5d5a8ac94ab.jpg)

OpenEBS 有许多组件，可以分为以下类别 :

- 控制面组件 - Provisioner, API Server, volume exports,volume sidecars
- 数据面组件 - Jiva、cStor
- 节点磁盘管理器 - Discover, monitor, 管理连接 k8s 的媒介
- 与云原生工具的集成 - 已经与 Prometheus,Grafana, Fluentd、Jaeger 集成



## 2.1 控制面

OpenEBS集群的控制平面通常被称为Maya。

OpenEBS 控制平面负责提供卷、相关的卷操作，如快照、克隆、创建存储策略、执行存储策略、导出 Prometheus/grafana 使用的卷指标，等等。

![img](https://s2.51cto.com/oss/202206/09/b4b2d60357d467515ee574cdabeb1881370e66.jpg)

OpenEBS 提供了一个动态提供程序，这是 Kubernetes 的标准外部存储插件。OpenEBS PV 提供者的主要任务是向应用程序 PODS 启动卷供应，并实现 PV 的 Kubernetes 规范。

m-apiserver 开放存储的 REST API，并承担大量卷策略处理和管理工作。

控制平面和数据平面之间的连通性使用 Kubernetes sidecar 模式。控制平面需要与数据平面通信的场景如下所示。

- 对于卷的统计，如 IOPS，吞吐量，延迟等--通过卷暴漏的 sidecar 实现
- 使用卷控制器 pod 执行卷策略，使用卷副本 pod 进行磁盘 / 池管理-通过卷管理 sidecar 实现

**OpenEBS PV Provisioner**

![img](https://s8.51cto.com/oss/202206/09/13c056a22473b20128b520f5cd5be7919c3aeb.jpg)

此组件作为 POD 运行，并做出配置决策 , 它的使用方式是 :

开发人员用所需的卷参数构造一个声明，选择适当的存储类，并在 YAML 规范上调用 kubelet。OpenEBS PV 动态提供程序与 maya-apiserver 交互，在适当的节点上为卷控制器 pod 和卷副本 pod 创建部署规范。可以使用 PVC 规范中的注释来控制卷 Pod(控制器 / 副本) 的调度。

目前，OpenEBS Provisioner 只支持一种绑定类型，即 iSCSI。



**Maya-ApiServer**

![img](https://s9.51cto.com/oss/202206/09/f53ddbb67763b1b6f98000502e9467523c5c96.jpg)

m-apiserver作为POD运行。顾名思义，m-apiserver公开OpenEBS REST api。

m-apiserver 还负责创建创建卷 pod 所需的部署规范文件。在生成这些规范文件之后，它将调用 kube-apiserver 来相应地调度这些 pods。OpenEBS PV 提供者在卷发放结束时，将创建一个 PV 对象并将其挂载到应用程序 pod 上。PV 由控制器 pod 承载，控制器 pod 由不同节点中的一组副本 pod 支持。控制器 pod 和复制 pod 是数据平面的一部分，在存储引擎部分有更详细的描述。

m-apiserver 的另一个重要任务是卷策略管理。OpenEBS 为表示策略提供了非常细粒度的规范。m-apiserver 解释这些 YAML 规范，将它们转换为可执行的组件，并通过容量管理 sidecar 来执行它们



**Maya Volume Exporter**

Maya卷导出器是每个存储控制器pod的sidecar。

这些 sidecar 将控制平面连接到数据平面以获取统计信息。统计信息的粒度在卷级别。一些统计数据示例如下：

- 卷读取延迟
- 卷写入延迟
- 卷每秒读取速度
- 卷每秒写入速度
- 读取块大小
- 写入块大小
- 容量统计

![img](https://s9.51cto.com/oss/202206/09/b24857e646a6f4a0f0599975222da09f16fa94.jpg)

这些统计信息通常由 Prometheus 客户端来拉取，该客户端在 OpenBS 安装期间安装和配置。



**卷管理 sidecar**

Sidecars 还用于将控制器配置参数和卷策略传递给卷控制器 pod(卷控制器 pod 是一个数据平面)， 并将副本配置参数和副本数据保护参数传递给卷副本 pod。

![img](https://s7.51cto.com/oss/202206/09/b531908995377189289648c66e67ffeef5614f.jpg)



## 2.2 数据面

OpenEBS 数据平面负责实际的卷 IO 路径。存储引擎在数据平面实现实际的 IO 路径。目前，OpenEBS 提供了两个可以轻松插入的存储引擎。它们被称为 Jiva 和 cStor。这两个存储引擎都完全运行在 Linux 用户空间中，并基于微服务架构。



**Jiva**

Jiva 存储引擎基于 Rancher's LongHorn 与 gotgt 开发实现， 使用 go 语言开发，并运行于用户命名空间下。LongHorn 控制器将输入的 IO 同步复制到 LongHorn 副本。该副本将 Linux 稀疏文件视为构建存储特性 (如精简配置、快照、重建等) 的基础。



**cStor**

cStor 数据引擎使用 C 语言编写，具有高性能的 iSCSI target 和 Copy-On-Write 块系统，提供数据完整性、数据弹性和时间点的快照和克隆。cStor 有一个池特性，它以条带、镜像或 RAIDZ 模式聚合一个节点上的磁盘，以提供更大的容量和性能单位。cStor 还可以跨区域将数据同步复制到多个节点，从而避免节点丢失或节点重启导致数据不可用。



**LocalPV**

对于那些不需要存储级复制的应用程序，LocalPV 可能是很好的选择，因为它能提供更高的性能。OpenEBS LocalPV 与 Kubernetes LocalPV 类似，不同之处在于它是由 OpenEBS 控制平面动态提供的， 就像任何其他常规 PV 一样。OpenEBS LocalPV 有两种类型 :hostpath LocalPV 和 device LocalPV。hostpath LocalPV 指的是主机上的子目录，LocalPV 指的是在节点上发现的磁盘 (可以是直接连接的，也可以是网络连接的)。OpenEBS 引入了一个 LocalPV 提供者，用于根据 PVC 和存储类规范中的一些标准选择匹配的磁盘或主机路径。



**节点磁盘管理器**

节点磁盘管理器 (NDM) 填补了使用 Kubernetes 管理有状态应用程序的持久存储所需的工具链的空白。容器时代的 DevOps 架构师必须以一种自动化的方式满足应用程序和应用程序开发人员的基础设施需求， 这种方式可以跨环境提供弹性和一致性。这些要求意味着存储堆栈本身必须非常灵活， 以便 Kubernetes 和云原生生态系统中的其他软件可以轻松地使用这个堆栈。NDM 在 Kubernetes 的存储堆栈中起着基础性的作用，它统一了不同的磁盘， 并通过将它们标识为 Kubernetes 对象，提供了将它们汇聚的能力。此外，NDM 发现、提供、监视和管理底层磁盘的方式，可以让 Kubernetes PV 提供者 (如 OpenEBS 和其他存储系统) 和 Prometheus 管理磁盘子系统

![img](https://s3.51cto.com/oss/202206/09/d74eb0092699b75cf7d50533e598579aa1c6c3.jpg)

# 3. CAS 引擎

## 3.1 存储引擎概述

存储引擎是持久化卷 IO 路径的数据平面组件。在 CAS 架构中，用户可以根据不同的配置策略，为不同的应用工作负载选择不同的数据平面。存储引擎可以通过特性集或性能优化给定的工作负载。

操作员或管理员通常选择具有特定软件版本的存储引擎，并构建优化的卷模板， 这些卷模板根据底层磁盘的类型、弹性、副本数量和参与 Kubernetes 集群的节点集进行微调。用户可以在发放卷时选择最优的卷模板，从而在给定的 Kubernetes 集群上为所有存储卷运行最优的软件和存储组合提供最大的灵活性。



## 3.2 存储引擎类型

OpenEBS 提供了三种存储引擎

- Jiva

  Jiva 是 OpenEBS 0.1 版中发布的第一个存储引擎，使用起来最简单。它基于 GoLang 开发，内部使用 LongHorn 和 gotgt 堆栈。 Jiva 完全在用户空间中运行，并提供同步复制等标准块存储功能。 Jiva 通常适用于容量较小的工作负载，不适用于大量快照和克隆特性是主要需求的情况

- cStor

  cStor 是 OpenEBS 0.7 版本中最新发布的存储引擎。cStor 非常健壮，提供数据一致性，很好地支持快照和克隆等企业存储特性。 它还提供了一个健壮的存储池特性，用于在容量和性能方面进行全面的存储管理。 cStor 与 NDM (Node Disk Manager) 一起，为 Kubernetes 上的有状态应用程序提供完整的持久化存储特性

- OpenEBS Local PV

OpenEBS Local PV 是一个新的存储引擎，它可以从本地磁盘或工作节点上的主机路径创建持久卷或 PV。 CAS 引擎可以从 OpenEBS 的 1.0.0 版本中获得。使用 OpenEBS Local PV，性能将等同于创建卷的本地磁盘或文件系统 (主机路径)。 许多云原生应用程序可能不需要复制、快照或克隆等高级存储特性，因为它们本身就提供了这些特性。这类应用程序需要以持久卷的形式访问管理的磁盘

![img](https://s6.51cto.com/oss/202206/09/643f0cf076398b61a3029554e36e8f1d27aedf.jpg)

- SP  存储池，表示 Jiva 自定义存储资源
- CV  cStor 卷，表示 cStor 卷自定义资源
- CVR  cStor 卷副本
- SPC  存储池声明，表示 cStor 池聚合的自定义资源
- CSP  cStor 存储池，表示 cStor Pool 每个节点上的自定义资源

一个 SPC 对应多个 CSP，相应的一个 CV 对应多个 CVR



## 3.3 存储引擎声明

通过指定注释 openebs 来选择存储引擎。StorageClass 规范中的 io/cas-type。StorageClass 定义了提供程序的细节。为每个 CAS 引擎指定单独的供应程序。

**cStor 存储类规范文件内容**

```
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: cStor-storageclass
 annotations:
   openebs.io/cas-type: cstor
   cas.openebs.io/config: |
     - name: StoragePoolClaim
       value: "cStorPool-SSD"
provisioner: openebs.io/provisioner-iscsi
```



**Jiva 存储类规范文件内容**

```
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: jiva-storageclass
 annotations:
   openebs.io/cas-type: jiva
   cas.openebs.io/config: |
     - name: StoragePool
       value: default
provisioner: openebs.io/provisioner-iscsi
```

当 cas 类型为 Jiva 时，StoragePool 的 default 值具有特殊含义。当 pool 为默认值时，Jiva 引擎将从容器 (副本 pod) 本身的存储空间中为副本 pod 开辟数据存储空间。当所需的卷大小很小 (比如 5G 到 10G) 时，StoragePool default 工作得很好，因为它可以容纳在容器本身内。



**Local PV 存储类规范文件内容-主机路径**

```
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: localpv-hostpath-sc
 annotations:
   openebs.io/cas-type: local
   cas.openebs.io/config: |
     - name: BasePath
       value: "/var/openebs/local"
     - name: StorageType
       value: "hostpath"
provisioner: openebs.io/local
```



**Local PV 存储类规范文件内容-主机设备**

```
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: localpv-device-sc
 annotations:
   openebs.io/cas-type: local
   cas.openebs.io/config: |
     - name: StorageType
       value: "device"
     - name: FSType
       value: ext4
provisioner: openebs.io/local
```



**cStor、Jiva、LocalPV 特性比较**：

| 特性                 | Jiva  | cStor    | Local PV |
| -------------------- | ----- | -------- | -------- |
| 轻量级运行于用户空间 | Yes   | Yes      | Yes      |
| 同步复制             | Yes   | Yes      | No       |
| 适合低容量工作负载   | Yes   | Yes      | Yes      |
| 支持快照，克隆       | Basic | Advanced | No       |
| 数据一致性           | Yes   | Yes      | NA       |
| 使用 Velero 恢复备份 | Yes   | Yes      | Yes      |
| 适合高容量工作负载   | No    | Yes      | Yes      |
| 自动精简配置         |       | Yes      | No       |
| 磁盘池或聚合支持     |       | Yes      | No       |
| 动态扩容             |       | Yes      | Yes      |
| 数据弹性 (RAID 支持) |       | Yes      | No       |
| 接近原生磁盘性能     | No    | No       | Yes      |

**大多数场景推荐 cStor**，因其提供了强大的功能，包括快照 / 克隆、存储池功能（如精简资源调配、按需扩容等）。

Jiva 适用于低容量需求的工作负载场景，例如 5 到 50G。尽管使用 Jiva 没有空间限制，但建议将其用于低容量工作负载。Jiva 非常易于使用，并提供企业级容器本地存储，而不需要专用硬盘。有快照和克隆功能的需求的场景，优先考虑使用 cStor 而不是 Jiva。



## 3.4 CAS 引擎使用场景

如上表所示，每个存储引擎都有自己的优势。选择引擎完全取决于应用程序的工作负载以及它当前和未来的容量和 / 或性能增长。下面的指导原则在定义存储类时为选择特定的引擎提供一些帮助。

**选择 cStor 的理想条件**:

- **当需要同步复制数据并在节点上有多个磁盘时**
- 当您从每个节点上的本地磁盘或网络磁盘池管理多个应用程序的存储时。通过精简配置、存储池和卷的按需扩容、存储池的性能按需扩容等特性，实现对存储层的管理。cStor 用于在本地运行的 Kubernetes 集群上构建 Kubernetes 本地存储服务，类似于 AWS EBS 或谷歌 PD。
- 当需要存储级快照和克隆能力时
- 当您需要企业级存储保护特性，如数据一致性、弹性 (RAID 保护)。
- 如果您的应用程序不需要存储级复制，那么使用 OpenEBS 主机路径 LocalPV 或 OpenEBS 设备 LocalPV 可能是更好的选择。



**选择 Jiva 的理想条件 :**

- **当您想要数据的同步复制，并且拥有单个本地磁盘或单个管理磁盘 (如云磁盘 (EBS、GPD))，并且不需要快照或克隆特性时**

- Jiva 是最容易管理的，因为磁盘管理或池管理不在这个引擎的范围内。Jiva 池是本地磁盘、网络磁盘、虚拟磁盘或云磁盘的挂载路径。

- 以下场景 Jiva 更优于 cStor :

  - 当程序不需要存储级的快照、克隆特性

  - 当节点上没有空闲磁盘时。Jiva 可以在主机目录上使用，并且仍然可以实现复制。

  - 当不需要动态扩展本地磁盘上的存储时。将更多磁盘添加到 Jiva 池是不可能的，因此 Jiva 池的大小是固定的，如果它在物理磁盘上。但是，如果底层磁盘是虚拟磁盘、网络磁盘或云磁盘，则可以动态地更改 Jiva 池的大小

  - 容量需求较小。大容量应用通常需要动态增加容量，cStor 更适合这种需求



**选择 OpenEBS 主机路径 LocalPV 的理想条件 :**

- **当应用程序本身具备管理复制能力（例如：es）时**，不需要在存储层进行复制。在大多数这样的情况下，应用程序是作为 statefulset 部署的
- 高于 Jiva 与 cStor 的读写性能需求
- 当特定应用程序没有专用的本地磁盘或特定应用程序不需要专用的存储时，建议使用 Hostpath。如果您想跨多个应用程序共享一个本地磁盘，主机路径 LocalPV 是正确的方法



**选择 OpenEBS 主机设备 LocalPV 的理想条件 :**

- **当应用程序管理复制本身，不需要在存储层进行复制时**。在大多数这种情况下，应用程序被部署为有状态集
- 高于 Jiva 与 cStor 的读写性能需求
- 高于 OpenEBS 主机路径 LocalPV 的读写性能需求
- 当需要接近磁盘性能时。该卷专用于写入单个 SSD 或 NVMe 接口以获得最高性能



**总结**

- 如果应用程序处于生产中，并且不需要存储级复制，那么首选 LocalPV
- 如果您的应用程序处于生产状态，并且需要存储级复制，那么首选 cStor
- 如果应用程序较小，需要存储级复制，但不需要快照或克隆，则首选 Jiva



## 3.5 节点磁盘管理器（NDM）

节点磁盘管理器 (NDM) 是 OpenEBS 体系结构中的一个重要组件。NDM 将块设备视为需要监视和管理的资源，就像 CPU、内存和网络等其他资源一样。它是一个在每个节点上运行的守护进程，基于过滤器检测附加的块设备，并将它们作为块设备自定义资源加载到 Kubernetes 中。这些定制资源旨在通过提供类似于 :

- 轻松访问 Kubernetes 集群中可用的块设备清单
- 预测磁盘的故障，以帮助采取预防措施
- 允许动态地将磁盘挂载 / 卸载到存储 Pod 中，而无需重新启动在磁盘挂载 / 卸载的节点上运行的相应 NDM Pod

尽管做了上述所有工作，NDM 还是有助于提供持久卷的总体简化。

![img](https://s5.51cto.com/oss/202206/09/d29b9106629c6416e3930516c59b633eef89bf.jpg)

NDM 是在 OpenEBS 安装期间作为守护进程部署的。NDM daemonset 发现每个节点上的磁盘，并创建一个名为 Block Device 或 BD 的自定义资源。

**访问权限说明**

NDM 守护进程运行在容器中，必须访问底层存储设备并以特权模式运行。NDM 需要特权模式，因为它需要访问 /dev、/proc 和 /sys 目录来监视附加设备，还需要使用各种探测器获取附加设备的详细信息。NDM 负责发现块设备并过滤掉不应该被 OpenEBS 使用的设备 ; 例如，检测有 OS 文件系统的磁盘。NDM pod 默认情况下在容器中挂载主机的 /proc 目录，然后加载 /proc/1/mounts，以找到操作系统使用的磁盘

**NDM 守护程序功能**

- 发现 Kubernetes 节点上的块设备

- 在启动时发现块设备-创建和 / 或更新状态。
- 维护集群范围内磁盘的唯一 id: 对 WWN / PartitionUUID / FileSystemUUID / DeviceMapperUUID 进行 Hash 计算

- 检测节点中添加 / 移除块设备，并更新块设备状态
- 添加块设备作为 Kubernetes 自定义资源，具有以下属性：

- Active : 节点上存在块设备
- Inactive : 给定节点上不存在块设备
- Unknown : NDM 在块设备最后被检测到的节点上停止 / 无法确定状态
- 主机名称 (kubernetes.io/hostname)
- 块设备类型 (ndm.io/blockdevice-type)
- Managed (ndm.io/managed)
- 设备路径
- 设备链接
- 供应商和型号信息
- WWN 和序列号
- 容量
- 扇区和区块大小
- spec: 如果可用，将更新以下内容
- labels:
- status: 状态可以有以下值

**过滤器**

- 为要创建块设备 CR 的块设备类型配置过滤器。过滤器可以通过供应商类型、设备路径模式或挂载点进行配置

- 过滤器可以是包含过滤器，也可以是排除过滤器。它们被配置为 configmap。管理员用户可以在 OpenEBS 安装时通过更改 OpenEBS 操作员 yaml 文件或 helm 值中的 NDM configmap 来配置这些过滤器。yaml 文件。如果这些过滤器需要在安装后更新，那么可以遵循以下方法之一 :

  - 使用 operator 方式安装 OpenEBS。在 Yaml 文件中，更新 configmap 中的过滤器并应用 operator.yaml

  - 如果 OpenEBS 是使用 helm 安装的，更新 values.yaml 中的 configmap 并使用 helm 进行升级

  - 或者，使用 kubectl 编辑 NDM configmap，更新过滤器



# 4. 落地实践

## 4.1 先决条件

OpenEBS依赖与iSCSI做存储管理，因此需要先确保您的集群上已有安装openiscsi。

**注意**：如果您使用kubeadm，容器方式安装的kublet，那么其中会自带iSCSI，不需要再手动安装，如果是直接使用二进制形式在裸机上安装的kubelet，则需要自己安装iSCSI。

iSCSI( Internet Small Computer System Interface 互联网小型计算机系统接口)是一种基于TCP/IP 的协议，用来建立和管理IP存储设备、主机和客户机等之间的相互连接，并创建存储区域网络（SAN）。SAN 使得SCSI 协议应用于高速数据传输网络成为可能，这种传输以数据块级别（block-level）在多个数据存储网络间进行。SCSI 结构基于C/S模式，其通常应用环境是：设备互相靠近，并且这些设备由SCSI 总线连接。

OpenEBS需要使用iSCSI作为存储协议，而CentOS上默认是没有安装该软件的，因此我们需要手动安装。

iSCSI中包括两种类型的角色：

- **target**：用来提供存储（server）
- **initiator**：使用存储的客户端（client）

在Kubernetes中使用iSCSI的架构图

![img](https://3291113816-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FgrbEKquD93OLsW4hFlF6%2Fuploads%2Fgit-blob-cdaeaccba2885edbb06d3e4c6c5fb0168ec01c8a%2Fiscsi-on-kubernetes.png?alt=media)



安装iSCSI服务十分简单，不需要额外的配置，只要安装后启动服务即可。

**Ubuntu/Debian**:

```bash
apt update
apt install open-iscsi -y
systemctl enable iscsid 
systemctl start iscsid
```



**CentOS/RedHat**:

```bash
yum install iscsi-initiator-utils -y 
systemctl enable iscsid
systemctl start iscsid
```



## 4.2 安装

```bash
helm repo add openebs https://openebs.github.io/charts
helm repo update
helm install openebs --namespace openebs openebs/openebs --create-namespace

helm install openebs --namespace openebs openebs/openebs --create-namespace --set ndm.filters.includePaths=/dev/vdb

# 默认安装Jiva和LocalPV引擎，其他引擎需要设定
helm install openebs --namespace openebs openebs/openebs --create-namespace --set cstor.enabled=true
```



## 4.3 Local PV

### 4.3.1 Hostpath

对比 Kubernetes Hostpath 卷相比，OpenEBS 本地 PV Hostpath 卷具有以下优势 :

- OpenEBS 本地 PV Hostpath 允许您的应用程序通过 StorageClass、PVC 和 PV 访问 Hostpath。这为您提供了更改 PV 提供者的灵活性，而无需重新设计应用程序 YAML
- 使用 Velero 备份和恢复进行数据保护
- 通过对应用程序 YAML 和 pod 完全屏蔽主机路径来防范主机路径安全漏洞



**创建数据目录**

在将要创建 Local PV Hostpaths 的节点上设置目录。这个目录将被称为 BasePath。默认位置是 /var/openebs/local

节点 node1、node2、node3 创建 /data/openebs/local 目录 （/data 可以预先挂载数据盘，如未挂载额外数据盘，则使用操作系统 '/' 挂载点存储空间）

```bash
$ mkdir -p /data/openebs/local
```



**创建存储类、存储请求及应用**

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: openebs-hostpath-demo
 annotations:
   openebs.io/cas-type: local
   cas.openebs.io/config: |
     #hostpath type will create a PV by
     # creating a sub-directory under the
     # BASEPATH provided below.
     - name: StorageType
       value: "hostpath"
     #Specify the location (directory) where
     # where PV(volume) data will be saved.
     # A sub-directory with pv-name will be
     # created. When the volume is deleted,
     # the PV sub-directory will be deleted.
     #Default value is /var/openebs/local
     - name: BasePath
       value: "/data/openebs/local/"
provisioner: openebs.io/local
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: local-hostpath-pvc
spec:
 storageClassName: openebs-hostpath-demo
 accessModes:
   - ReadWriteOnce
 resources:
   requests:
     storage: 200Mi
     
---
apiVersion: v1
kind: Pod
metadata:
 name: openebs-local-hostpath-pod
spec:
 volumes:
 - name: local-storage
   persistentVolumeClaim:
     claimName: local-hostpath-pvc
 containers:
 - name: hello-container
   image: busybox
   command:
      - sh
      - -c
      - 'while true; do echo "`date` [`hostname`] Hello from OpenEBS Local PV." >> /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done'
   volumeMounts:
   - mountPath: /mnt/store
     name: local-storage
```



### 4.3.2 Device

对比 Kubernetes 本地持久卷，OpenEBS 本地 PV 设备卷有以下优点 :

- OpenEBS 本地 PV 设备卷 provider 是动态的，Kubernetes 设备卷 provider 是静态的
- OpenEBS NDM 更好地管理用于创建本地 pv 的块设备。NDM 提供了发现块设备属性、设置设备筛选器、度量集合以及检测块设备是否已经跨节点移动等功能



```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-device
  annotations:
    openebs.io/cas-type: local
    cas.openebs.io/config: |
      - name: StorageType
        value: device
      - name: FSType
        value: xfs      
provisioner: openebs.io/local
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer

---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: local-device-pvc
spec:
 storageClassName: local-device
 accessModes:
   - ReadWriteOnce
 resources:
   requests:
     storage: 1G

---
apiVersion: v1
kind: Pod
metadata:
 name: hello-local-device-pod
spec:
 volumes:
 - name: local-storage
   persistentVolumeClaim:
     claimName: local-device-pvc
 containers:
 - name: hello-container
   image: busybox
   command:
      - sh
      - -c
      - 'while true; do echo "`date` [`hostname`] Hello from OpenEBS Local PV." >> /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done'
   volumeMounts:
   - mountPath: /mnt/store
     name: local-storage
```



成功后：

```bash
root@master-01:~# kubectl get pod hello-local-device-pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE     IP              NODE        NOMINATED NODE   READINESS GATES
hello-local-device-pod   1/1     Running   0          2m37s   10.244.172.17   worker-01   <none>           <none>

root@worker-01:~# lsblk
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
...
vdb    252:16   0    50G  0 disk
└─vdb1 252:17   0    50G  0 part /var/lib/kubelet/pods/d27ecb6c-2477-4734-9f60-542afb8f2ba7/volumes/kubernetes.io~local-volume/pvc-42578cca-26ff-48fd-86eb-7ea929bc6dc2

root@worker-01:~# ls -l /var/lib/kubelet/pods/d27ecb6c-2477-4734-9f60-542afb8f2ba7/volumes/kubernetes.io~local-volume/pvc-42578cca-26ff-48fd-86eb-7ea929bc6dc2
total 4
-rw-r--r-- 1 root root 83 Jun 28 06:15 greet.txt

root@worker-01:~# blkid
...
/dev/vdb1: UUID="21f378d4-5d81-414e-b0cf-9ea0ef499768" TYPE="xfs" PARTLABEL="OpenEBS_NDM" PARTUUID="9afb805a-8484-43e5-aadb-36a5cc21b705"

root@worker-01:~# lsblk -f
NAME   FSTYPE   LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINT
...
vdb
└─vdb1 xfs            21f378d4-5d81-414e-b0cf-9ea0ef499768   49.6G     1% /var/lib/kubelet/pods/d27ecb6c-2477-4734-9f60-542afb8f2ba7/volumes/kubernetes.io~local-volume/pvc-42578cca-26ff-48fd-86eb-7ea929
```





# 5. 存储类型

| Volume              | Storage                  | Requirement                                           |
| ------------------- | ------------------------ | ----------------------------------------------------- |
| OpenEBS Mayastor    | SSDs/Cloud Volumes       | 低延时, HA, 同步副本, 快照, 克隆, Thin provisioning   |
| OpenEBS cStor       | Disks/SSDs/Cloud Volumes | 保护节点异常, 同步副本, 快照, 克隆, Thin provisioning |
| OpenEBS Jiva        | hostpath, 外部挂载       | 保护节点异常, 同步副本, Thin provisioning             |
| Local PV - Hostpath | hostpath, 外部挂载       | 低延时, 本地持久卷                                    |
| Local PV - Device   | Disks/SSDs/Cloud Volumes | 低延时, 本地持久卷                                    |
| Local PV - ZFS      | Disks/SSDs/Cloud Volumes | 低延时, 本地持久卷, 快照, 克隆                        |
| Local PV - Rawfile  |                          | 低延时, 本地持久卷                                    |









## 存储策略

OpenEBS的存储策略使用StorageClaass实现，包括如下的StorageClass：

- openebs-cassandra
- openebs-es-data-sc
- openebs-jupyter
- openebs-kafka
- openebs-mongodb
- openebs-percona
- openebs-redis
- openebs-standalone
- openebs-standard
- openebs-zk





### 总结

在整个测试验证过程，OpenEBS 给我的感觉是：极简的操作，尤其 Local PV 引擎的部署使用。

但 OpenEBS 现阶段也存在一些不足：

- cStor 与 Jiva 数据面组件较多，配置较为繁琐（第一感觉概念性的组件过多，）
- cStor 与 Jiva 部分组件创建依赖内部定义的镜像 tag，在离线环境下无法通过调整为私有库 tag 导致组件无法成功运行
- 存储类型单一，多个引擎仅支持块存储类型，不支持原生多节点读写（需结合 NFS 实现），对比 ceph 等稍显逊色

建议以下场景使用 OpenEBS 作为后端存储：

- 单机测试环境
- 多机实验 / 演示环境







## 装 dbench 性能测试工具[¶](https://docs.daocloud.io/storage/solutions/openebs-helm/#dbench)

```
[root@k8s-10-6-162-31 ~]# cat fio-deploy.yaml
** NOTE: For details of params to construct an fio job, refer to this link:
** https://fio.readthedocs.io/en/latest/fio_doc.html

---
apiVersion: batch/v1
kind: Job
metadata:
generateName: dbench-
spec:
template:
spec:
containers:
- name: dbench
image: openebs/perf-test:latest
imagePullPolicy: IfNotPresent
env:

** storage mount point on which testfiles are created

- name: DBENCH_MOUNTPOINT
value: /data
```

## 测试 io 性能[¶](https://docs.daocloud.io/storage/solutions/openebs-helm/#io)

```
kubectl delete pod hello-local-hostpath-pod

[root@k8s-10-6-162-31 ~]# kubectl create -f fio-deploy.yaml
job.batch/dbench-rmdqr created

[root@k8s-10-6-162-31 ~]# kubectl get pod
NAME READY STATUS RESTARTS AGE
dbench-rmdqr-qbl74 1/1 Running 0 4m59s
longhorn-iscsi-installation-2thd5 1/1 Running 1 54d
longhorn-iscsi-installation-ctqtg 1/1 Running 1 (52d ago) 54d
longhorn-iscsi-installation-mrm4h 1/1 Running 1 (52d ago) 54d

[root@k8s-10-6-162-31 ~]# kubectl logs -f dbench-729cw-nqfpt
Error from server (NotFound): pods "dbench-729cw-nqfpt" not found
[root@k8s-10-6-162-31 ~]# kubectl logs -f dbench-rmdqr-qbl74
Working dir: /data

Testing Read IOPS...
read_iops: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.13
Starting 1 process
read_iops: Laying out IO file (1 file / 2048MiB)

read_iops: (groupid=0, jobs=1): err= 0: pid=17: Mon Feb 6 06:27:30 2023
read: IOPS=137, BW=566KiB/s (580kB/s)(8600KiB/15193msec)
bw ( KiB/s): min= 8, max= 1752, per=100.00%, avg=570.00, stdev=518.97, samples=29
iops : min= 2, max= 438, avg=142.41, stdev=129.78, samples=29
cpu : usr=0.22%, sys=1.13%, ctx=1984, majf=0, minf=16
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=100.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
issued rwts: total=2087,0,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
READ: bw=566KiB/s (580kB/s), 566KiB/s-566KiB/s (580kB/s-580kB/s), io=8600KiB (8806kB), run=15193-15193msec

Disk stats (read/write):
dm-0: ios=2475/39, merge=0/0, ticks=1076796/25702, in_queue=1114531, util=100.00%, aggrios=2494/51, aggrmerge=0/3, aggrticks=1094965/36042, aggrin_queue=1130703, aggrutil=99.98%
sda: ios=2494/51, merge=0/3, ticks=1094965/36042, in_queue=1130703, util=99.98%


Testing Write IOPS...
write_iops: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.13
Starting 1 process

write_iops: (groupid=0, jobs=1): err= 0: pid=33: Mon Feb 6 06:27:49 2023
write: IOPS=199, BW=815KiB/s (835kB/s)(13.0MiB/16358msec); 0 zone resets
bw ( KiB/s): min= 40, max= 3408, per=100.00%, avg=871.87, stdev=932.70, samples=30
iops : min= 10, max= 852, avg=217.87, stdev=233.18, samples=30
cpu : usr=0.17%, sys=1.05%, ctx=958, majf=0, minf=16
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=100.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
issued rwts: total=0,3271,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
WRITE: bw=815KiB/s (835kB/s), 815KiB/s-815KiB/s (835kB/s-835kB/s), io=13.0MiB (13.7MB), run=16358-16358msec

Disk stats (read/write):
dm-0: ios=0/4802, merge=0/0, ticks=0/1128001, in_queue=1129209, util=99.50%, aggrios=0/4826, aggrmerge=0/4, aggrticks=0/1147779, aggrin_queue=1147776, aggrutil=99.41%
sda: ios=0/4826, merge=0/4, ticks=0/1147779, in_queue=1147776, util=99.41%


Testing Read Bandwidth...
read_bw: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=libaio, iodepth=64
fio-3.13
Starting 1 process

read_bw: (groupid=0, jobs=1): err= 0: pid=49: Mon Feb 6 06:28:09 2023
read: IOPS=128, BW=16.5MiB/s (17.3MB/s)(280MiB/16965msec)
bw ( KiB/s): min= 256, max=50176, per=100.00%, avg=17994.26, stdev=14173.96, samples=31
iops : min= 2, max= 392, avg=140.48, stdev=110.71, samples=31
cpu : usr=0.18%, sys=1.66%, ctx=2092, majf=0, minf=16
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=100.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
issued rwts: total=2180,0,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
READ: bw=16.5MiB/s (17.3MB/s), 16.5MiB/s-16.5MiB/s (17.3MB/s-17.3MB/s), io=280MiB (294MB), run=16965-16965msec

Disk stats (read/write):
dm-0: ios=2366/3, merge=0/0, ticks=1093765/4077, in_queue=1100333, util=99.51%, aggrios=2366/6, aggrmerge=0/0, aggrticks=1096465/8183, aggrin_queue=1104639, aggrutil=99.42%
sda: ios=2366/6, merge=0/0, ticks=1096465/8183, in_queue=1104639, util=99.42%


Testing Write Bandwidth...
write_bw: (g=0): rw=randwrite, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=libaio, iodepth=64
fio-3.13
Starting 1 process

write_bw: (groupid=0, jobs=1): err= 0: pid=65: Mon Feb 6 06:28:27 2023
write: IOPS=69, BW=9453KiB/s (9680kB/s)(145MiB/15707msec); 0 zone resets
bw ( KiB/s): min= 256, max=29952, per=100.00%, avg=10800.19, stdev=9114.78, samples=26
iops : min= 2, max= 234, avg=84.35, stdev=71.20, samples=26
cpu : usr=0.15%, sys=0.67%, ctx=555, majf=0, minf=16
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=100.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=99.9%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
issued rwts: total=0,1097,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
WRITE: bw=9453KiB/s (9680kB/s), 9453KiB/s-9453KiB/s (9680kB/s-9680kB/s), io=145MiB (152MB), run=15707-15707msec

Disk stats (read/write):
dm-0: ios=0/1499, merge=0/0, ticks=0/1047546, in_queue=1137554, util=99.39%, aggrios=0/1502, aggrmerge=0/3, aggrticks=0/1152415, aggrin_queue=1152412, aggrutil=99.25%
sda: ios=0/1502, merge=0/3, ticks=0/1152415, in_queue=1152412, util=99.25%


Testing Read Latency...
read_latency: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=4
fio-3.13
Starting 1 process

read_latency: (groupid=0, jobs=1): err= 0: pid=81: Mon Feb 6 06:28:46 2023
read: IOPS=9, BW=39.7KiB/s (40.7kB/s)(652KiB/16407msec)
slat (usec): min=25, max=224, avg=106.02, stdev=36.24
clat (usec): min=165, max=3176.6k, avg=395710.97, stdev=707730.31
lat (usec): min=227, max=3176.7k, avg=395817.82, stdev=707732.74
clat percentiles (usec):
| 1.00th=[ 229], 5.00th=[ 2147], 10.00th=[ 10683],
| 20.00th=[ 23462], 30.00th=[ 35390], 40.00th=[ 57934],
| 50.00th=[ 91751], 60.00th=[ 145753], 70.00th=[ 263193],
| 80.00th=[ 505414], 90.00th=[1652556], 95.00th=[2021655],
| 99.00th=[3170894], 99.50th=[3170894], 99.90th=[3170894],
| 99.95th=[3170894], 99.99th=[3170894]
bw ( KiB/s): min= 7, max= 216, per=100.00%, avg=63.80, stdev=57.40, samples=20
iops : min= 1, max= 54, avg=15.80, stdev=14.41, samples=20
lat (usec) : 250=1.25%, 500=0.62%, 1000=1.25%
lat (msec) : 2=1.88%, 4=2.50%, 10=2.50%, 20=7.50%, 50=19.38%
lat (msec) : 100=16.25%, 250=18.12%, 500=10.00%, 750=6.25%, 1000=1.25%
cpu : usr=0.01%, sys=0.13%, ctx=162, majf=0, minf=17
IO depths : 1=0.0%, 2=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
issued rwts: total=160,0,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
READ: bw=39.7KiB/s (40.7kB/s), 39.7KiB/s-39.7KiB/s (40.7kB/s-40.7kB/s), io=652KiB (668kB), run=16407-16407msec

Disk stats (read/write):
dm-0: ios=195/15, merge=0/0, ticks=62870/9094, in_queue=113278, util=99.51%, aggrios=195/22, aggrmerge=0/4, aggrticks=72364/49995, aggrin_queue=122359, aggrutil=99.49%
sda: ios=195/22, merge=0/4, ticks=72364/49995, in_queue=122359, util=99.49%


Testing Write Latency...
write_latency: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=4
fio-3.13
Starting 1 process

write_latency: (groupid=0, jobs=1): err= 0: pid=97: Mon Feb 6 06:29:04 2023
write: IOPS=30, BW=123KiB/s (126kB/s)(1868KiB/15174msec); 0 zone resets
slat (usec): min=11, max=235, avg=58.23, stdev=38.98
clat (usec): min=205, max=2619.4k, avg=136822.73, stdev=357077.08
lat (usec): min=281, max=2619.5k, avg=136881.82, stdev=357079.12
clat percentiles (usec):
| 1.00th=[ 277], 5.00th=[ 383], 10.00th=[ 537],
| 20.00th=[ 10421], 30.00th=[ 17171], 40.00th=[ 23200],
| 50.00th=[ 32375], 60.00th=[ 44303], 70.00th=[ 68682],
| 80.00th=[ 92799], 90.00th=[ 274727], 95.00th=[ 784335],
| 99.00th=[1937769], 99.50th=[2365588], 99.90th=[2634023],
| 99.95th=[2634023], 99.99th=[2634023]
bw ( KiB/s): min= 7, max= 688, per=100.00%, avg=184.90, stdev=186.47, samples=20
iops : min= 1, max= 172, avg=46.00, stdev=46.69, samples=20
lat (usec) : 250=0.43%, 500=9.05%, 750=2.59%
lat (msec) : 2=0.86%, 4=1.29%, 10=5.82%, 20=15.73%, 50=26.72%
lat (msec) : 100=19.40%, 250=7.54%, 500=5.39%, 750=0.43%, 1000=1.29%
cpu : usr=0.06%, sys=0.20%, ctx=222, majf=0, minf=17
IO depths : 1=0.0%, 2=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
issued rwts: total=0,464,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=4

Run status group 0 (all jobs):
WRITE: bw=123KiB/s (126kB/s), 123KiB/s-123KiB/s (126kB/s-126kB/s), io=1868KiB (1913kB), run=15174-15174msec

Disk stats (read/write):
dm-0: ios=0/475, merge=0/0, ticks=0/68658, in_queue=69443, util=99.48%, aggrios=0/475, aggrmerge=0/0, aggrticks=0/69983, aggrin_queue=69983, aggrutil=99.40%
sda: ios=0/475, merge=0/0, ticks=0/69983, in_queue=69983, util=99.40%


Testing Read Sequential Speed...
read_seq: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=libaio, iodepth=16
fio-3.13
Starting 1 thread

read_seq: (groupid=0, jobs=1): err= 0: pid=113: Mon Feb 6 06:29:28 2023
read: IOPS=17, BW=17.8MiB/s (18.7MB/s)(358MiB/20078msec)
bw ( KiB/s): min= 2043, max=81920, per=100.00%, avg=24218.28, stdev=22785.58, samples=29
iops : min= 1, max= 80, avg=23.52, stdev=22.30, samples=29
cpu : usr=0.03%, sys=0.60%, ctx=351, majf=0, minf=0
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=100.0%, 32=0.0%, >=64=0.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=99.7%, 8=0.0%, 16=0.3%, 32=0.0%, 64=0.0%, >=64=0.0%
issued rwts: total=343,0,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
READ: bw=17.8MiB/s (18.7MB/s), 17.8MiB/s-17.8MiB/s (18.7MB/s-18.7MB/s), io=358MiB (375MB), run=20078-20078msec

Disk stats (read/write):
dm-0: ios=764/10, merge=0/0, ticks=589803/16574, in_queue=609402, util=99.61%, aggrios=764/13, aggrmerge=0/1, aggrticks=592905/15938, aggrin_queue=608802, aggrutil=99.52%
sda: ios=764/13, merge=0/1, ticks=592905/15938, in_queue=608802, util=99.52%


Testing Write Sequential Speed...
write_seq: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=libaio, iodepth=16
...
fio-3.13
Starting 4 threads
write_seq: Laying out IO file (1 file / 2798MiB)

write_seq: (groupid=0, jobs=1): err= 0: pid=129: Mon Feb 6 06:29:59 2023
write: IOPS=5, BW=6679KiB/s (6839kB/s)(187MiB/28670msec); 0 zone resets
bw ( KiB/s): min= 2043, max=67449, per=59.42%, avg=16001.91, stdev=15698.61, samples=22
iops : min= 1, max= 65, avg=15.50, stdev=15.23, samples=22
cpu : usr=0.04%, sys=0.17%, ctx=149, majf=0, minf=0
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=100.0%, 32=0.0%, >=64=0.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=99.4%, 8=0.0%, 16=0.6%, 32=0.0%, 64=0.0%, >=64=0.0%
issued rwts: total=0,172,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=16
write_seq: (groupid=0, jobs=1): err= 0: pid=130: Mon Feb 6 06:29:59 2023
write: IOPS=6, BW=6827KiB/s (6991kB/s)(189MiB/28350msec); 0 zone resets
bw ( KiB/s): min= 2048, max=57344, per=60.09%, avg=16183.05, stdev=15552.27, samples=21
iops : min= 2, max= 56, avg=15.67, stdev=15.15, samples=21
cpu : usr=0.04%, sys=0.18%, ctx=160, majf=0, minf=0
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=100.0%, 32=0.0%, >=64=0.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=99.4%, 8=0.0%, 16=0.6%, 32=0.0%, 64=0.0%, >=64=0.0%
issued rwts: total=0,174,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=16
write_seq: (groupid=0, jobs=1): err= 0: pid=131: Mon Feb 6 06:29:59 2023
write: IOPS=6, BW=6932KiB/s (7098kB/s)(189MiB/27921msec); 0 zone resets
bw ( KiB/s): min= 2048, max=55185, per=75.56%, avg=20347.18, stdev=14701.69, samples=17
iops : min= 2, max= 53, avg=19.71, stdev=14.20, samples=17
cpu : usr=0.03%, sys=0.19%, ctx=163, majf=0, minf=0
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=100.0%, 32=0.0%, >=64=0.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=99.4%, 8=0.0%, 16=0.6%, 32=0.0%, 64=0.0%, >=64=0.0%
issued rwts: total=0,174,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=16
write_seq: (groupid=0, jobs=1): err= 0: pid=132: Mon Feb 6 06:29:59 2023
write: IOPS=6, BW=6767KiB/s (6929kB/s)(189MiB/28600msec); 0 zone resets
bw ( KiB/s): min= 2048, max=67584, per=60.14%, avg=16196.64, stdev=15468.10, samples=22
iops : min= 2, max= 66, avg=15.73, stdev=15.17, samples=22
cpu : usr=0.04%, sys=0.19%, ctx=162, majf=0, minf=0
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=100.0%, 32=0.0%, >=64=0.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=99.4%, 8=0.0%, 16=0.6%, 32=0.0%, 64=0.0%, >=64=0.0%
issued rwts: total=0,174,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
WRITE: bw=26.3MiB/s (27.6MB/s), 6679KiB/s-6932KiB/s (6839kB/s-7098kB/s), io=754MiB (791MB), run=27921-28670msec

Disk stats (read/write):
dm-0: ios=0/1536, merge=0/0, ticks=0/3438611, in_queue=3460488, util=99.70%, aggrios=0/1553, aggrmerge=0/0, aggrticks=0/3505870, aggrin_queue=3506177, aggrutil=99.72%
sda: ios=0/1553, merge=0/0, ticks=0/3505870, in_queue=3506177, util=99.72%


Testing Read/Write Mixed...
rw_mix: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.13
Starting 1 process

rw_mix: (groupid=0, jobs=1): err= 0: pid=148: Mon Feb 6 06:30:22 2023
read: IOPS=294, BW=1185KiB/s (1214kB/s)(23.5MiB/20301msec)
bw ( KiB/s): min= 24, max=10088, per=100.00%, avg=2173.91, stdev=2974.57, samples=22
iops : min= 6, max= 2522, avg=543.41, stdev=743.66, samples=22
write: IOPS=93, BW=377KiB/s (386kB/s)(7656KiB/20301msec); 0 zone resets
bw ( KiB/s): min= 8, max= 3000, per=100.00%, avg=754.55, stdev=998.04, samples=20
iops : min= 2, max= 750, avg=188.60, stdev=249.52, samples=20
cpu : usr=0.25%, sys=1.12%, ctx=2198, majf=0, minf=16
IO depths : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=100.0%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
issued rwts: total=5971,1896,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
READ: bw=1185KiB/s (1214kB/s), 1185KiB/s-1185KiB/s (1214kB/s-1214kB/s), io=23.5MiB (24.6MB), run=20301-20301msec
WRITE: bw=377KiB/s (386kB/s), 377KiB/s-377KiB/s (386kB/s-386kB/s), io=7656KiB (7840kB), run=20301-20301msec

Disk stats (read/write):
dm-0: ios=2926/2535, merge=0/0, ticks=676178/621959, in_queue=1332944, util=99.60%, aggrios=2926/2630, aggrmerge=0/21, aggrticks=676172/1200603, aggrin_queue=1875954, aggrutil=100.00%
sda: ios=2926/2630, merge=0/21, ticks=676172/1200603, in_queue=1875954, util=100.00%


All tests complete.

==================
= Dbench Summary =
==================
Random Read/Write IOPS: 137/199. BW: 16.5MiB/s / 9453KiB/s
Average Latency (usec) Read/Write: 395817.82/136881.82
Sequential Read/Write: 17.8MiB/s / 26.3MiB/s
Mixed Random Read/Write IOPS: 294/93
```

