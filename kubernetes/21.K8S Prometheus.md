# 1. Prometheus (监控工具)

## 1.1 概述

Prometheus 是在 SoundCloud 上构建并开源的系统监视和警报工具包。

Prometheus 非常适合记录**任何纯数字时间序列**。它既适合以机器为中心的监控，也适合监控高度动态的面向服务的体系结构。在微服务世界中，其对多维数据收集和查询的支持是一种特别的优势。 Prometheus 是为可靠性而设计的，在出现故障时，你可以使用该系统快速诊断问题。每个 Prometheus 服务器都是独立的，而不依赖于网络存储或其他远程服务。当基础结构的其他部分损坏时单独依赖它就行，而且不需要设置大量的基础设施来使用它。



**主要特征**：

- 多维数据模型（时序数据由 **metric** 名和一组 **key/value** 组成）
- 提供 **`PromQL`** 查询语言，可利用多维数据完成复杂的查询 

- 不依赖分布式存储，支持服务器**本地存储**
- 基于 HTTP 的 **Pull** 方式采集时间序数据
- 通过 `PushGateway` 可以支持 **Push** 模式推送 **时间序列**

- 可通过 **动态服务发现** 或 **静态配置** 等方式发现目标对象



组件说明：

- Prometheus：采用HTTP Pull方式从 apiserver, scheduler, controller-manager, kubelet 等组件抓取指标、存储时间序列数据
- Grafana: 可视化数据统计和监控平台
- NodeExporter: 各个node的关键度量指标状态数据
- KubeStateMetrics: 收集kuberenets集群内资源对象数据，制定告警规则
- cAdvisor: 容器相关数据采集
- PushGateway: 支持短期工作的推送网关
- AlertManager: 用于处理报警的组件



生态组件架构图：

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-architecture.png) 



## 1.2 部署

### 1.2.1 权限控制

Prometheus 通过 `kube-apiserver` 获取数据，需要先创建特定RBAC

```yaml
# prometheus-rbac.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["nodes","nodes/proxy","services","endpoints","pods"]
  verbs: ["get", "list", "watch"] 
- apiGroups: ["extensions"]
  resources: ["ingress"]
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
  namespace: kube-system
roleRef: 
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: kube-system
```



### 1.2.2 存储准备

暂时使用 NFS 做存储介质

```yaml
# prometheus-storage.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    k8s-app: prometheus
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: prom-nfs-storage
  mountOptions:
  - hard
  - nfsvers=4.2
  nfs:
    server: 192.168.3.103
    path: /mnt/nfs_share
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    k8s-app: prometheus
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: prom-nfs-storage
  resources:
    requests:
      storage: 5Gi
  selector:
    matchLabels:
      k8s-app: prometheus
```



### 1.2.3 配置 Prometheus

```yaml
# prometheus-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
      external_labels:
        cluster: "kubernetes"
        
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets: ['127.0.0.1:9090']
        labels:
          instance: prometheus   
```



### 1.2.4 部署 Prometheus

```yaml
# prometheus-deploy.yml
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    k8s-app: prometheus
spec:
  type: NodePort
  ports:
  - name: http
    port: 9090
    targetPort: 9090
    nodePort: 30090
  selector:
    k8s-app: prometheus
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    k8s-app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: prometheus
  template:
    metadata:
      labels:
        k8s-app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.26.1
        ports:
        - name: http
          containerPort: 9090
        securityContext:
          runAsUser: 65534
          privileged: true
        command:
        - "/bin/prometheus"
        args:
        - "--config.file=/etc/prometheus/prometheus.yml"
        - "--web.enable-lifecycle"
        - "--storage.tsdb.path=/prometheus"
        - "--storage.tsdb.retention.time=10d"
        - "--web.console.libraries=/etc/prometheus/console_libraries"
        - "--web.console.templates=/etc/prometheus/consoles"
        resources:
          limits:
            cpu: 2000m
            memory: 1024Mi
          requests:
            cpu: 1000m
            memory: 512Mi
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 5
          timeoutSeconds: 10
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        volumeMounts:
        - name: data
          mountPath: /prometheus
          subPath: prometheus
        - name: config
          mountPath: /etc/prometheus
      - name: configmap-reload
        image: jimmidyson/configmap-reload:v0.7.1
        args:
        - "--volume-dir=/etc/config"
        - "--webhook-url=http://localhost:9090/-/reload"
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
          requests:
            cpu: 10m
            memory: 10Mi
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: prometheus
      - name: config
        configMap:
          name: prometheus-config
```



## 1.3 登录

访问地址：http://192.168.80.100:30090/

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-ui.png) 



# 2. Grafana (可视化工具)

## 2.1 概述

Grafana 是一个开源的度量分析和可视化工具，它提供查询、可视化、告警和指标展示等功能，能够灵活创建图表、仪表盘等可视化界面。

主要的功能：

- 可视化：支持多种类型的图形，能灵活绘制不同样式
- 动态仪表盘：提供以模板和变量的方式来创建动态且可重复使用的仪表盘
- 浏览指标：通过瞬时查询和动态变化等方式展示数据，可根据不同的时间范围拆分视图
- 浏览日志：可快速搜索所有日志或实时流式传输的数据
- 告警：可直观地根据重要的指标定义警报规则
- 混合数据源：在同一个图种混合不同的数据源，可以基于每个查询指定不同的数据源

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-features.png) 



## 2.2 部署

### 2.2.1 存储准备

```yaml
# grafana-storage.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: grafana
  namespace: kube-system
  labels:
    k8s-app: grafana
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: grafana-nfs-storage
  mountOptions:
  - hard
  - nfsvers=4.2
  nfs:
    server: 192.168.3.103
    path: /mnt/nfs_share
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: grafana
  namespace: kube-system
  labels:
    k8s-app: grafana
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: grafana-nfs-storage
  resources:
    requests:
      storage: 5Gi
  selector:
    matchLabels:
      k8s-app: grafana
```



### 2.2.2 部署 Grafana

```yaml
# grafana-deploy.yml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: kube-system
  labels:
    k8s-app: grafana
spec:
  type: NodePort
  ports:
  - name: http
    port: 3000
    targetPort: 3000
    nodePort: 30000
  selector:
    k8s-app: grafana
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: kube-system
  labels:
    k8s-app: grafana
spec:
  selector:
    matchLabels:
      k8s-app: grafana
  template:
    metadata:
      labels:
        k8s-app: grafana
    spec:
      securityContext:
        runAsNonRoot: false
        fsGroup: 0
        supplementalGroups:
        - 0
      containers:                
      - name: grafana
        image: grafana/grafana:8.3.6
        ports:
        - name: http
          containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_USER
          value: "admin"
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ORG_NAME
          value: ANONYMOUS
        - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          value: Viewer
        - name: GF_SECURITY_ALLOW_EMBEDDING
          value: "true"
        resources:
          limits:
            cpu: 2
            memory: 1Gi
          requests:
            cpu: 1
            memory: 512Mi
        readinessProbe:
          failureThreshold: 10
          httpGet:
            path: /api/health
            port: 3000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        livenessProbe:
          failureThreshold: 10
          httpGet:
            path: /api/health
            port: 3000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        volumeMounts:
        - name: data
          mountPath: /var/lib/grafana
          subPath: grafana
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: grafana
```



## 2.3 数据源

http://192.168.80.100:30000    admin/admin

**新增数据源：**

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-ds-new.png)

**配置数据源**：

```bash
# 获取数据源 dns 地址
$ kubectl run dns -it --rm --image=e2eteam/dnsutils:1.1 -- /bin/sh
/ # nslookup prometheus.kube-system
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   prometheus.kube-system.svc.cluster.local
Address: 10.109.159.64
```

数据源地址：`http://prometheus.kube-system.svc.cluster.local:9090` 

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-ds-prometheus.png)



# 3. Node Exporter (集群节点监控)

## 3.1 概述

Node Exporter 是 Prometheus 官方提供的节点资源采集组件，用于收集节点服务器资源，如 **CPU频率信息**、**磁盘IO统计**、**剩余内存** 等。并将这些信息转换成 Prometheus 可识别的  **Metrics** 数据。



## 3.2 部署

```yaml
# node-exporter-deploy.yml
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  namespace: kube-system
  labels:
    k8s-app: node-exporter
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 9100
    targetPort: 9100
  selector:
    k8s-app: node-exporter
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-system
  labels:
    k8s-app: node-exporter
spec:
  selector:
    matchLabels:
      k8s-app: node-exporter
  template:
    metadata:
      labels:
        k8s-app: node-exporter
    spec:
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.2.2
        ports:
        - name: metrics
          containerPort: 9100
        args:
        - "--path.procfs=/host/proc"
        - "--path.sysfs=/host/sys"
        - "--path.rootfs=/host"
        volumeMounts:
        - name: dev
          mountPath: /host/dev
        - name: proc
          mountPath: /host/proc
        - name: sys
          mountPath: /host/sys
        - name: rootfs
          mountPath: /host
      volumes:
        - name: dev
          hostPath:
            path: /dev
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs
          hostPath:
            path: /
      hostPID: true
      hostNetwork: true
      tolerations:
      - operator: "Exists"
```



收集数据测试：

```bash
$ curl -kL http://127.0.0.1:9100/metrics
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
...
```



## 3.3 配置 Prometheus

每个节点上的 `Node Exporter` 都会通过 `9100` 端口和 `/metrics` 接口暴露节点节点监控指标数据。要想采集这些指标数据，需要在 `Prometheus` 配置文件中，添加全部的 `Node Exporter` 的 `地址` 与 `端口` 这样的静态配置：

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-node-exporter.png)

```yaml
scrape_configs:
- job_name: 'node-exporter'
  kubernetes_sd_configs: # 服务发现配置
  - role: node           # 服务发现模式为node， 即从kubernetes集群中的每个节点发现目标，默认地址为kubelet的HTTP端口
  relabel_configs:       # 对采集的标签进行重新标记
  - action: replace
    source_labels: [__address__] # 从得到的标签列表中找到 __address__ 标签的值，即 kueblet 的地址
    regex: '(.*):10250'          # 使用正则表达式截取标签值中的IP部分
    replacement: '${1}:9100'     # {1} 上述正则表达截取的IP地址
    target_label: __address__
```



**Prometheus 完整配置**：

```bash
# prometheus-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
      external_labels:
        cluster: "kubernetes"
        
    scrape_configs:
    ###################### Node Exporter ######################
    - job_name: 'node-exporter'
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: replace
        source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:9100'
        target_label: __address__ 
```



**更新配置后，强制刷新**：

```bash
curl -X POST http://192.168.80.100:30090/-/reload
```



## 3.4 集群节点看板

步骤1： 新增看板 

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-new.png)

步骤2：导入 ID为 **8919** 的 `Node Exporter` 模板，然后加载到配置数据库

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-load-8919.png)

步骤3：选择使用上面配置的 `Prometheus` 数据库，之后点击 `Import` 按钮进入看板

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-import.png)

步骤4：监控信息页面

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-node-exporter.png)



# 4. StateMetrics+cAdvisor (集群服务监控)

## 4.1 概述

**`cAdvisor`**: Container Advisor，Google开源的容器监控工具，可用于对容器资源的使用情况和性能进行监控。它以守护进程方式运行，用于收集、聚合、处理和导出容器的运行信息，包含完整历史资源使用情况和网络统计等信息。在 K8S 中，`kubelet` 组件集成了 `cAdvisor`，无需单独安装。

**`KubeStateMetrics`**: 是一个独立服务，支持从 `Kubernetes API` 对象中获取指标数据，这个过程不会对这些原始数据进行修改。



## 4.2 部署

### 4.2.1 权限控制

KubeStateMetrics 通过 `kube-apiserver` 获取数据，需要先创建特定RBAC

```yaml
# kube-state-metrics-rbac.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: kube-system
  labels:
    k8s-app: kube-state-metrics
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
  namespace: kube-system
  labels:
    k8s-app: kube-state-metrics
rules:
- apiGroups: [""]
  resources: ["configmaps","secrets","nodes","pods",
              "services","resourcequotas",
              "replicationcontrollers","limitranges",
              "persistentvolumeclaims","persistentvolumes",
              "namespaces","endpoints"]
  verbs: ["list","watch"]
- apiGroups: ["extensions"]
  resources: ["daemonsets","deployments","replicasets"]
  verbs: ["list","watch"]
- apiGroups: ["apps"]
  resources: ["statefulsets","daemonsets","deployments","replicasets"]
  verbs: ["list","watch"]
- apiGroups: ["batch"]
  resources: ["cronjobs","jobs"]
  verbs: ["list","watch"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["list","watch"]
- apiGroups: ["authentication.k8s.io"]
  resources: ["tokenreviews"]
  verbs: ["create"]
- apiGroups: ["authorization.k8s.io"]
  resources: ["subjectaccessreviews"]
  verbs: ["create"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["list","watch"]
- apiGroups: ["certificates.k8s.io"]
  resources: ["certificatesigningrequests"]
  verbs: ["list","watch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses","volumeattachments"]
  verbs: ["list","watch"]
- apiGroups: ["admissionregistration.k8s.io"]
  resources: ["mutatingwebhookconfigurations","validatingwebhookconfigurations"]
  verbs: ["list","watch"]
- apiGroups: ["networking.k8s.io"]
  resources: ["networkpolicies","ingresses"]
  verbs: ["list","watch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["list","watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
  namespace: kube-system
  labels:
    k8s-app: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
```



### 4.2.2 部署 KubeStateMetrics

```yaml
# kube-state-metrics-deploy.yml
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: kube-system
  labels:
    k8s-app: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics   # prometheus自动发现
spec:
  type: ClusterIP
  ports:
  - name: http-metrics
    port: 8080
    targetPort: 8080
  - name: telemetry
    port: 8081
    targetPort: 8081
  selector:
    k8s-app: kube-state-metrics
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: kube-system
  labels:
    k8s-app: kube-state-metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: kube-state-metrics
  template:
    metadata:
      labels:
        k8s-app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: bitnami/kube-state-metrics:2.2.4
        securityContext:
          runAsUser: 65534
        ports:
        - name: http-metrics
          containerPort: 8080
        - name: telemetry 
          containerPort: 8081
        resources:
          limits:
            cpu: 200m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 5
          timeoutSeconds: 5
```



**访问 KubeStateMetrics 暴露的指标数据**:

```bash
$ curl -kL $(kubectl get service -n kube-system | grep kube-state-metrics | awk '{print $3}'):8080/metrics
...
# TYPE kube_configmap_info gauge
kube_configmap_info{namespace="kube-system",configmap="extension-apiserver-authentication"} 1
kube_configmap_info{namespace="kube-system",configmap="kube-proxy"} 1
kube_configmap_info{namespace="kube-system",configmap="kube-root-ca.crt"} 1
kube_configmap_info{namespace="kube-system",configmap="kubeadm-config"} 1
kube_configmap_info{namespace="kube-public",configmap="kube-root-ca.crt"} 1
kube_configmap_info{namespace="kube-system",configmap="kube-flannel-cfg"} 1
kube_configmap_info{namespace="kube-system",configmap="kubelet-config-1.21"} 1
kube_configmap_info{namespace="kube-public",configmap="cluster-info"} 1
kube_configmap_info{namespace="kube-system",configmap="coredns"} 1
kube_configmap_info{namespace="kube-system",configmap="prometheus-config"} 1
kube_configmap_info{namespace="default",configmap="kube-root-ca.crt"} 1
kube_configmap_info{namespace="kube-node-lease",configmap="kube-root-ca.crt"} 1
...
```



## 4.3 配置 Prometheus

### 4.3.1 cAdvisor

获取 kubelet 的metrics 数据，需要通过 `kube-apiserver` 提供的 api 做代理: `https://kube-apiserver:443/api/v1/nodes/${NODE_NAME}/proxy/metrics/cadivisor`

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-access-cadvisor.png)

```yaml
- job_name: 'kubernetes-cadvisor'
  scheme: https
  metrics_path: /metrics/cadvisor
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  kubernetes_sd_configs: # 服务发现配置
  - role: node           # 服务发现模式为node， 即从kubernetes集群中的每个节点发现目标，默认地址为kubelet的HTTP端口
  relabel_configs:       # 对采集的标签进行重新标记
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)  # 截取正则表达匹配部分，替换原有标签，即去除标签前缀__meta_kubernetes_node_label_
  - target_label: __address__                 # 修改指标数据采集address为 kubernetes.default.svc:443
    replacement: kubernetes.default.svc:443   
  - source_labels: [__meta_kubernetes_node_name]
    target_label: __metrics_path__            
    replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor  # ${1} 获取从标签列表中获取到的 node_name

  ## 用于适配对应的Grafana Dashboard图表，编号13105
  metric_relabel_configs:
  - source_labels: [instance]
    separator: ;
    regex: (.+)
    target_label: node
    replacement: $1
    action: replace
  - source_labels: [pod_name]
    separator: ;
    regex: (.+)
    target_label: pod
    replacement: $1
    action: replace
  - source_labels: [container_name]
    separator: ;
    regex: (.+)
    target_label: container
    replacement: $1
    action: replace
```



### 4.3.2 KubeStateMetrics

```yaml
- job_name: "kube-state-metrics"
  kubernetes_sd_configs:
  - role: endpoints  # 服务发现模式为endpoints，它调用kube-apiserver的接口获取指标数据
    namespaces:      # 限定只获取命名空间为kube-system的endpoint信息
      names: ["kube-system"]
  relabel_configs:
  ## 指定从 app.kubernetes.io/name 标签等于 kube-state-metrics 的 service 服务获取指标信息
  - action: keep
    source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
    regex: kube-state-metrics
  ## 配置为了适配 Grafana Dashboard 模板(编号1310)
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - action: replace
    source_labels: [__meta_kubernetes_namespace]
    target_label: k8s_namespace
  - action: replace
    source_labels: [__meta_kubernetes_service_name]
    target_label: k8s_sname
```

相关的标签：

http://192.168.80.100:30090/service-discovery

```bash
__meta_kubernetes_service_label_app_kubernetes_io_name="kube-state-metrics"

__meta_kubernetes_service_label_app="kube-state-metrics"
__meta_kubernetes_service_label_app_kubernetes_io_name="kube-state-metrics"
__meta_kubernetes_service_label_app="node-exporter"
__meta_kubernetes_service_label_k8s_app="kube-dns"
__meta_kubernetes_service_label_kubernetes_io_cluster_service="true"
__meta_kubernetes_service_label_kubernetes_io_name="CoreDNS"
__meta_kubernetes_service_label_app="grafana"

__meta_kubernetes_namespace="kube-system"

__meta_kubernetes_service_name="kube-state-metrics"
__meta_kubernetes_service_name="node-exporter"
__meta_kubernetes_service_name="kube-dns"
__meta_kubernetes_service_name="prometheus"
_meta_kubernetes_service_name="grafana"
```



### 4.3.3 Prometheus 集成

```yaml
# prometheus-config.yml
kind: ConfigMap
apiVersion: v1
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
      external_labels:
        cluster: "kubernetes"
    scrape_configs:
    ############################ kubernetes-cadvisor ##############################
    - job_name: 'kubernetes-cadvisor'
      scheme: https
      metrics_path: /metrics/cadvisor
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
      metric_relabel_configs:
      - source_labels: [instance]
        separator: ;
        regex: (.+)
        target_label: node
        replacement: $1
        action: replace
      - source_labels: [pod_name]
        separator: ;
        regex: (.+)
        target_label: pod
        replacement: $1
        action: replace
      - source_labels: [container_name]
        separator: ;
        regex: (.+)
        target_label: container
        replacement: $1
        action: replace
    ############################ kube-state-metrics ##############################
    - job_name: "kube-state-metrics"
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names: ["kube-system"]
      relabel_configs:
      - action: keep
        source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
        regex: kube-state-metrics
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels: [__meta_kubernetes_namespace]
        target_label: k8s_namespace
      - action: replace
        source_labels: [__meta_kubernetes_service_name]
        target_label: k8s_sname    
```

**更新配置后，强制刷新**：

```bash
curl -X POST http://192.168.80.100:30090/-/reload
```



## 4.6 集群服务看板

步骤1： 新增看板 

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-new.png)

步骤2：导入 ID为 **13105** 的 `cAdvisor` 模板，然后加载到配置数据库

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-load-13105.png)

步骤3：选择使用上面配置的 `Prometheus` 数据库，之后点击 `Import` 按钮进入看板

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-import.png)

步骤4：监控信息页面

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-kubestatemetrics-cadvisor.png)



# 5. 监控 Service

## 5.1 服务数据采集

Prometheus 自动采集 kuberenets 服务数据方式：

- **静态配置**：将要采集的 目标地址、端口、接口等添加到 Prometheus 配置中

- **动态配置**：使用服务发现机制，动态发现指定的服务。Prometheus 支持 Consule、DNS、Kubernetes 等动态服务发现。可根据指定条件获取要采集的目标地址、端口、接口等信息，然后添加到 Prometheus Target 目标中

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-service-discovery.png)



## 5.2 配置 Prometheus

在 Prometheus 配置文件中，添加 kubernetes endpoints 服务发现机制，并且配置标签，这样服务发现只会去找指定了特定 annotations 的 Service 资源

补充：**`relabel_config` action** 解释

- `replace`: Match `regex` against the concatenated `source_labels`. Then, set `target_label` to `replacement`, with match group references (`${1}`, `${2}`, ...) in `replacement` substituted by their value. If `regex` does not match, no replacement takes place.
- `keep`: Drop targets for which `regex` does not match the concatenated `source_labels`.
- `drop`: Drop targets for which `regex` matches the concatenated `source_labels`.
- `hashmod`: Set `target_label` to the `modulus` of a hash of the concatenated `source_labels`.
- `labelmap`: Match `regex` against all label names. Then copy the values of the matching labels to label names given by `replacement` with match group references (`${1}`, `${2}`, ...) in `replacement` substituted by their value.
- `labeldrop`: Match `regex` against all label names. Any label that matches will be removed from the set of labels.
- `labelkeep`: Match `regex` against all label names. Any label that does not match will be removed from the set of labels.

```yaml
# prometheus-config.yml
kind: ConfigMap
apiVersion: v1
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
      external_labels:
        cluster: "kubernetes"
    scrape_configs:    
    ############################### kubernetes-service-endpoints #########################################
    - job_name: 'kubernetes-service-endpoints'
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        regex: "true"
      - action: replace
        source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        regex: (https?)
        target_label: __scheme__
      - action: replace
        source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        regex: ([^:]+)(?::\d+)?;(\d+)
        target_label: __address__
        replacement: $1:$2
      - action: replace
        source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        regex: (.+)
        target_label: __metrics_path__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - action: replace
        source_labels: [__meta_kubernetes_service_name]
        target_label: kubernetes_name
      - action: replace
        source_labels: [__address__]
        target_label: instance
        regex: (.+):(.+)    
```

**更新配置后，强制刷新**：

```bash
curl -X POST http://192.168.80.100:30090/-/reload
```



## 5.3 自动采集 CoreDNS 指标

### 5.3.1 CoreDNS 服务添加标签

```yaml
# coredns-svc-patch.yml
metadata:
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/scheme: "http"
    prometheus.io/port: "9153"
    prometheus.io/path: "/metrics"
```

CoreDNS 服务打上 patch：

```bash
kubectl patch svc kube-dns -n kube-system --type merge --patch-file coredns-svc-patch.yml
```



### 5.3.2 观察采集指标

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-service-targets.png)



# 6. 监控 ETCD

## 6.1 采集 ETCD 指标数据

在 Kuberenetes 集群的 ETCD 默认时开启暴露 metrics 数据，但在 ETCD 部署在集群外，并且其暴露的接口是基于 HTTPS 协议。为统一管理，需要将 ETCD 服务代理到 Kuberetnets 集群中，然后使用 Prometheus 的 Kubernetes 动态服务发现机制。

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-etcd-metrics.png)



## 6.2 ETCD 服务代理到 Kuberenetes 集群

```yaml
# etcd-proxy.yml
apiVersion: v1
kind: Service
metadata:
  name: etcd
  namespace: kube-system
  labels:
    k8s-app: etcd
    app.kubernetes.io/name: etcd
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: port
    port: 2379          
    protocol: TCP
---
apiVersion: v1
kind: Endpoints
metadata:
  name: etcd
  namespace: kube-system
  labels:
    k8s-app: etcd
subsets:
- addresses:
  - ip: 192.168.80.100   
  ports:
  - port: 2379
```



## 6.3 导入 ETCD 证书

### 6.3.1 ETCD 证书存入 ConfigMap

```bash
kubectl create secret generic etcd-certs \
  --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  --from-file=/etc/kubernetes/pki/etcd/ca.crt \
  -n kube-system
```



### 6.3.2 修改 Prometheus 部署参数

增加 etcd 证书挂载

```yaml
# prometheus-deploy.yml
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    k8s-app: prometheus
spec:
  type: NodePort
  ports:
  - name: http
    port: 9090
    targetPort: 9090
    nodePort: 30090
  selector:
    k8s-app: prometheus
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    k8s-app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: prometheus
  template:
    metadata:
      labels:
        k8s-app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.26.1
        ports:
        - name: http
          containerPort: 9090
        securityContext:
          runAsUser: 65534
          privileged: true
        command:
        - "/bin/prometheus"
        args:
        - "--config.file=/etc/prometheus/prometheus.yml"
        - "--web.enable-lifecycle"
        - "--storage.tsdb.path=/prometheus"
        - "--storage.tsdb.retention.time=10d"
        - "--web.console.libraries=/etc/prometheus/console_libraries"
        - "--web.console.templates=/etc/prometheus/consoles"
        resources:
          limits:
            cpu: 2000m
            memory: 1024Mi
          requests:
            cpu: 1000m
            memory: 512Mi
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 5
          timeoutSeconds: 10
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        volumeMounts:
        - name: data
          mountPath: /prometheus
          subPath: prometheus
        - name: config
          mountPath: /etc/prometheus
        - name: certs   # new add
          readOnly: true
          mountPath: /certs
      - name: configmap-reload
        image: jimmidyson/configmap-reload:v0.7.1
        args:
        - "--volume-dir=/etc/config"
        - "--webhook-url=http://localhost:9090/-/reload"
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
          requests:
            cpu: 10m
            memory: 10Mi
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: prometheus
      - name: config
        configMap:
          name: prometheus-config
      - name: certs   # new add
        secret:      
          secretName: etcd-certs      
```



## 6.4 配置 Prometheus

```yml
# prometheus-config.yml
kind: ConfigMap
apiVersion: v1
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
      external_labels:
        cluster: "kubernetes"
    scrape_configs:
    ###################### kubernetes-etcd ######################
    - job_name: "kubernetes-etcd"
      scheme: https
      tls_config:
        ca_file: /certs/ca.crt
        cert_file: /certs/healthcheck-client.crt
        key_file: /certs/healthcheck-client.key
        insecure_skip_verify: false
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:               
          names: ["kube-system"]         
      relabel_configs:
      - action: keep
        source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
        regex: etcd    
```

**更新配置后，强制刷新**：

```bash
curl -X POST http://192.168.80.100:30090/-/reload
```



## 6.5 观察采集指标

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-etcd-targets.png)



## 6.6 ETCD 集群看板

步骤1： 新增看板 

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-new.png)

步骤2：导入 ID为 **9733** 的 `ETCD` 模板，然后加载到配置数据库

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-load-9733.png)

步骤3：选择使用上面配置的 `Prometheus` 数据库，之后点击 `Import` 按钮进入看板

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-import.png)

步骤4：监控信息页面

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-etcd.png)



# 7. BlackBox Expertor

## 7.1 概述

BlackBox Expoter 是 Prometheus 官方提供的黑盒监控解决方案，允许用户通过 HTTP、HTTPS、DNS、TCP 及 ICMP 等方式对网络进行探测，这种探测方式常常用于探测一个服务的运行状态，观察服务是否正常运行。

黑盒和白盒监控：

- **黑盒**：以用户的身份测试服务的运行状态。常见的黑盒监控手段包括 HTTP、HTTPS 探针，DNS 探测、ICMP等。常用于检测站点与服务可用性、连通性，以及访问效率等
- **白盒**：一般指日常的服务器状态监控，如服务器资源使用量、容器的运行状态、中间件的稳定情况等一系列比较直观的监控数据。

黑白盒区别：

- 黑盒监控是以故障为主导，当被监控的服务发生故障时，能快速进行预警。
- 白盒监控则更偏向于主动的和提前预判方式，预测可能发生的故障。



## 7.2 部署

### 7.2.1 配置 BlackBox

```yaml
# blackbox-exporter-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: blackbox-exporter
  namespace: kube-system
  labels:
    k8s-app: blackbox-exporter
data:
  blackbox.yml: |-
    modules:
      ## ----------- DNS 探针 -----------
      dns_tcp:  
        prober: dns
        dns:
          transport_protocol: "tcp"
          preferred_ip_protocol: "ip4"
          query_name: "kubernetes.default.svc.cluster.local"
          query_type: "A" 
      ## ----------- TCP 探针 -----------
      tcp_connect:
        prober: tcp
        timeout: 5s
      ## ----------- ICMP 探针 -----------
      ping:
        prober: icmp
        timeout: 5s
        icmp:
          preferred_ip_protocol: "ip4"
      ## ----------- HTTP GET 2xx 探针 -----------
      http_get_2xx:  
        prober: http
        timeout: 10s
        http:
          method: GET
          preferred_ip_protocol: "ip4"
          valid_http_versions: ["HTTP/1.1","HTTP/2"]
          valid_status_codes: [200] 
          no_follow_redirects: false  
      ## ----------- HTTP GET 3xx 探针 -----------
      http_get_3xx:  
        prober: http
        timeout: 10s
        http:
          method: GET
          preferred_ip_protocol: "ip4"
          valid_http_versions: ["HTTP/1.1","HTTP/2"]
          valid_status_codes: [301,302,304,305,306,307] 
          no_follow_redirects: false          
      ## ----------- HTTP POST 探针 -----------
      http_post_2xx: 
        prober: http
        timeout: 10s
        http:
          method: POST
          preferred_ip_protocol: "ip4"
          valid_http_versions: ["HTTP/1.1", "HTTP/2"]
          headers:
             Content-Type: application/json
          body: '{"username": "admin", "password": "123456"}'  
```



### 7.2.2 部署 BlackBox

```yaml
# blackbox-exporter-deploy.yml
apiVersion: v1
kind: Service
metadata:
  name: blackbox-exporter
  namespace: kube-system
  labels:
    k8s-app: blackbox-exporter
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 9115
    targetPort: 9115
  selector:
    k8s-app: blackbox-exporter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blackbox-exporter
  namespace: kube-system
  labels:
    k8s-app: blackbox-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: blackbox-exporter
  template:
    metadata:
      labels:
        k8s-app: blackbox-exporter
    spec:
      containers:
      - name: blackbox-exporter
        image: prom/blackbox-exporter:v0.19.0
        args:
        - --config.file=/etc/blackbox_exporter/blackbox.yml
        - --web.listen-address=:9115
        - --log.level=info
        ports:
        - name: http
          containerPort: 9115
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 50Mi
        livenessProbe:
          tcpSocket:
            port: 9115
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          tcpSocket:
            port: 9115
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        volumeMounts:
        - name: config
          mountPath: /etc/blackbox_exporter
      volumes:
      - name: config
        configMap:
          name: blackbox-exporter
          defaultMode: 420
```



## 7.3 配置 Prometheus

### 7.3.1 创建 DNS 探测配置

```yaml
################ DNS 服务器监控 ###################
- job_name: "kubernetes-dns"
  metrics_path: /probe
  params:
    ## DNS 探针
    module: [dns_tcp]
  static_configs:
    - targets:
      - kube-dns.kube-system:53
      - 114.114.114.114
      - 8.8.8.8
      - 1.1.1.1
  relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: blackbox-exporter.kube-system:9115
```



### 7.3.2 创建 Service 探测配置

创建用于探测 Kubernetes 服务的配置，对那些配置了 `prometheus.io/http-probe: "true"` 标签的 **Kubernetes Service** 资源的健康状态进行探测

```yaml
- job_name: "kubernetes-services"
  metrics_path: /probe
  ## 使用HTTP_GET_2xx与HTTP_GET_3XX模块
  params: 
    module:
    - "http_get_2xx"
    - "http_get_3xx"
  ## 使用Kubernetes动态服务发现,且使用Service类型的发现
  kubernetes_sd_configs:
  - role: service
  relabel_configs:
    ## 设置只监测Kubernetes Service中Annotation里配置了注解prometheus.io/http_probe: true的service
  - action: keep
    source_labels: [__meta_kubernetes_service_annotation_prometheus_io_http_probe]
    regex: "true"
  - action: replace
    source_labels: 
    - "__meta_kubernetes_service_name"
    - "__meta_kubernetes_namespace"
    - "__meta_kubernetes_service_annotation_prometheus_io_http_probe_port"
    - "__meta_kubernetes_service_annotation_prometheus_io_http_probe_path"
    target_label: __param_target
    regex: (.+);(.+);(.+);(.+)
    replacement: $1.$2:$3$4
  - target_label: __address__
    replacement: blackbox-exporter.kube-system:9115
  - source_labels: [__param_target]
    target_label: instance
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    target_label: kubernetes_name
```



### 7.3.3 Prometheus 集成

```yaml
# prometheus-config.yml
kind: ConfigMap
apiVersion: v1
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
      external_labels:
        cluster: "kubernetes"
    scrape_configs:
    ################################## Kubernetes BlackBox DNS ###################################
    - job_name: "kubernetes-dns"
      metrics_path: /probe
      params:
        module: [dns_tcp]
      static_configs:
        - targets:
          - kube-dns.kube-system:53
          - 114.114.114.114
          - 8.8.8.8
          - 1.1.1.1
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: blackbox-exporter.kube-system:9115
    ################################## Kubernetes BlackBox Services ###################################
    - job_name: 'kubernetes-services'
      metrics_path: /probe
      params:
        module:
        - "http_get_2xx"
        - "http_get_3xx"
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        source_labels: [__meta_kubernetes_service_annotation_prometheus_io_http_probe]
        regex: "true"
      - action: replace
        source_labels: 
        - "__meta_kubernetes_service_name"
        - "__meta_kubernetes_namespace"
        - "__meta_kubernetes_service_annotation_prometheus_io_http_probe_port"
        - "__meta_kubernetes_service_annotation_prometheus_io_http_probe_path"
        target_label: __param_target
        regex: (.+);(.+);(.+);(.+)
        replacement: $1.$2:$3$4
      - target_label: __address__
        replacement: blackbox-exporter.kube-system:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        target_label: kubernetes_name    
```

**更新配置后，强制刷新**：

```bash
curl -X POST http://192.168.80.100:30090/-/reload
```



## 7.4 部署探测 Service 示例

```yaml
# nginx-deploy.yml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    k8s-app: nginx
  annotations:
    prometheus.io/http-probe: "true"        ### 设置该服务执行HTTP探测
    prometheus.io/http-probe-port: "80"     ### 设置HTTP探测的接口
    prometheus.io/http-probe-path: "/"      ### 设置HTTP探测的地址
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    k8s-app: nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    k8s-app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: nginx
  template:
    metadata:
      labels:
        k8s-app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```





## 7.5 观察采集指标

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/prometheus-blackbox-targets.png)



## 7.6 BlackBox Exporter 集群看板

步骤1： 新增看板 

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-new.png)

步骤2：导入 ID为 **`9965`** 的 `ETCD` 模板，然后加载到配置数据库

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-load-9965.png)

步骤3：选择使用上面配置的 `Prometheus` 数据库，之后点击 `Import` 按钮进入看板

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-import.png)

步骤4：监控信息页面

![img](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/kubernetes/grafana-dashboard-blackbox.png)
