常见限流手段：

1. 漏桶：一个一直装满水的桶，每隔固定的一段时间向外漏一滴水。如果你接到了这滴水，那么你就可以继续服务请求，如果没有接到，那么就需要等待下一滴水
2. 令牌桶：匀速想桶中添加令牌，服务请求时需要从桶中获取令牌，令牌的数目可以按照需要消耗的资源进行相应的调整。如果没有令牌，可以选择等待，或者放弃

区别：漏桶流出的速率固定。令牌桶只要在桶中有令牌，那就可以拿，即令牌桶允许一定程度的并发。



令牌桶原理：对全局计数的加减法操作，但这个计数需要添加读写锁

```go
func main() {
	var fillInterval = time.Millisecond * 500
	var capacity = 100
	var tokenBucket = make(chan struct{}, capacity)

	fillToken := func() {
		ticker := time.NewTicker(fillInterval)
		for {
			select {
			case <-ticker.C:
				select {
				case tokenBucket <- struct{}{}:
				default:
				}
				fmt.Println("current token count:", len(tokenBucket), time.Now())
			}
		}
	}

	go fillToken()

	time.Sleep(time.Minute * 5)
}


func TakeAvailable(block bool) bool {
	var takenResult bool
	if block {
		select {
		case <-tokenBucket:
			takenResult = true
		}
	} else {
		select {
		case <-tokenBucket:
			takenResult = true
		default:
			takenResult = false
		}
	}

	return takenResult
}
```



分布式id生成器：

Twitter的snowflake算法

![snowflake](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/golang/twitter-snowflake.png)



snowflake的64位bit分布：

- 开头的第一个bit，符号位，不使用
- 41bit的请求时间的时间戳，单位毫秒，最长支持69年，该timestamp不需要从1970年算起，可从系统上线时间算起
- 5bit 数据中心id
- 5bit 机器实例id
- 12bit 循环自增长id，在同数据中心、同一台机器、同一毫秒下，可产生2^12=4096条消息



轻量化的snowflake的Go实现: github.com/bwmarrin/snowflake

| 1 Bit  | 41 Bit    | 10 Bit | 12 Bit     |
| ------ | --------- | ------ | ---------- |
| Unused | Timestamp | NodeID | SequenceID |

```go
func main() {
	n, err := snowflake.NewNode(1)
	if err != nil {
		log.Fatal(err)
	}

	for i := 0; i < 3; i++ {
		id := n.Generate()
		fmt.Println("id:", id)
		fmt.Println("node:", id.Node(), "step:", id.Step(), "time:", id.Time())

		fmt.Println(id.Base64())
	}
}
```



分布式锁：

1. 基于Redis的setnx

   setnx 如何获取锁失败，相关的任务逻辑则不执行。它适合高并发下，用来争抢一些“唯一”资源，比如订单等。但它依赖于Redis节点的顺序来做正确的抢锁操作，一旦出现网络问题，则无法保证。

```go
func incr() {
	client := redis.NewClient(&redis.Options{
		Addr:     "localhost:6379",
		Password: "",
		DB:       0,
	})

	var lockKey = "counter_lock"
	var counterKey = "counter"

	// lock
	resp := client.SetNX(lockKey, 1, time.Second*5)
	locked, err := resp.Result()
	if err != nil || !locked {
		fmt.Println("lock result:", locked)
		return
	}

	// counter++
	getResp := client.Get(counterKey)
	cntValue, err := getResp.Int64()
	if err == nil || err == redis.Nil {
		incrResp := client.Incr(counterKey)
		newCntValue, err := incrResp.Result()
		if err != nil {
			fmt.Println(err)
		}

		fmt.Printf("counter %d changed to %d\n", cntValue, newCntValue)
	}

	// unlock
	delResp := client.Del(lockKey)
	delNum, err := delResp.Result()
	if err != nil || delNum == 0 {
		fmt.Println("unlock error:", err)
	}
}

func main() {
	var wg sync.WaitGroup
	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			incr()
		}()
	}

	wg.Wait()
}
```



2. Zookeeper

   zookeeper 锁，与Redis锁不同，Lock成功之前会一直阻塞

   它基于临时Sequence节点和watch API，例如使用的是/lock节点，Lock在该节点下的节点列表中插入自己的值，只要节点下的子节点发生变化，就会通知所有watch该节点的程序。此时程序会检查当前节点下最小节点的id是否与自己的一致，如果一致，加锁成功。

   此类型分布式的阻塞锁比较合适分布式任务调度场景，但不适合高频次持锁时间短的抢锁场景

```go
func main() {
	c, _, err := zk.Connect([]string{"127.0.0.1"}, time.Second)
	if err != nil {
		panic(err)
	}

	lock := zk.NewLock(c, "/lock", zk.WorldACL(zk.PermAll))
	err = lock.Lock()
	if err != nil {
		panic(err)
	}

	fmt.Println("lock successfully, do your business logic here")
	time.Sleep(time.Second * 10)

	// do something here

	lock.Unlock()
	fmt.Println("unlock successfully")
}
```



3. Etcd

   etcd 没有像zookeeper的sequence节点，它的sync的Lock流出如下：

   a. 先检查 /lock 路径下是否有值，如果有值，说明锁已经被别人抢了

   b. 如果没有值，那么写入自己的值，写入成功说明加锁成功

   c. 写入失败，说明被其他节点写入了值，加锁失败，watch /lock 下的事件，进入阻塞状态

   d. 当 /lock 路径下发生事件变化时，当前进程被唤醒。检查发生的事件是否时删除事件（说明锁被持有者主动unlock），或者过期事件（说明锁过期失效）。如果是上述两种事件，回到a，重走抢锁流程

```go
func main() {
	cli, err := clientv3.New(clientv3.Config{
		Endpoints: []string{"http://127.0.0.1:2379"},
	})
	if err != nil {
		log.Fatal(err)
	}
	defer cli.Close()

	s1, err := concurrency.NewSession(cli)
	if err != nil {
		log.Fatal(err)
	}
	defer s1.Close()
	m1 := concurrency.NewMutex(s1, "/lock")

	s2, err := concurrency.NewSession(cli)
	if err != nil {
		log.Fatal(err)
	}
	defer s2.Close()
	m2 := concurrency.NewMutex(s2, "/lock")

	// acquire lock for s1
	if err = m1.Lock(context.TODO()); err != nil {
		log.Fatal(err)
	}
	fmt.Println("acquired lock for s1")

	m2Locked := make(chan struct{})
	go func() {
		defer close(m2Locked)
		// wait until s1 is locks /lock
		if err := m2.Lock(context.TODO()); err != nil {
			log.Fatal(err)
		}
	}()

	if err := m1.Unlock(context.TODO()); err != nil {
		log.Fatal(err)
	}
	fmt.Println("released lock for s1")

	<-m2Locked
	fmt.Println("acquired lock for s2")

	// Output:
	// acquired lock for s1
	// released lock for s1
	// acquired lock for s2
}

```









