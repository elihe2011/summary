# 1. 微服务概述

## 1.1 单体应用

- 所有的功能都在一个应用程序中
- 维护同一个代码库

- 架构简单，典型的三层架构：前端 -> 后端 -> 数据库



## 1.2 微服务

概念：

- 基本原则：每个服务专注做好一件事

- 每个服务单独开发和部署，服务间是完全隔离的

优势：

- 迭代周期短，极大地提升了开发效率
- 独立部署，独立开发
- 可伸缩性好，能够针对指定的服务进行伸缩
- 故障隔离，不会相互影响

劣势：

- 复杂度增加，一个请求往往要经过多个服务，请求链路比较长
- 监控和定位问题困难
- 服务管理比较复杂



## 1.3 微服务架构

什么是微服务架构？

- 将单一应用程序，划分成一组微小的服务，每个服务运行独立的进程中，服务之间相互协调、配合。

- 服务之间采用轻量级的通信机制相互沟通（RPC, HTTP等）

- 每个服务都围绕具体的业务进行构建，并且能够被独立地部署。



微服务架构设计：

- 业务解藕，RPC框架、服务发现、k8s
- 网关
- 多级缓存
- 服务熔断降级、限流机制
- 自动集成部署 CI
- 自动扩容/缩容机制
- 链路监控 Prometheus
- 统一日志处理
- Metrics 指针对每个请求信息仪表盘化



微服务架构的优点：

- 松耦合，独立

- 局部修改，不影响整个应用

- 易于集成、替换、支持多语言

- 每个微服务都很小，代码容易理解

- 业务功能单一，开发效率高



缺点：

- 运维复杂，需要具备一定的DevOps技巧

- 分布式系统可能复杂难以管理



微服务框架的关注点：

- 服务注册、发现，配置中心
- 负载均衡
- 健康检查
- 限流和容错
- 统一的日志和错误处理
- 扩容和收缩
- 接口管理，文档等



服务注册中心：

- 解耦服务提供者和服务消费者
- K8S 中，每个服务创建一个 Service, 其他服务只要配置这个 Service Name 即可发现服务
- K8s 的服务发现和负载均衡是通过 iptables 和 内部的 DNS 来实现的



限流/熔断器：

- 本地限流：
  - CAS

  - 缓冲channel

- 分布式限流：
  - 熔断降级：调用链路中某个资源不稳定时，对资源进行限制，让请求快速失败，以免影响到其他资源的练级错误



## 1.4 分布式事务

分布式事务是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上



CAP 原则

- C (Consistency): 一致性，即分布式系统中数据的一致性。
- A (Availability): 可用性，即分布式系统中服务的高可用性，某个服务瘫痪不影响整个系统的正常运行。
- P (Partition Tolerance): 分区容错性，即分布式系统中的网络故障，整个系统任保持可用性和一致性。

业界通常的做法是在CAP中取其二，忽略其中一项。



# 2. 微服务组件

## 2.1 Etcd

### 2.1.1 简介

- Go 语言实现的高可靠 KV 存储系统
- 支持HTTP协议的PUT/GET/DELETE操作
- 支持服务注册与发现，WATCH接口（通过 HTTP Long Polling 实现）
- 支持 KEY 持有 TTL 属性
- CAS (Compare and Swap) 操作
- 支持多key的事务操作
- 支持目录操作



### 2.1.2 Etcd 使用场景

- 服务注册和发现
- 配置中心
- 分布式锁
- Leader选举



### 2.1.3 Raft 算法

应用场景：

- 解决分布式系统一致性问题
- 基于复制的



工作机制：

- leader选举
- 日志复制
- 安全性



基本概念：

- 角色

  - Leader
  - Follower
  - Candidate

- Term (任期)

  - 在raft协议中，将时间分成一个个任期

- 复制状态机：保证数据的一致性

- 心跳(heartbeat) 和 超时机制(timeout)

  在Raft算法中，有两个timeout机制来控制leader选举：

  - 选举定时器(election timeout): follower等待成为candidate的等待时间，它被随机设定为150ms~300ms
  - 心跳定时器(heartbeat timeout): 某节点成为leader后，它会发送Append Entries消息个其他节点，这些消息就是通过heartbeat timeout来传递，follower接收到leader的心跳包的同时也重置选举定时器



Leader选举：

- 触发条件：

  - 正常情况下，follower收到leader的心跳后，会把定时器清零，不会触发选举
  - follower的选举定时器超时(可能是leader故障)，会变成candidate，触发leader选举

- 选举过程：

  - 一开始，所有节点都是follower，同时启动选举定时器(随机的，降低冲突概率)
  - 定时器到期，变成candidate
  - 当前任期加+1，并投自己一票
  - 发起 RequestVote 的 RPC 请求，要求其他节点为自己投票
  - 如果得到超过半数的节点同意，就成为了leader
  - 如果选举超时，未产生leader，则进入下一个任期，重新选举

- 限制条件：
  - 每个节点最多投一次票，采用先到先服务原则
  - 如果没有投过票，则对比candidate的log与当前节点的log那个最新，**谁的lastLog的term越大谁就越新，如果term相同，谁的index越大谁就越新**。<font color="red">如果当前节点比candidate新，拒绝投票。</font>



日志复制：

- Client向Leader提交指令(如：SET 5)，Leader收到命令后，将命令追加到本地日志中，该目录状态处于"uncommitted"，复制状态机不会执行该命令
- Leader将命令(SET 5)并发复制给其他节点，并等待其他节点将命令写入到日志中，如果此时有些节点失败或者慢，Leader节点会一直重试，直到所有节点将该命令写入到了日志中。之后Leader节点就提交命令，并返回给Client节点

- Leader提交命令后，下一次的心跳包中会通知其他follower也来提交这条命令。收到Leader的消息后，就将命令应用到状态机中(State Machine)，最终保证每个节点的数据一致。

![etcd](https://raw.githubusercontent.com/elihe2011/bedgraph/master/etcd/etcd-sync.png)



### 2.1.4 Etcd 分布式锁

实现机制：

- **Lease机制**：租约机制 (TTL, Time To Live)。租约到期，KV 将失效删除；同时支持续约，即KeepAlive
- **Revision机制**：每个key都带有一个 Revision属性值，每一次事务操作，Revision的值都会加1，通过比较Revision的大小即可知道写操作的顺序。**在实现分布式锁时，多个程序同时抢锁，根据Revision值大小依次获得锁，避免“羊群效应”**
- **Prefix机制**：即目录机制，可根据前缀获取该目录下所有的key及其对应的属性(key, value 和 revision等)
- **Watch机制**：监听机制支持Watch某个固定的 key，也支持Watch一个目录，当key或者目录发生变化时，客户端将收到通知

使用 etcdctl 模拟锁：

```bash
# 租约
./etcdctl lease grant 60
./etcdctl lease revoke 694d7811c7947b0a
./etcdctl lease keep-alive 694d7811c7947b0a  # 续租

# 设置值
./etcdctl put --lease=694d7811c7947b0a /lock/mylock1 "hello world"

# terminal 1
./etcdctl lock /lock/mylock1
/lock/mylock1/694d7811c7947b0e

# terminal 2
./etcdctl lock /lock/mylock1
/lock/mylock1/694d7811c7947b12  # 在 terminal 1 完成后才能执行
```

使用 etcd 客户端：

```bash
go get go.etcd.io/etcd/client/v3
```

```go
func NewMutex(s *Session, pfx string) *Mutex
func (m *Mutex) Lock(ctx context.Context) error
func (m *Mutex) Unlock(ctx context.Context) error
```

实现分布式锁：

```go
func main() {
	ch := make(chan os.Signal)
	signal.Notify(ch)

	cli, err := clientv3.New(clientv3.Config{
		Endpoints:   []string{"localhost:2379"},
		DialTimeout: 5 * time.Second,
	})
	if err != nil {
		log.Fatal(err)
	}
	defer cli.Close()

	key := "/lock/mylock1"

	go func() {
		session, err := concurrency.NewSession(cli, concurrency.WithTTL(15))
		if err != nil {
			log.Fatal(err)
		}
		defer session.Close()

		mutex := concurrency.NewMutex(session, key)
		if err := mutex.Lock(context.TODO()); err != nil {
			log.Fatal("go1 get mutex error:", err.Error())
		}
		log.Println("go1 has obtained mutex")
		log.Println(mutex)

		time.Sleep(10 * time.Second)

		mutex.Unlock(context.TODO())
		log.Println("go1 has released mutex")
	}()

	go func() {
		session, err := concurrency.NewSession(cli, concurrency.WithTTL(15))
		if err != nil {
			log.Fatal(err)
		}
		defer session.Close()

		mutex := concurrency.NewMutex(session, key)
		if err := mutex.Lock(context.TODO()); err != nil {
			log.Fatal("go2 get mutex error:", err.Error())
		}
		log.Println("go2 has obtained mutex")
		log.Println(mutex)

		time.Sleep(2 * time.Second)

		mutex.Unlock(context.TODO())
		log.Println("go2 has released mutex")
	}()

	<-ch
}
```



### 2.1.5 Etcd 选主

Leader 选举，本质上一种抢锁。etcd有如下机制保证：

- **MVCC**，key存在版本属性，未被创建时，版本为0

- **CAS操作**，结合MVCC，可以实现竞争逻辑，`if(version==0) set(key, value)`, 通过原子操作，确保只有一台机器能set成功。

- **Lease租约**，key绑定的租约，如果到期没续约，该key将会被回收。

- **Watch监听**，监听key的变化事件，如果key被删除，则重新发起竞争。

```go
const prefix = "/election"
const prop = "local"

func main() {
	endpoints := []string{"localhost:2379"}
	cli, err := clientv3.New(clientv3.Config{Endpoints: endpoints})
	if err != nil {
		log.Fatal(err)
	}
	defer cli.Close()

	for {
		// 租约 5s
		s, err := concurrency.NewSession(cli, concurrency.WithTTL(5))
		if err != nil {
			log.Println(err)
			continue
		}

		// election
		e := concurrency.NewElection(s, prefix)
		ctx := context.TODO()

		log.Println("开始选举...")
		err = e.Campaign(ctx, prop)
		if err != nil {
			log.Println("竞选Leader失败，继续...")
			switch err {
			case context.Canceled:
				return
			default:
				continue
			}
		}

		log.Println("竞选Leader成功")

		if err := doCrontab(); err != nil {
			log.Println("调用失败，辞去Leader，重新选举")
			_ = e.Resign(ctx)
			continue
		}

		return
	}
}

func doCrontab() error {
	for {
		fmt.Println("doCrontab")
		time.Sleep(5 * time.Second)
		return errors.New("数据库连接失败")
	}
}

```



### 2.1.6 KV 操作

```go
func kvOPS(cli *clientv3.Client) {
	lease, err := cli.Grant(context.TODO(), 5)
	if err != nil {
		log.Fatal(err)
	}

	ctx, cancel := context.WithTimeout(context.TODO(), time.Second)
	_, err = cli.Put(ctx, logCfgKey, "sample_value", clientv3.WithLease(lease.ID))
	cancel()
	if err != nil {
		log.Fatal(err)
	}

	resp, err := cli.Get(context.TODO(), logCfgKey)
	if err != nil {
		log.Fatal(err)
	}
	for _, kv := range resp.Kvs {
		fmt.Printf("key: %s, value: %s\n", kv.Key, kv.Value)
	}

	time.Sleep(10 * time.Second)
	resp, err = cli.Get(context.TODO(), logCfgKey)
	if err != nil {
		log.Fatal(err)
	}

	// ttl 过期，无法获取值
	for _, kv := range resp.Kvs {
		fmt.Printf("key: %s, value: %s\n", kv.Key, kv.Value)
	}
}
```



### 2.1.7 watch 监控

```go
func watch(cli *clientv3.Client) {
	// 开多个线程去修改
	for i := 0; i < 10; i++ {
		go func(i int) {
			val := fmt.Sprintf("change-%d", i)
			ctx, cancel := context.WithTimeout(context.TODO(), time.Second)
			_, err := cli.Put(ctx, logCfgKey, val)
			cancel()
			if err != nil {
				log.Fatal(err)
			}
		}(i)
	}

	// 监控
	for {
		rch := cli.Watch(context.TODO(), logCfgKey)
		for resp := range rch {
			for _, ev := range resp.Events {
				log.Printf("watch: %s %q: %q\n", ev.Type, ev.Kv.Key, ev.Kv.Value)
			}
		}
	}
}
```



## 2.2 gRPC

### 2.2.1 简介

- 使用HTTP/2协议
- 使用Protocol Buffers作为序列化工具
- 实现动态代理，客户端可像调用本地服务一样直接调用另一台服务器的应用方法
- 使用静态路径，性能高



### 2.2.2 HTTP/2

**1. HTTP 1.1 & 2:**

| HTTP1.1                        | HTTP2               |
| ------------------------------ | ------------------- |
| 持久连接                       | 二进制分帧 （传输） |
| 请求管道化                     | 多路复用，连接共享  |
| 增加缓存处理 (cache-control)   | 头部压缩            |
| 增加Host字段、支持断点文件传输 | 服务器推送          |

**二进制分帧：**

![a](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/http2/http2_binary_framing_layer.svg)



**多路复用：并行发送请求**

![a](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/http2/http2_multiplexing.png)



**头部压缩：**

![a](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/http2/http2_header_compression.jpg)



**服务端推送：**

![a](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/http2/http2_server_push.svg)



**2. 使用http2:**

Go中只要使用https，默认支持http2，如何兼容？

通过客户端协商解决，协商算法：ALPN



协议协商：

- Upgrade机制:

  ```http
  GET /index.html HTTP/1.1
  Connection: Upgrade, HTTP2-Settings
  Upgrade: h2c
  
  h2c: http
  h2: https  主流浏览器支持
  ```

- ALPN机制： Application Layer Protocol Negotiation

  在https密钥交换过程中，增加ALPN扩展

  

### 2.2.3 Protobuf

#### 2.2.3.1 安装工具

```bash
# 工具
https://github.com/protocolbuffers/protobuf/releases

# MacOS
brew search protobuf
brew install protobuf@3.6

# 安装插件
go get -u github.com/golang/protobuf/protoc-gen-go
```

#### 2.2.3.2 开发流程

1. 编写IDL

```idl
// person.proto
syntax = "proto3";

option go_package = ".;address"; // 重要

package address;

enum PhoneType {
  HOME = 0;
  WORK = 1;
}

message Phone {
  PhoneType type = 1;
  string number = 2;
}

message Person {
  int32 id = 1;
  string name = 2;
  repeated Phone phones = 3;
}

message ContactBook {
  repeated Person persons = 1;
}
```

2. 生成指定语言的代码

```bash
protoc --go_out=./address ./address/person.proto
```

3. 序列化 & 反序列化

```go
func Marshal() {
	var contactBook address.ContactBook

	for i := 0; i < 5; i++ {
		person := &address.Person{
			Id:   int32(i),
			Name: uuid.New().String(),
		}

		phone := &address.Phone{
			Type:   address.PhoneType_HOME,
			Number: fmt.Sprintf("%d", rand.Int()),
		}
		person.Phones = append(person.Phones, phone)

		contactBook.Persons = append(contactBook.Persons, person)
	}

	data, err := proto.Marshal(&contactBook)
	if err != nil {
		log.Fatal(err)
	}

	err = ioutil.WriteFile("test.dat", data, 0644)
	if err != nil {
		log.Fatal(err)
	}

	log.Println("Marshal done.")
}

func Unmarshal() {
	data, err := ioutil.ReadFile("test.dat")
	if err != nil {
		log.Fatal(err)
	}

	var msg address.ContactBook
	err = proto.Unmarshal(data, &msg)
	if err != nil {
		log.Fatal(err)
	}

	for _, person := range msg.Persons {
		fmt.Printf("%d: %s\n", person.Id, person.Name)
	}

	log.Println("Unmarshal done.")
}
```



# 3. 服务注册和发现

实现服务注册和发现所需的基本功能：

- 服务注册：同一种服务的所有节点注册到相同目录下，节点启动后将自己的信息注册到所属服务的目录中
- 健康检查：服务节点定时发送心跳，注册到服务目录的信息设置一个较短的TTL，运行正常的服务节点，每隔一段时间去更新TTL
- 服务发现：通过服务名称，能查询到服务提供访问的 IP 和 Port。保证各个服务间能知道对方的存在。



## 3.1 Etcd 服务注册

```go
var endpoints = []string{"localhost:2379"}

func RegisterService(serviceTarget string, value string) {
	prefix := strings.TrimRight(serviceTarget, "/") + "/"

	cli, err := clientv3.New(clientv3.Config{
		Endpoints:   endpoints,
		DialTimeout: 5 * time.Second,
	})
	if err != nil {
		panic(err)
	}
	defer cli.Close()

	kv := clientv3.NewKV(cli)
	lease := clientv3.NewLease(cli)
	var curLeaseId clientv3.LeaseID = 0

	for {
		if curLeaseId == 0 {
			leaseResp, err := lease.Grant(context.TODO(), 10)
			if err != nil {
				panic(err)
			}

			key := fmt.Sprintf("%s%d", prefix, leaseResp.ID)
			if _, err = kv.Put(context.TODO(), key, value, clientv3.WithLease(leaseResp.ID)); err != nil {
				panic(err)
			}

			curLeaseId = leaseResp.ID
		} else {
			// 续租
			_, err = lease.KeepAliveOnce(context.TODO(), curLeaseId)
			if err == rpctypes.ErrLeaseNotFound {
				curLeaseId = 0
				continue
			} else if err != nil {
				panic(err)
			}
		}

		time.Sleep(time.Second)
	}
}
```



## 3.2 Etcd 服务发现

```go
type RemoteService struct {
	Name  string
	Nodes map[string]string
	mutex sync.Mutex
}

func DiscoveryService(serviceTarget string) *RemoteService {
	cli, err := clientv3.New(clientv3.Config{
		Endpoints:   endpoints,
		DialTimeout: 5 * time.Second,
	})
	if err != nil {
		panic(err)
	}
	defer cli.Close()

	service := &RemoteService{
		Name:  serviceTarget,
		Nodes: make(map[string]string),
	}

	kv := clientv3.NewKV(cli)
	resp, err := kv.Get(context.TODO(), serviceTarget, clientv3.WithPrefix())
	if err != nil {
		panic(err)
	}

	// 获取结点信息
	service.mutex.Lock()
	for _, kv := range resp.Kvs {
		service.Nodes[string(kv.Key)] = string(kv.Value)
	}
	service.mutex.Unlock()

	go watchServiceUpdate(cli, service)

	return service
}

func watchServiceUpdate(cli *clientv3.Client, service *RemoteService) {
	watcher := clientv3.NewWatcher(cli)

	for {
		rch := watcher.Watch(context.TODO(), service.Name, clientv3.WithPrefix())
		for resp := range rch {
			for _, event := range resp.Events {
				service.mutex.Lock()
				switch event.Type {
				case mvccpb.PUT:
					service.Nodes[string(event.Kv.Key)] = string(event.Kv.Value)
				case mvccpb.DELETE:
					delete(service.Nodes, string(event.Kv.Key))
				}
				service.mutex.Unlock()
			}
		}
	}
}
```



## 3.3 调用示例

服务端：

```go
// server.go
type HelloService struct{}

func (s *HelloService) Hello(request string, reply *string) error {
	*reply = "hello: " + request
	return nil
}

var serviceTarget = "Hello"
var address = "localhost:18080"

func main() {
	rpc.RegisterName("HelloService", new(HelloService))

	listener, err := net.Listen("tcp", address)
	if err != nil {
		panic(err)
	}

	// 注册服务
	go etcd.RegisterService(serviceTarget, address)

	for {
		conn, err := listener.Accept()
		if err != nil {
			fmt.Println(err)
			continue
		}
		rpc.ServeConn(conn)
	}
}
```

客户端：

```go
var serviceTarget = "Hello"

func main() {
	service := etcd.DiscoveryService(serviceTarget)

	var reply string

	for key, addr := range service.Nodes {
		fmt.Printf("%s, %s\n", key, addr)
		client, err := rpc.Dial("tcp", addr)
		if err != nil {
			panic(err)
		}

		for i := 0; i < 5; i++ {
			err = client.Call("HelloService.Hello", strconv.Itoa(i), &reply)
			if err != nil {
				fmt.Println(err)
			} else {
				fmt.Println(reply)
			}
		}
	}
}
```



# 4. 负载均衡

传统解决方案：

- DNS+LVS：
  - 集中式解决方案
  - 单点故障
- 软件负载均衡
  - 通过提供负载均衡的lib库，在调用方实现负载均衡
  - 结合服务发现，实现节点动态增删 (扩容和缩容)



负载均衡的好处：

- 服务水平可扩展（解决性能问题）

- 稳定性大大提升（解决单点故障问题）



常见负载均衡解决方案：

- DNS解决方案：
  - 把一个域名解析到多个IP上
  - 用户访问域名后，dns服务器通过一定策略返回一个IP
  - 具体策略：
    - 随机策略
    - 轮询策略
    - 加权策略
  - 缺点：其中一个IP宕机后，有一定概率失败
- 动态DNS解决方案 （k8s, coredns)
  - 可以通过程序动态的修改dns中域名配置的IP
  - 监控程序发现后端IP宕机后，通过dns进行删除
- Nginx反向代理
  - Nginx负载均衡
  - 扩容后，动态增加web server
  - web server宕机，nginx实时摘除
- LVS 负载均衡 
  - Nginx实时扩容
  - Nginx挂了，实时摘除
  - Lvs通过virtual ip实现高可用 （双机，浮点IP）



负载均衡算法：

- 负载均衡算法本质：
  - 从一系列节点中，通过一定的策略，找到一个节点
  - 然后调用方使用该节点进行连接和调用
- 负载均衡算法
  - 随机算法
  - 轮询算法
  - 加权算法：可将加权二维数组转化为一位数组
    - 加权随机算法
    - 加权轮询算法
  - 一致性hash算法



负载均衡器策略：

- **轮询**：平等分摊，认为后端所有机器的处理能力相同
- **加权轮询**：基于后端机器处理能力不同，加上不同的权重
- **最少连接数**：负载被分流到连接数最少的机器上



## 4.1 随机算法

```go
var (
	DefaultNodeWeight = 100
)

type RandomBalance struct{}

func NewRandomBalance() LoadBalance {
	return &RandomBalance{}
}

func (r *RandomBalance) Name() string {
	return "random"
}

func (r *RandomBalance) Select(ctx context.Context, nodes []*registry.Node) (node *registry.Node, err error) {
	if len(nodes) == 0 {
		err = errno.NotHaveInstance
		return
	}

	var totalWeight int
	for _, n := range nodes {
		if n.Weight == 0 {
			n.Weight = DefaultNodeWeight
		}
		totalWeight += n.Weight
	}

	// 随机权重
	randWeight := rand.Intn(totalWeight)
	for _, n := range nodes {
		randWeight -= n.Weight
		if randWeight < 0 {
			node = n
			return
		}
	}

	return
}
```



## 4.2 轮询算法

 ```go
type RoundRobinBalance struct {
	name  string
	index int
}

func NewRoundRobinBalance() LoadBalance {
	return &RoundRobinBalance{
		name: "RoundRobin",
	}
}

func (r *RoundRobinBalance) Name() string {
	return r.Name()
}

func (r *RoundRobinBalance) Select(ctx context.Context, nodes []*registry.Node) (node *registry.Node, err error) {
	if len(nodes) == 0 {
		err = errno.NotHaveInstance
		return
	}

	defer func() {
		if node != nil {
			setSelected(ctx, node)
		}
	}()

	var newNodes = filterNodes(ctx, nodes)
	if len(newNodes) == 0 {
		err = errno.AllNodeFailed
		return
	}

	r.index = (r.index + 1) % len(nodes)
	node = nodes[r.index]
	return
}
 ```



# 5. 异常容错

- 健康检查
- 熔断
- 限流



## 5.1 限流

服务限流：

- 常见限流思路：
  - 排队：秒杀抢购
  - 拒绝：除秒杀外的任何场景
- 限流算法：
  - 计数器限流
  - 漏桶限流
  - 令牌桶限流

限流的目的应当是通过对并发访问/请求进行限速或者一个时间窗口内的的请求进行限速来保护系统，一旦达到限制速率就可以拒绝服务、等待、降级。



### 5.1.1 计数器限流

- 在单位时间内进行计数，如果大于阀值，则拒绝服务

- 当过了单位时间，则重新进行计数

- 缺点：

  - 突发流量会出现毛刺现象：比如1秒内限流100个请求，前100ms处理完成了100个请求，后900ms空转
  - 计数不准确

  

### 5.1.2 漏桶算法

- 一个固定大小的水桶
- 以固定速率流出
- 水桶满了，则进行溢出(拒绝)

- 优点：
  - 解决了计数器限流算法的毛刺问题
  - 整体流量控制的比较平稳
- 缺点：
  - 流速固定
  - 无法应对某些突发的流量

![leaky-bucket](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/microservice/leaky-bucket.jpeg)

### 5.1.3 令牌桶算法

- 一个固定大小的水桶
- **以固定速率放入token**
- 如果能够拿到token则处理，否则拒绝
- 优点：

  - Google已实现`import golang.org/x/time/rate`

![token-bucket](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/microservice/token-bucket.jpeg)

区别：

- **漏桶**：传输速率始终保持不变，无法处理突发流量
- **令牌桶**：限制平均传输速率，但允许突发状况改变速度。（主流算法）



令牌桶原理：对全局计数的加减法操作，但这个计数需要添加读写锁

```go
func main() {
	var fillInterval = time.Millisecond * 500
	var capacity = 100
	var tokenBucket = make(chan struct{}, capacity)

	fillToken := func() {
		ticker := time.NewTicker(fillInterval)
		for {
			select {
			case <-ticker.C:
				select {
				case tokenBucket <- struct{}{}:
				default:
				}
				fmt.Println("current token count:", len(tokenBucket), time.Now())
			}
		}
	}

	go fillToken()

	time.Sleep(time.Minute * 5)
}


func TakeAvailable(block bool) bool {
	var takenResult bool
	if block {
		select {
		case <-tokenBucket:
			takenResult = true
		}
	} else {
		select {
		case <-tokenBucket:
			takenResult = true
		default:
			takenResult = false
		}
	}

	return takenResult
}
```



## 5.2 熔断机制

降级策略有三个相关的技术概念：“隔离”、“熔断”、“容错”：

- “隔离”是一种异常检测机制，常用的检测方法是请求超时、流量过大等。一般的设置参数包括超时时间、同时并发请求个数等。
- “熔断”是一种异常反应机制，“熔断”依赖于“隔离”。熔断通常基于错误率来实现。一般的设置参数包括统计请求的个数、错误率等。
- “容错”是一种异常处理机制，“容错”依赖于“熔断”。熔断以后，会调用“容错”的方法。一般的设置参数包括调用容错方法的次数等。



熔断目的：**阻止有潜在失败可能的请求。**

- 如果一个请求，有比较大的失败可能，那么就应该及时拒绝这个请求



核心思路：**对每一个发送请求的成功率进行预测**

最佳方案：

- 采用机器学习的方式进行预测
- 机器学习本质上是统计学，统计学玩的就是大数据

实现思路：

- 针对每一个请求的结果，进行<font color="red">失败率统计</font>
- 在一定时间窗口内，如果<font color="red">失败率超过一个比率，那么熔断就打开</font>
- 一段时间后，自动关闭熔断器
- 改进：引入半开状态 `Half-Open`, 半开时只允许非常有限的请求正常进行。这些请求任何一个失败，则进入Open状态；这些请求全部成功，熔断器关闭(Closed)



熔断降级库：

- [Sentinel](https://github.com/alibaba/Sentinel) 由阿里开源，主要以流量为切入点，从限流、流量整形、熔断降级、系统自适应保护等多个维度来帮助开发者保障微服务的稳定性
- Hystrix(Porcupine)  用 Netflix 开源,，现实了对服务的 熔断、隔离检测、请求并发量控制、请求延时控制和服务降级处理功能。



### 5.2.1 Hystrix

Hystrix 功能：

- 过载保护：防止雪崩
- 熔断器：快速失败，快速恢复
- 并发控制：防止单个依赖把线程全部耗尽
- 超时控制：防止永远阻塞



熔断触发条件：一个统计窗口内，请求数量大于`RequestVolumeThreshold`, 且失败率大于`ErrorPercentThreshold`才会触发熔断

控制熔断的 5 个主要参数:

```go
type CommandConfig struct {
    Timeout                int  // 超时时间，默认1000 ms
    MaxConcurrentRequests  int  // 并发控制，默认10
    RequestVolumeThreshold int  // 熔断器打开后，冷却时间，默认500 ms
    SleepWindow            int  // 一个统计窗口的请求数量，默认20
    ErrorPercentThreshold  int  // 失败百分率，默认50%
}
```



熔断器状态：

<img src="https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/microservice/hystrix_1.png" width="700" height="400" align="left" />

- Closed：让所有请求都通过的默认状态。在阈值下的请求不管成功还是失败，熔断器的状态都不会改变。可能出现的错误是 **Max Concurrency**（最大并发数）和 **Timeout**（超时）。
- Open：所有的请求都会返回 **Circuit Open** 错误并被标记为失败。这是一种不等待处理结束的 timeout 时间的 fail-fast 机制。
- Half Open：周期性地向下游服务发出请求，检查它是否已恢复。如果下游服务已恢复，熔断器切换到 Closed 状态，否则熔断器保持 Open 状态。



Hystrix 的主要模块：

- **setting** 管理熔断器的配置，包括存储，新增和读取
- **hystrix** 熔断器核心部分，对外提供同步和异步的方法，对内上报请求事件以及fallback降级处理
- **circuit** 管理熔断器的状态变更
- **metrics** 统计和计算请求的响应情况
- **pool** 管理请求池，控制请求池最大数目以及请求ticket的发放和回收
- **eventstream** 各项指标的监控



示例：

```go
// server.go
type response struct {
	msg string
}

func ordinaryHandler(w http.ResponseWriter, r *http.Request) {
	w.Write([]byte("Hello\n"))
}

func AHandler(w http.ResponseWriter, r *http.Request) {
	handle(w, r, "aaa")
}

func BHandler(w http.ResponseWriter, r *http.Request) {
	handle(w, r, "bbb")
}

func handle(w http.ResponseWriter, r *http.Request, name string) {
	done := make(chan *response, 1)

	/*	fallback := func(err error) error {
		done <- &response{"fallback response\n"}
		return nil
	}*/

	errChan := hystrix.Go(name, func() error {
		time.Sleep(2 * time.Second)
		done <- &response{"OK\n"}
		return nil
	}, nil)

	select {
	case err := <-errChan:
		http.Error(w, err.Error(), 500)
	case res := <-done:
		w.Write([]byte(res.msg))
	}
}

func main() {
	hystrix.ConfigureCommand("aaa", hystrix.CommandConfig{
		Timeout: 1000,
	})

	hystrix.ConfigureCommand("bbb", hystrix.CommandConfig{
		Timeout: 5000,
	})

	http.HandleFunc("/aaa", AHandler)
	http.HandleFunc("/bbb", BHandler)
	http.HandleFunc("/hello", ordinaryHandler)

	http.ListenAndServe(":3001", nil)
}

//client.go
func run(result chan string, name string) {
	start := time.Now().String()
	msg := fmt.Sprintf("Request start: %s, url: %s, ", start, name)

	resp, err := http.Get("http://localhost:3001/" + name)
	if err != nil {
		result <- msg + "response: " + err.Error()
	} else {
		data, _ := ioutil.ReadAll(resp.Body)
		result <- msg + "response: " + string(data)
		resp.Body.Close()
	}
}

func main() {
	result := make(chan string)

	for i := 0; i < 50; i++ {
		go func() {
			run(result, "aaa")
			run(result, "bbb")
		}()
	}

	for {
		select {
		case r := <-result:
			fmt.Print(r)
		}
	}
}
```






# 6. 服务监控

日志收集：

- 日志收集器 -> kafka集群 -> 数据处理 -> 日志查询和报警

Metrics打点：

- 实时采样服务的运行状态
- 直观的报表展示



中间件：连接软件组件或应用的软件，提供一致性的服务，比如Web服务器、事务监控、消息队列等。



## 6.1 Prometheus

- 分布式监控系统
- 使用go开发，完全开源
- 被广泛用于监控这个云基础架构设施

![a](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/k8s/prometheus-ecosystem.jpg)

度量类型：

- 计数器 Counter 采样：只能累积或重置
- Gauges 采样：被监控对象的瞬时状态，如当前时刻 CPU 的使用率、内存的使用量、硬盘的容量等。

- 柱状图 Histogram 采样：
  - 对每个采样点进行统计，打到各个桶中(bucket)
  - 对每个采样点值累积和(sum)
  - 对采样点的次数累积和(count)
- Summary 采样：在客户端对于一段时间内(默认10m)的每个采样点进行统计，并形成分位图



## 6.2 Grafana

- 跨平台开源的度量分析可视化工具，可通过将采集的数据查询然后可视化展示，并及时通知
- 展示方式：提供丰富的仪表盘插件，如热土、折线图、图表等
- 数据源：Graphite，InfluxDB，Prometheus，ElasticSearch等
- 通知提醒：可设置视化定义的重要指标的报警规则，达到阀值后进行告警操作

- 混合展示：在同一图表中混合使用不同的数据源，可以居于每个查询指定数据源






# 7. 分布式追踪

微服务架构问题:

- 故障定位难
- 容量预估难
- 资源浪费多
- 链路梳理难



分布式追踪系统：

- trace_id：
  - 为每个请求分配唯一的id
  - 日志聚合
- Span：
  - 每个子系统的详细处理过程
  - 通过span进行抽象, span之间有父子关系
- Span Context:
  - 传播问题
    - 进程内传播
    - 进程间传播
  - http协议
    - 通过http头部进行透明传播
  - Tcp协议
    - Thrift: 需要改造thrift进行支持



技术选型：

- 使用 opentracing 提供的通用接口
- 底层 jeagger 做分布式系统



`google.golang.org/grpc/metadata`: grpc服务元数据



# 8. 分布式全局ID生成器

|      | UUID                             | Sequence                   | Snowflake                          |
| ---- | -------------------------------- | -------------------------- | ---------------------------------- |
| 描述 | uuid                             | 使用DB自增id实现           | 根据时间+机器分配标识+自增序列     |
| 依赖 | NA                               | DB                         | NA                                 |
| 优点 | 性能高                           | 简单                       | 简单                               |
| 缺点 | 生成的字符串复杂，很多场景不适用 | 依赖DB的性能和重复发号问题 | 依赖系统时间，时间回拨，可造成重复 |



Twitter的snowflake算法

![snowflake](https://cdn.jsdelivr.net/gh/elihe2011/bedgraph@master/golang/twitter-snowflake.png)



snowflake的64位bit分布：

- 开头的第一个bit，符号位，不使用
- 41bit的请求时间的时间戳，单位毫秒，最长支持69年，该timestamp不需要从1970年算起，可从系统上线时间算起
- 5bit 数据中心id
- 5bit 机器实例id
- 12bit 循环自增长id，在同数据中心、同一台机器、同一毫秒下，可产生2^12=4096条消息



轻量化的snowflake的Go实现: github.com/bwmarrin/snowflake

| 1 Bit  | 41 Bit    | 10 Bit | 12 Bit     |
| ------ | --------- | ------ | ---------- |
| Unused | Timestamp | NodeID | SequenceID |

```go
func main() {
	n, err := snowflake.NewNode(1)
	if err != nil {
		log.Fatal(err)
	}

	for i := 0; i < 3; i++ {
		id := n.Generate()
		fmt.Println("id:", id)
		fmt.Println("node:", id.Node(), "step:", id.Step(), "time:", id.Time())

		fmt.Println(id.Base64())
	}
}
```

变种：https://github.com/sony/sonyflake



# 9. 分布式锁

|          | redis                  | Zookeeper                    | Etcd                         |
| -------- | ---------------------- | ---------------------------- | ---------------------------- |
| 描述     | 使用set nx实现         | 使用临时节点+watch           | Lease, Watch, Revison,Prefix |
| 依赖     | redis                  | zk                           | etcd                         |
| 适用场景 | 并发抢锁               | 锁占用时间长，其他任务可等待 | 等待锁                       |
| 高可用性 | redis 故障可导致锁失效 | paxos协议                    | raft协议                     |



## 9.1 Redis

基于 Redis 的 setnx 命令，如果 setnx 获取锁失败，相关的任务逻辑则不执行。它适合高并发下，用来争抢一些“唯一”资源，比如订单等。但它依赖于Redis节点的顺序来做正确的抢锁操作，一旦出现网络问题，则无法保证。

```go
func incr() {
	client := redis.NewClient(&redis.Options{
		Addr:     "localhost:6379",
		Password: "",
		DB:       0,
	})

	var lockKey = "counter_lock"
	var counterKey = "counter"

	// lock
	resp := client.SetNX(lockKey, 1, time.Second*5)
	locked, err := resp.Result()
	if err != nil || !locked {
		fmt.Println("lock result:", locked)
		return
	}

	// counter++
	getResp := client.Get(counterKey)
	cntValue, err := getResp.Int64()
	if err == nil || err == redis.Nil {
		incrResp := client.Incr(counterKey)
		newCntValue, err := incrResp.Result()
		if err != nil {
			fmt.Println(err)
		}

		fmt.Printf("counter %d changed to %d\n", cntValue, newCntValue)
	}

	// unlock
	delResp := client.Del(lockKey)
	delNum, err := delResp.Result()
	if err != nil || delNum == 0 {
		fmt.Println("unlock error:", err)
	}
}

func main() {
	var wg sync.WaitGroup
	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			incr()
		}()
	}

	wg.Wait()
}
```



## 9.2 Zookeeper

zookeeper 锁，与Redis锁不同，Lock成功之前会一直阻塞

它基于临时Sequence节点和watch API，例如使用的是/lock节点，Lock在该节点下的节点列表中插入自己的值，只要节点下的子节点发生变化，就会通知所有watch该节点的程序。此时程序会检查当前节点下最小节点的id是否与自己的一致，如果一致，加锁成功。

此类型分布式的阻塞锁比较合适分布式任务调度场景，但不适合高频次持锁时间短的抢锁场景

```go
func main() {
	c, _, err := zk.Connect([]string{"127.0.0.1"}, time.Second)
	if err != nil {
		panic(err)
	}

	lock := zk.NewLock(c, "/lock", zk.WorldACL(zk.PermAll))
	err = lock.Lock()
	if err != nil {
		panic(err)
	}

	fmt.Println("lock successfully, do your business logic here")
	time.Sleep(time.Second * 10)

	// do something here

	lock.Unlock()
	fmt.Println("unlock successfully")
}
```



## 9.3 Etcd

etcd 没有像zookeeper的sequence节点，它的sync的Lock流出如下：

a. 先检查 /lock 路径下是否有值，如果有值，说明锁已经被别人抢了

b. 如果没有值，那么写入自己的值，写入成功说明加锁成功

c. 写入失败，说明被其他节点写入了值，加锁失败，watch /lock 下的事件，进入阻塞状态

d. 当 /lock 路径下发生事件变化时，当前进程被唤醒。检查发生的事件是否时删除事件（说明锁被持有者主动unlock），或者过期事件（说明锁过期失效）。如果是上述两种事件，回到a，重走抢锁流程

```go
func main() {
	cli, err := clientv3.New(clientv3.Config{
		Endpoints: []string{"http://127.0.0.1:2379"},
	})
	if err != nil {
		log.Fatal(err)
	}
	defer cli.Close()

	s1, err := concurrency.NewSession(cli)
	if err != nil {
		log.Fatal(err)
	}
	defer s1.Close()
	m1 := concurrency.NewMutex(s1, "/lock")

	s2, err := concurrency.NewSession(cli)
	if err != nil {
		log.Fatal(err)
	}
	defer s2.Close()
	m2 := concurrency.NewMutex(s2, "/lock")

	// acquire lock for s1
	if err = m1.Lock(context.TODO()); err != nil {
		log.Fatal(err)
	}
	fmt.Println("acquired lock for s1")

	m2Locked := make(chan struct{})
	go func() {
		defer close(m2Locked)
		// wait until s1 is locks /lock
		if err := m2.Lock(context.TODO()); err != nil {
			log.Fatal(err)
		}
	}()

	if err := m1.Unlock(context.TODO()); err != nil {
		log.Fatal(err)
	}
	fmt.Println("released lock for s1")

	<-m2Locked
	fmt.Println("acquired lock for s2")

	// Output:
	// acquired lock for s1
	// released lock for s1
	// acquired lock for s2
}
```



更简单的 etcd 分布式锁实现：

```go
// go get -u github.com/zieckey/etcdsync

func main() {
	m, err := etcdsync.New("/mylock", 10, []string{"http://127.0.0.1:2379"})
	if m == nil || err != nil {
		log.Printf("etcdsync.New failed")
		return
	}
	err = m.Lock()
	if err != nil {
		log.Printf("etcdsync.Lock failed")
	} else {
		log.Printf("etcdsync.Lock OK")
	}

	log.Printf("Get the lock. Do something here.")

	err = m.Unlock()
	if err != nil {
		log.Printf("etcdsync.Unlock failed")
	} else {
		log.Printf("etcdsync.Unlock OK")
	}
}
```





